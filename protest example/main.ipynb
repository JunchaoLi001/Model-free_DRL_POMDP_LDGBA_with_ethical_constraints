{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b05e71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Omega-automaton states (including the trap state): 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<csrl.oaa.oaa at 0x204401a4490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: 0\n",
      "Transition function: [\n",
      "  {(): 0, ('a',): 1, ('b',): 0, ('c',): 2, ('a', 'b'): 0, ('a', 'c'): 2, ('b', 'c'): 2, ('a', 'b', 'c'): 2},\n",
      "  {(): 1, ('a',): 1, ('b',): 0, ('c',): 2, ('a', 'b'): 0, ('a', 'c'): 2, ('b', 'c'): 2, ('a', 'b', 'c'): 2},\n",
      "  {(): 2, ('a',): 2, ('b',): 2, ('c',): 2, ('a', 'b'): 2, ('a', 'c'): 2, ('b', 'c'): 2, ('a', 'b', 'c'): 2}\n",
      "]\n",
      "Acceptance: [\n",
      "  {(): [None], ('a',): [True], ('b',): [None], ('c',): [None], ('a', 'b'): [None], ('a', 'c'): [None], ('b', 'c'): [None], ('a', 'b', 'c'): [None]},\n",
      "  {(): [None], ('a',): [None], ('b',): [True], ('c',): [None], ('a', 'b'): [None], ('a', 'c'): [None], ('b', 'c'): [None], ('a', 'b', 'c'): [None]},\n",
      "  {(): [None], ('a',): [None], ('b',): [None], ('c',): [None], ('a', 'b'): [None], ('a', 'c'): [None], ('b', 'c'): [None], ('a', 'b', 'c'): [None]}\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### import manually defined automata\n",
    "%matplotlib inline\n",
    "from csrl.pomdp import GridPOMDP\n",
    "from csrl.oaa import oaa\n",
    "from csrl import ControlSynthesis\n",
    "import numpy as np \n",
    "\n",
    "oa=oaa()\n",
    "\n",
    "# LTL Specification\n",
    "#ltl = 'GF(a & Fb) & G!c' ### goes to 'a' then 'b' recurrently, gallobly ! c\n",
    "print('Number of Omega-automaton states (including the trap state):',oa.shape[1])\n",
    "display(oa)\n",
    "\n",
    "print('Initial state:',oa.q0)\n",
    "print('Transition function: ['),print(*['  '+str(t) for t in oa.delta],sep=',\\n'),print(']')\n",
    "print('Acceptance: ['),print(*['  '+str(t) for t in oa.acc],sep=',\\n'),print(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38bb76c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JUNCHA~1\\AppData\\Local\\Temp/ipykernel_25148/633445410.py:29: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  ],dtype=np.object)\n"
     ]
    }
   ],
   "source": [
    "# POMDP Description\n",
    "shape = (10,10)\n",
    "# E: Empty, T: Trap, B: Obstacle\n",
    "structure = np.array([\n",
    "['E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E'],\n",
    "['E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E'],\n",
    "['E',  'E',  'B',  'B',  'B',  'E',  'E',  'E',  'E',  'E'],\n",
    "['E',  'E',  'B',  'B',  'B',  'E',  'E',  'B',  'E',  'E'],\n",
    "['E',  'E',  'E',  'E',  'E',  'E',  'E',  'B',  'E',  'E'],\n",
    "['E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E'],\n",
    "['E',  'E',  'B',  'B',  'E',  'E',  'B',  'B',  'E',  'E'],\n",
    "['E',  'E',  'E',  'E',  'E',  'E',  'B',  'B',  'E',  'E'],\n",
    "['E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E'],\n",
    "['E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E']\n",
    "])\n",
    "\n",
    "# Labels of the states\n",
    "label = np.array([\n",
    "[('c',),       ('c',),       (),       (),       (),       (),       (),       (),       ('b',),       ('b',)],\n",
    "[('c',),       ('c',),       (),       (),       (),       (),       (),       (),       ('b',),       ('b',)],\n",
    "[(),       (),       (),       (),       (),       ('c',),       ('c',),       (),       (),       ()],\n",
    "[(),       (),       (),       (),       (),       ('c',),       ('c',),       (),       (),       ()],\n",
    "[(),       (),       (),       (),       (),       (),       (),       (),       (),       ()],\n",
    "[(),       (),       (),       (),       (),       (),       (),       (),       (),       ()],\n",
    "[(),       (),       (),       (),       (),       (),       (),       (),       (),       ()],\n",
    "[(),       (),       (),       (),       (),       (),       (),       (),       (),       ()],\n",
    "[('a',),       ('a',),       (),       (),       (),       (),       (),       (),       (),       ()],\n",
    "[('a',),       ('a',),       (),       (),       (),       (),       (),       (),       (),       ()]\n",
    "],dtype=np.object)\n",
    "# Colors of the labels\n",
    "lcmap={\n",
    "    ('a',):'lightgreen',\n",
    "    ('b',):'lightgreen',\n",
    "    ('c',):'pink'\n",
    "}\n",
    "grid_pomdp = GridPOMDP(shape=shape,structure=structure,label=label,lcmap=lcmap,figsize=5)  # Use figsize=4 for smaller figures\n",
    "\n",
    "# Construct the product MDP\n",
    "csrl = ControlSynthesis(grid_pomdp,oa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6820a7e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csrl.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe1396d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])]],\n",
       "\n",
       "        [[list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])]],\n",
       "\n",
       "        [[list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])],\n",
       "         [list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "          list([0, 1, 2, 3, 4, 5, 6, 7])]]]], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csrl.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b01afbdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[([(0, 2, 0, 1), (0, 2, 0, 0)], [0.024999999999999994, 0.975]),\n",
       "           ([(0, 2, 1, 0), (0, 2, 0, 1), (0, 2, 1, 1), (0, 2, 0, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 1, 0), (0, 2, 0, 1), (0, 2, 1, 1), (0, 2, 0, 0)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 0, 0), (0, 2, 0, 2), (0, 2, 0, 1)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 2, 1, 1), (0, 2, 0, 0), (0, 2, 0, 2), (0, 2, 1, 0), (0, 2, 1, 2)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 1, 1), (0, 2, 0, 2), (0, 2, 1, 2), (0, 2, 0, 1)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 0, 0, 1), (0, 0, 0, 3), (0, 0, 0, 2)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 0, 1, 2), (0, 0, 0, 1), (0, 0, 0, 3), (0, 0, 1, 1), (0, 0, 1, 3)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 0, 1, 2), (0, 0, 0, 3), (0, 0, 1, 3), (0, 0, 0, 2)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          ...,\n",
       "          [([(0, 0, 0, 6), (0, 0, 0, 8), (0, 0, 0, 7)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 0, 1, 7), (0, 0, 0, 6), (0, 0, 0, 8), (0, 0, 1, 6), (0, 0, 1, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 0, 1, 7), (0, 0, 0, 8), (0, 0, 1, 8), (0, 0, 0, 7)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 0, 0, 7), (0, 0, 0, 9), (0, 0, 0, 8)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 0, 1, 8), (0, 0, 0, 7), (0, 0, 0, 9), (0, 0, 1, 7), (0, 0, 1, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 0, 1, 8), (0, 0, 0, 9), (0, 0, 1, 9), (0, 0, 0, 8)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 0, 0, 8), (0, 0, 0, 9)], [0.024999999999999994, 0.975]),\n",
       "           ([(0, 0, 1, 9), (0, 0, 0, 8), (0, 0, 1, 8), (0, 0, 0, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 0, 1, 9), (0, 0, 0, 9)], [0.024999999999999994, 0.975]),\n",
       "           ..., None, None, None]],\n",
       "\n",
       "         [[([(0, 2, 0, 0), (0, 2, 1, 1), (0, 2, 0, 1), (0, 2, 1, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 2, 0), (0, 2, 1, 1), (0, 2, 2, 1), (0, 2, 1, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 2, 0), (0, 2, 0, 0), (0, 2, 1, 1), (0, 2, 2, 1), (0, 2, 0, 1)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 0, 1), (0, 2, 1, 0), (0, 2, 1, 2), (0, 2, 0, 0), (0, 2, 0, 2)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 2, 1), (0, 2, 1, 0), (0, 2, 1, 2), (0, 2, 2, 0), (0, 2, 1, 1)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ([(0, 2, 2, 1), (0, 2, 0, 1), (0, 2, 1, 2), (0, 2, 0, 2), (0, 2, 1, 1)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.025000000000000022]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 0, 0, 2), (0, 0, 1, 1), (0, 0, 1, 3), (0, 0, 0, 1), (0, 0, 0, 3)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 0, 1, 1), (0, 0, 1, 3), (0, 0, 2, 1), (0, 0, 1, 2)], [0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.925]),\n",
       "           ([(0, 0, 0, 2), (0, 0, 1, 3), (0, 0, 0, 3), (0, 0, 1, 2)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          ...,\n",
       "          [([(0, 0, 0, 7), (0, 0, 1, 6), (0, 0, 1, 8), (0, 0, 0, 6), (0, 0, 0, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 0, 2, 7), (0, 0, 1, 6), (0, 0, 1, 8), (0, 0, 2, 6), (0, 0, 2, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 0, 2, 7), (0, 0, 0, 7), (0, 0, 1, 8), (0, 0, 2, 8), (0, 0, 0, 8)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 0, 0, 8), (0, 0, 1, 7), (0, 0, 1, 9), (0, 0, 0, 7), (0, 0, 0, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 0, 2, 8), (0, 0, 1, 7), (0, 0, 1, 9), (0, 0, 2, 7), (0, 0, 2, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 0, 2, 8), (0, 0, 0, 8), (0, 0, 1, 9), (0, 0, 2, 9), (0, 0, 0, 9)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 0, 0, 9), (0, 0, 1, 8), (0, 0, 0, 8), (0, 0, 1, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 0, 2, 9), (0, 0, 1, 8), (0, 0, 2, 8), (0, 0, 1, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 0, 2, 9), (0, 0, 0, 9), (0, 0, 1, 9)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ..., None, None, None]],\n",
       "\n",
       "         [[([(0, 0, 1, 0), (0, 0, 2, 1), (0, 0, 1, 1), (0, 0, 2, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 0, 3, 0), (0, 0, 2, 1), (0, 0, 3, 1), (0, 0, 2, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 0, 3, 0), (0, 0, 1, 0), (0, 0, 2, 1), (0, 0, 3, 1), (0, 0, 1, 1)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 0, 1, 1), (0, 0, 2, 0), (0, 0, 1, 0), (0, 0, 1, 2), (0, 0, 2, 1)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ([(0, 0, 3, 1), (0, 0, 2, 0), (0, 0, 3, 0), (0, 0, 2, 1)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 0, 3, 1), (0, 0, 1, 1), (0, 0, 1, 2), (0, 0, 2, 1)], [0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.925]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 0, 2, 2)], array([1.])),\n",
       "           ([(0, 0, 2, 2)], array([1.])),\n",
       "           ([(0, 0, 2, 2)], array([1.])), ..., None, None, None],\n",
       "          ...,\n",
       "          [([(0, 0, 1, 7), (0, 0, 2, 6), (0, 0, 2, 8), (0, 0, 1, 6), (0, 0, 1, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 0, 2, 6), (0, 0, 2, 8), (0, 0, 3, 6), (0, 0, 3, 8), (0, 0, 2, 7)], [0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.9]),\n",
       "           ([(0, 0, 1, 7), (0, 0, 2, 8), (0, 0, 3, 8), (0, 0, 1, 8), (0, 0, 2, 7)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 0, 1, 8), (0, 0, 2, 7), (0, 0, 2, 9), (0, 0, 1, 7), (0, 0, 1, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 0, 3, 8), (0, 0, 2, 7), (0, 0, 2, 9), (0, 0, 3, 9), (0, 0, 2, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ([(0, 0, 3, 8), (0, 0, 1, 8), (0, 0, 2, 9), (0, 0, 3, 9), (0, 0, 1, 9)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 0, 1, 9), (0, 0, 2, 8), (0, 0, 1, 8), (0, 0, 2, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 0, 3, 9), (0, 0, 2, 8), (0, 0, 3, 8), (0, 0, 2, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 0, 3, 9), (0, 0, 1, 9), (0, 0, 2, 9)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ..., None, None, None]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[([(0, 0, 6, 0), (0, 0, 7, 1), (0, 0, 6, 1), (0, 0, 7, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 0, 8, 0), (0, 0, 7, 1), (0, 0, 8, 1), (0, 0, 7, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 0, 8, 0), (0, 0, 6, 0), (0, 0, 7, 1), (0, 0, 8, 1), (0, 0, 6, 1)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 0, 6, 1), (0, 0, 7, 0), (0, 0, 7, 2), (0, 0, 6, 0), (0, 0, 7, 1)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ([(0, 0, 8, 1), (0, 0, 7, 0), (0, 0, 7, 2), (0, 0, 8, 0), (0, 0, 8, 2)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 0, 8, 1), (0, 0, 6, 1), (0, 0, 7, 2), (0, 0, 8, 2), (0, 0, 7, 1)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.025000000000000022]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 0, 7, 1), (0, 0, 7, 3), (0, 0, 6, 1), (0, 0, 7, 2)], [0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.925]),\n",
       "           ([(0, 0, 8, 2), (0, 0, 7, 1), (0, 0, 7, 3), (0, 0, 8, 1), (0, 0, 8, 3)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 0, 8, 2), (0, 0, 7, 3), (0, 0, 8, 3), (0, 0, 7, 2)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          ...,\n",
       "          [([(0, 0, 7, 7)], array([1.])),\n",
       "           ([(0, 0, 7, 7)], array([1.])),\n",
       "           ([(0, 0, 7, 7)], array([1.])), ..., None, None, None],\n",
       "          [([(0, 0, 6, 8), (0, 0, 7, 9), (0, 0, 6, 9), (0, 0, 7, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 0, 8, 8), (0, 0, 7, 9), (0, 0, 8, 7), (0, 0, 8, 9), (0, 0, 7, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ([(0, 0, 8, 8), (0, 0, 6, 8), (0, 0, 7, 9), (0, 0, 8, 9), (0, 0, 6, 9)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 0, 6, 9), (0, 0, 7, 8), (0, 0, 6, 8), (0, 0, 7, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 0, 8, 9), (0, 0, 7, 8), (0, 0, 8, 8), (0, 0, 7, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 0, 8, 9), (0, 0, 6, 9), (0, 0, 7, 9)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ..., None, None, None]],\n",
       "\n",
       "         [[([(0, 1, 7, 0), (0, 1, 8, 1), (0, 1, 7, 1), (0, 1, 8, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 1, 9, 0), (0, 1, 8, 1), (0, 1, 9, 1), (0, 1, 8, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 1, 9, 0), (0, 1, 7, 0), (0, 1, 8, 1), (0, 1, 9, 1), (0, 1, 7, 1)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 1, 7, 1), (0, 1, 8, 0), (0, 1, 8, 2), (0, 1, 7, 0), (0, 1, 7, 2)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 1, 9, 1), (0, 1, 8, 0), (0, 1, 8, 2), (0, 1, 9, 0), (0, 1, 9, 2)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 1, 9, 1), (0, 1, 7, 1), (0, 1, 8, 2), (0, 1, 9, 2), (0, 1, 7, 2)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 0, 7, 2), (0, 0, 8, 1), (0, 0, 8, 3), (0, 0, 7, 1), (0, 0, 7, 3)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 0, 9, 2), (0, 0, 8, 1), (0, 0, 8, 3), (0, 0, 9, 1), (0, 0, 9, 3)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 0, 9, 2), (0, 0, 7, 2), (0, 0, 8, 3), (0, 0, 9, 3), (0, 0, 7, 3)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          ...,\n",
       "          [([(0, 0, 8, 6), (0, 0, 8, 8), (0, 0, 7, 8), (0, 0, 8, 7)], [0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.925]),\n",
       "           ([(0, 0, 9, 7), (0, 0, 8, 6), (0, 0, 8, 8), (0, 0, 9, 6), (0, 0, 9, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 0, 9, 7), (0, 0, 8, 8), (0, 0, 9, 8), (0, 0, 7, 8), (0, 0, 8, 7)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 0, 7, 8), (0, 0, 8, 7), (0, 0, 8, 9), (0, 0, 7, 9), (0, 0, 8, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ([(0, 0, 9, 8), (0, 0, 8, 7), (0, 0, 8, 9), (0, 0, 9, 7), (0, 0, 9, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 0, 9, 8), (0, 0, 7, 8), (0, 0, 8, 9), (0, 0, 9, 9), (0, 0, 7, 9)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 0, 7, 9), (0, 0, 8, 8), (0, 0, 7, 8), (0, 0, 8, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 0, 9, 9), (0, 0, 8, 8), (0, 0, 9, 8), (0, 0, 8, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 0, 9, 9), (0, 0, 7, 9), (0, 0, 8, 9)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ..., None, None, None]],\n",
       "\n",
       "         [[([(0, 1, 8, 0), (0, 1, 9, 1), (0, 1, 8, 1), (0, 1, 9, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 1, 9, 1), (0, 1, 9, 0)], [0.024999999999999994, 0.975]),\n",
       "           ([(0, 1, 8, 0), (0, 1, 9, 1), (0, 1, 8, 1), (0, 1, 9, 0)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 1, 8, 1), (0, 1, 9, 0), (0, 1, 9, 2), (0, 1, 8, 0), (0, 1, 8, 2)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 1, 9, 0), (0, 1, 9, 2), (0, 1, 9, 1)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 1, 8, 1), (0, 1, 9, 2), (0, 1, 8, 2), (0, 1, 9, 1)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 0, 8, 2), (0, 0, 9, 1), (0, 0, 9, 3), (0, 0, 8, 1), (0, 0, 8, 3)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 0, 9, 1), (0, 0, 9, 3), (0, 0, 9, 2)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 0, 8, 2), (0, 0, 9, 3), (0, 0, 8, 3), (0, 0, 9, 2)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          ...,\n",
       "          [([(0, 0, 8, 7), (0, 0, 9, 6), (0, 0, 9, 8), (0, 0, 8, 6), (0, 0, 8, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 0, 9, 6), (0, 0, 9, 8), (0, 0, 9, 7)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 0, 8, 7), (0, 0, 9, 8), (0, 0, 8, 8), (0, 0, 9, 7)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 0, 8, 8), (0, 0, 9, 7), (0, 0, 9, 9), (0, 0, 8, 7), (0, 0, 8, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 0, 9, 7), (0, 0, 9, 9), (0, 0, 9, 8)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 0, 8, 8), (0, 0, 9, 9), (0, 0, 8, 9), (0, 0, 9, 8)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 0, 8, 9), (0, 0, 9, 8), (0, 0, 8, 8), (0, 0, 9, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 0, 9, 8), (0, 0, 9, 9)], [0.024999999999999994, 0.975]),\n",
       "           ([(0, 0, 8, 9), (0, 0, 9, 9)], [0.024999999999999994, 0.975]),\n",
       "           ..., None, None, None]]],\n",
       "\n",
       "\n",
       "        [[[([(0, 2, 0, 1), (0, 2, 0, 0)], [0.024999999999999994, 0.975]),\n",
       "           ([(0, 2, 1, 0), (0, 2, 0, 1), (0, 2, 1, 1), (0, 2, 0, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 1, 0), (0, 2, 0, 1), (0, 2, 1, 1), (0, 2, 0, 0)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 0, 0), (0, 2, 0, 2), (0, 2, 0, 1)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 2, 1, 1), (0, 2, 0, 0), (0, 2, 0, 2), (0, 2, 1, 0), (0, 2, 1, 2)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 1, 1), (0, 2, 0, 2), (0, 2, 1, 2), (0, 2, 0, 1)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 1, 0, 1), (0, 1, 0, 3), (0, 1, 0, 2)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 1, 1, 2), (0, 1, 0, 1), (0, 1, 0, 3), (0, 1, 1, 1), (0, 1, 1, 3)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 1, 1, 2), (0, 1, 0, 3), (0, 1, 1, 3), (0, 1, 0, 2)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          ...,\n",
       "          [([(0, 1, 0, 6), (0, 1, 0, 8), (0, 1, 0, 7)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 1, 1, 7), (0, 1, 0, 6), (0, 1, 0, 8), (0, 1, 1, 6), (0, 1, 1, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 1, 1, 7), (0, 1, 0, 8), (0, 1, 1, 8), (0, 1, 0, 7)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 0, 0, 7), (0, 0, 0, 9), (0, 0, 0, 8)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 0, 1, 8), (0, 0, 0, 7), (0, 0, 0, 9), (0, 0, 1, 7), (0, 0, 1, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 0, 1, 8), (0, 0, 0, 9), (0, 0, 1, 9), (0, 0, 0, 8)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 0, 0, 8), (0, 0, 0, 9)], [0.024999999999999994, 0.975]),\n",
       "           ([(0, 0, 1, 9), (0, 0, 0, 8), (0, 0, 1, 8), (0, 0, 0, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 0, 1, 9), (0, 0, 0, 9)], [0.024999999999999994, 0.975]),\n",
       "           ..., None, None, None]],\n",
       "\n",
       "         [[([(0, 2, 0, 0), (0, 2, 1, 1), (0, 2, 0, 1), (0, 2, 1, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 2, 0), (0, 2, 1, 1), (0, 2, 2, 1), (0, 2, 1, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 2, 0), (0, 2, 0, 0), (0, 2, 1, 1), (0, 2, 2, 1), (0, 2, 0, 1)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 0, 1), (0, 2, 1, 0), (0, 2, 1, 2), (0, 2, 0, 0), (0, 2, 0, 2)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 2, 1), (0, 2, 1, 0), (0, 2, 1, 2), (0, 2, 2, 0), (0, 2, 1, 1)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ([(0, 2, 2, 1), (0, 2, 0, 1), (0, 2, 1, 2), (0, 2, 0, 2), (0, 2, 1, 1)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.025000000000000022]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 1, 0, 2), (0, 1, 1, 1), (0, 1, 1, 3), (0, 1, 0, 1), (0, 1, 0, 3)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 1, 1, 1), (0, 1, 1, 3), (0, 1, 2, 1), (0, 1, 1, 2)], [0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.925]),\n",
       "           ([(0, 1, 0, 2), (0, 1, 1, 3), (0, 1, 0, 3), (0, 1, 1, 2)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          ...,\n",
       "          [([(0, 1, 0, 7), (0, 1, 1, 6), (0, 1, 1, 8), (0, 1, 0, 6), (0, 1, 0, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 1, 2, 7), (0, 1, 1, 6), (0, 1, 1, 8), (0, 1, 2, 6), (0, 1, 2, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 1, 2, 7), (0, 1, 0, 7), (0, 1, 1, 8), (0, 1, 2, 8), (0, 1, 0, 8)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 0, 0, 8), (0, 0, 1, 7), (0, 0, 1, 9), (0, 0, 0, 7), (0, 0, 0, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 0, 2, 8), (0, 0, 1, 7), (0, 0, 1, 9), (0, 0, 2, 7), (0, 0, 2, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 0, 2, 8), (0, 0, 0, 8), (0, 0, 1, 9), (0, 0, 2, 9), (0, 0, 0, 9)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 0, 0, 9), (0, 0, 1, 8), (0, 0, 0, 8), (0, 0, 1, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 0, 2, 9), (0, 0, 1, 8), (0, 0, 2, 8), (0, 0, 1, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 0, 2, 9), (0, 0, 0, 9), (0, 0, 1, 9)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ..., None, None, None]],\n",
       "\n",
       "         [[([(0, 1, 1, 0), (0, 1, 2, 1), (0, 1, 1, 1), (0, 1, 2, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 1, 3, 0), (0, 1, 2, 1), (0, 1, 3, 1), (0, 1, 2, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 1, 3, 0), (0, 1, 1, 0), (0, 1, 2, 1), (0, 1, 3, 1), (0, 1, 1, 1)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 1, 1, 1), (0, 1, 2, 0), (0, 1, 1, 0), (0, 1, 1, 2), (0, 1, 2, 1)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ([(0, 1, 3, 1), (0, 1, 2, 0), (0, 1, 3, 0), (0, 1, 2, 1)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 1, 3, 1), (0, 1, 1, 1), (0, 1, 1, 2), (0, 1, 2, 1)], [0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.925]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 1, 2, 2)], array([1.])),\n",
       "           ([(0, 1, 2, 2)], array([1.])),\n",
       "           ([(0, 1, 2, 2)], array([1.])), ..., None, None, None],\n",
       "          ...,\n",
       "          [([(0, 1, 1, 7), (0, 1, 2, 6), (0, 1, 2, 8), (0, 1, 1, 6), (0, 1, 1, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 1, 2, 6), (0, 1, 2, 8), (0, 1, 3, 6), (0, 1, 3, 8), (0, 1, 2, 7)], [0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.9]),\n",
       "           ([(0, 1, 1, 7), (0, 1, 2, 8), (0, 1, 3, 8), (0, 1, 1, 8), (0, 1, 2, 7)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 1, 1, 8), (0, 1, 2, 7), (0, 1, 2, 9), (0, 1, 1, 7), (0, 1, 1, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 1, 3, 8), (0, 1, 2, 7), (0, 1, 2, 9), (0, 1, 3, 9), (0, 1, 2, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ([(0, 1, 3, 8), (0, 1, 1, 8), (0, 1, 2, 9), (0, 1, 3, 9), (0, 1, 1, 9)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 1, 1, 9), (0, 1, 2, 8), (0, 1, 1, 8), (0, 1, 2, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 1, 3, 9), (0, 1, 2, 8), (0, 1, 3, 8), (0, 1, 2, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 1, 3, 9), (0, 1, 1, 9), (0, 1, 2, 9)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ..., None, None, None]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[([(0, 1, 6, 0), (0, 1, 7, 1), (0, 1, 6, 1), (0, 1, 7, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 1, 8, 0), (0, 1, 7, 1), (0, 1, 8, 1), (0, 1, 7, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 1, 8, 0), (0, 1, 6, 0), (0, 1, 7, 1), (0, 1, 8, 1), (0, 1, 6, 1)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 1, 6, 1), (0, 1, 7, 0), (0, 1, 7, 2), (0, 1, 6, 0), (0, 1, 7, 1)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ([(0, 1, 8, 1), (0, 1, 7, 0), (0, 1, 7, 2), (0, 1, 8, 0), (0, 1, 8, 2)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 1, 8, 1), (0, 1, 6, 1), (0, 1, 7, 2), (0, 1, 8, 2), (0, 1, 7, 1)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.025000000000000022]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 1, 7, 1), (0, 1, 7, 3), (0, 1, 6, 1), (0, 1, 7, 2)], [0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.925]),\n",
       "           ([(0, 1, 8, 2), (0, 1, 7, 1), (0, 1, 7, 3), (0, 1, 8, 1), (0, 1, 8, 3)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 1, 8, 2), (0, 1, 7, 3), (0, 1, 8, 3), (0, 1, 7, 2)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          ...,\n",
       "          [([(0, 1, 7, 7)], array([1.])),\n",
       "           ([(0, 1, 7, 7)], array([1.])),\n",
       "           ([(0, 1, 7, 7)], array([1.])), ..., None, None, None],\n",
       "          [([(0, 1, 6, 8), (0, 1, 7, 9), (0, 1, 6, 9), (0, 1, 7, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 1, 8, 8), (0, 1, 7, 9), (0, 1, 8, 7), (0, 1, 8, 9), (0, 1, 7, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ([(0, 1, 8, 8), (0, 1, 6, 8), (0, 1, 7, 9), (0, 1, 8, 9), (0, 1, 6, 9)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 1, 6, 9), (0, 1, 7, 8), (0, 1, 6, 8), (0, 1, 7, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 1, 8, 9), (0, 1, 7, 8), (0, 1, 8, 8), (0, 1, 7, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 1, 8, 9), (0, 1, 6, 9), (0, 1, 7, 9)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ..., None, None, None]],\n",
       "\n",
       "         [[([(0, 1, 7, 0), (0, 1, 8, 1), (0, 1, 7, 1), (0, 1, 8, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 1, 9, 0), (0, 1, 8, 1), (0, 1, 9, 1), (0, 1, 8, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 1, 9, 0), (0, 1, 7, 0), (0, 1, 8, 1), (0, 1, 9, 1), (0, 1, 7, 1)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 1, 7, 1), (0, 1, 8, 0), (0, 1, 8, 2), (0, 1, 7, 0), (0, 1, 7, 2)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 1, 9, 1), (0, 1, 8, 0), (0, 1, 8, 2), (0, 1, 9, 0), (0, 1, 9, 2)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 1, 9, 1), (0, 1, 7, 1), (0, 1, 8, 2), (0, 1, 9, 2), (0, 1, 7, 2)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 1, 7, 2), (0, 1, 8, 1), (0, 1, 8, 3), (0, 1, 7, 1), (0, 1, 7, 3)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 1, 9, 2), (0, 1, 8, 1), (0, 1, 8, 3), (0, 1, 9, 1), (0, 1, 9, 3)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 1, 9, 2), (0, 1, 7, 2), (0, 1, 8, 3), (0, 1, 9, 3), (0, 1, 7, 3)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          ...,\n",
       "          [([(0, 1, 8, 6), (0, 1, 8, 8), (0, 1, 7, 8), (0, 1, 8, 7)], [0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.925]),\n",
       "           ([(0, 1, 9, 7), (0, 1, 8, 6), (0, 1, 8, 8), (0, 1, 9, 6), (0, 1, 9, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 1, 9, 7), (0, 1, 8, 8), (0, 1, 9, 8), (0, 1, 7, 8), (0, 1, 8, 7)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 1, 7, 8), (0, 1, 8, 7), (0, 1, 8, 9), (0, 1, 7, 9), (0, 1, 8, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ([(0, 1, 9, 8), (0, 1, 8, 7), (0, 1, 8, 9), (0, 1, 9, 7), (0, 1, 9, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 1, 9, 8), (0, 1, 7, 8), (0, 1, 8, 9), (0, 1, 9, 9), (0, 1, 7, 9)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 1, 7, 9), (0, 1, 8, 8), (0, 1, 7, 8), (0, 1, 8, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 1, 9, 9), (0, 1, 8, 8), (0, 1, 9, 8), (0, 1, 8, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 1, 9, 9), (0, 1, 7, 9), (0, 1, 8, 9)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ..., None, None, None]],\n",
       "\n",
       "         [[([(0, 1, 8, 0), (0, 1, 9, 1), (0, 1, 8, 1), (0, 1, 9, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 1, 9, 1), (0, 1, 9, 0)], [0.024999999999999994, 0.975]),\n",
       "           ([(0, 1, 8, 0), (0, 1, 9, 1), (0, 1, 8, 1), (0, 1, 9, 0)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 1, 8, 1), (0, 1, 9, 0), (0, 1, 9, 2), (0, 1, 8, 0), (0, 1, 8, 2)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 1, 9, 0), (0, 1, 9, 2), (0, 1, 9, 1)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 1, 8, 1), (0, 1, 9, 2), (0, 1, 8, 2), (0, 1, 9, 1)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 1, 8, 2), (0, 1, 9, 1), (0, 1, 9, 3), (0, 1, 8, 1), (0, 1, 8, 3)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 1, 9, 1), (0, 1, 9, 3), (0, 1, 9, 2)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 1, 8, 2), (0, 1, 9, 3), (0, 1, 8, 3), (0, 1, 9, 2)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          ...,\n",
       "          [([(0, 1, 8, 7), (0, 1, 9, 6), (0, 1, 9, 8), (0, 1, 8, 6), (0, 1, 8, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 1, 9, 6), (0, 1, 9, 8), (0, 1, 9, 7)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 1, 8, 7), (0, 1, 9, 8), (0, 1, 8, 8), (0, 1, 9, 7)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 1, 8, 8), (0, 1, 9, 7), (0, 1, 9, 9), (0, 1, 8, 7), (0, 1, 8, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 1, 9, 7), (0, 1, 9, 9), (0, 1, 9, 8)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 1, 8, 8), (0, 1, 9, 9), (0, 1, 8, 9), (0, 1, 9, 8)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 1, 8, 9), (0, 1, 9, 8), (0, 1, 8, 8), (0, 1, 9, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 1, 9, 8), (0, 1, 9, 9)], [0.024999999999999994, 0.975]),\n",
       "           ([(0, 1, 8, 9), (0, 1, 9, 9)], [0.024999999999999994, 0.975]),\n",
       "           ..., None, None, None]]],\n",
       "\n",
       "\n",
       "        [[[([(0, 2, 0, 1), (0, 2, 0, 0)], [0.024999999999999994, 0.975]),\n",
       "           ([(0, 2, 1, 0), (0, 2, 0, 1), (0, 2, 1, 1), (0, 2, 0, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 1, 0), (0, 2, 0, 1), (0, 2, 1, 1), (0, 2, 0, 0)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 0, 0), (0, 2, 0, 2), (0, 2, 0, 1)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 2, 1, 1), (0, 2, 0, 0), (0, 2, 0, 2), (0, 2, 1, 0), (0, 2, 1, 2)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 1, 1), (0, 2, 0, 2), (0, 2, 1, 2), (0, 2, 0, 1)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 0, 1), (0, 2, 0, 3), (0, 2, 0, 2)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 2, 1, 2), (0, 2, 0, 1), (0, 2, 0, 3), (0, 2, 1, 1), (0, 2, 1, 3)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 1, 2), (0, 2, 0, 3), (0, 2, 1, 3), (0, 2, 0, 2)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          ...,\n",
       "          [([(0, 2, 0, 6), (0, 2, 0, 8), (0, 2, 0, 7)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 2, 1, 7), (0, 2, 0, 6), (0, 2, 0, 8), (0, 2, 1, 6), (0, 2, 1, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 1, 7), (0, 2, 0, 8), (0, 2, 1, 8), (0, 2, 0, 7)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 0, 7), (0, 2, 0, 9), (0, 2, 0, 8)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 2, 1, 8), (0, 2, 0, 7), (0, 2, 0, 9), (0, 2, 1, 7), (0, 2, 1, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 1, 8), (0, 2, 0, 9), (0, 2, 1, 9), (0, 2, 0, 8)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 0, 8), (0, 2, 0, 9)], [0.024999999999999994, 0.975]),\n",
       "           ([(0, 2, 1, 9), (0, 2, 0, 8), (0, 2, 1, 8), (0, 2, 0, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 1, 9), (0, 2, 0, 9)], [0.024999999999999994, 0.975]),\n",
       "           ..., None, None, None]],\n",
       "\n",
       "         [[([(0, 2, 0, 0), (0, 2, 1, 1), (0, 2, 0, 1), (0, 2, 1, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 2, 0), (0, 2, 1, 1), (0, 2, 2, 1), (0, 2, 1, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 2, 0), (0, 2, 0, 0), (0, 2, 1, 1), (0, 2, 2, 1), (0, 2, 0, 1)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 0, 1), (0, 2, 1, 0), (0, 2, 1, 2), (0, 2, 0, 0), (0, 2, 0, 2)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 2, 1), (0, 2, 1, 0), (0, 2, 1, 2), (0, 2, 2, 0), (0, 2, 1, 1)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ([(0, 2, 2, 1), (0, 2, 0, 1), (0, 2, 1, 2), (0, 2, 0, 2), (0, 2, 1, 1)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.025000000000000022]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 0, 2), (0, 2, 1, 1), (0, 2, 1, 3), (0, 2, 0, 1), (0, 2, 0, 3)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 1, 1), (0, 2, 1, 3), (0, 2, 2, 1), (0, 2, 1, 2)], [0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.925]),\n",
       "           ([(0, 2, 0, 2), (0, 2, 1, 3), (0, 2, 0, 3), (0, 2, 1, 2)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          ...,\n",
       "          [([(0, 2, 0, 7), (0, 2, 1, 6), (0, 2, 1, 8), (0, 2, 0, 6), (0, 2, 0, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 2, 7), (0, 2, 1, 6), (0, 2, 1, 8), (0, 2, 2, 6), (0, 2, 2, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 2, 7), (0, 2, 0, 7), (0, 2, 1, 8), (0, 2, 2, 8), (0, 2, 0, 8)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 0, 8), (0, 2, 1, 7), (0, 2, 1, 9), (0, 2, 0, 7), (0, 2, 0, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 2, 8), (0, 2, 1, 7), (0, 2, 1, 9), (0, 2, 2, 7), (0, 2, 2, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 2, 8), (0, 2, 0, 8), (0, 2, 1, 9), (0, 2, 2, 9), (0, 2, 0, 9)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 0, 9), (0, 2, 1, 8), (0, 2, 0, 8), (0, 2, 1, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 2, 9), (0, 2, 1, 8), (0, 2, 2, 8), (0, 2, 1, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 2, 9), (0, 2, 0, 9), (0, 2, 1, 9)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ..., None, None, None]],\n",
       "\n",
       "         [[([(0, 2, 1, 0), (0, 2, 2, 1), (0, 2, 1, 1), (0, 2, 2, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 3, 0), (0, 2, 2, 1), (0, 2, 3, 1), (0, 2, 2, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 3, 0), (0, 2, 1, 0), (0, 2, 2, 1), (0, 2, 3, 1), (0, 2, 1, 1)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 1, 1), (0, 2, 2, 0), (0, 2, 1, 0), (0, 2, 1, 2), (0, 2, 2, 1)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ([(0, 2, 3, 1), (0, 2, 2, 0), (0, 2, 3, 0), (0, 2, 2, 1)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 3, 1), (0, 2, 1, 1), (0, 2, 1, 2), (0, 2, 2, 1)], [0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.925]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 2, 2)], array([1.])),\n",
       "           ([(0, 2, 2, 2)], array([1.])),\n",
       "           ([(0, 2, 2, 2)], array([1.])), ..., None, None, None],\n",
       "          ...,\n",
       "          [([(0, 2, 1, 7), (0, 2, 2, 6), (0, 2, 2, 8), (0, 2, 1, 6), (0, 2, 1, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 2, 6), (0, 2, 2, 8), (0, 2, 3, 6), (0, 2, 3, 8), (0, 2, 2, 7)], [0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.9]),\n",
       "           ([(0, 2, 1, 7), (0, 2, 2, 8), (0, 2, 3, 8), (0, 2, 1, 8), (0, 2, 2, 7)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 1, 8), (0, 2, 2, 7), (0, 2, 2, 9), (0, 2, 1, 7), (0, 2, 1, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 3, 8), (0, 2, 2, 7), (0, 2, 2, 9), (0, 2, 3, 9), (0, 2, 2, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ([(0, 2, 3, 8), (0, 2, 1, 8), (0, 2, 2, 9), (0, 2, 3, 9), (0, 2, 1, 9)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 1, 9), (0, 2, 2, 8), (0, 2, 1, 8), (0, 2, 2, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 3, 9), (0, 2, 2, 8), (0, 2, 3, 8), (0, 2, 2, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 3, 9), (0, 2, 1, 9), (0, 2, 2, 9)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ..., None, None, None]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[([(0, 2, 6, 0), (0, 2, 7, 1), (0, 2, 6, 1), (0, 2, 7, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 8, 0), (0, 2, 7, 1), (0, 2, 8, 1), (0, 2, 7, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 8, 0), (0, 2, 6, 0), (0, 2, 7, 1), (0, 2, 8, 1), (0, 2, 6, 1)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 6, 1), (0, 2, 7, 0), (0, 2, 7, 2), (0, 2, 6, 0), (0, 2, 7, 1)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ([(0, 2, 8, 1), (0, 2, 7, 0), (0, 2, 7, 2), (0, 2, 8, 0), (0, 2, 8, 2)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 8, 1), (0, 2, 6, 1), (0, 2, 7, 2), (0, 2, 8, 2), (0, 2, 7, 1)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.025000000000000022]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 7, 1), (0, 2, 7, 3), (0, 2, 6, 1), (0, 2, 7, 2)], [0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.925]),\n",
       "           ([(0, 2, 8, 2), (0, 2, 7, 1), (0, 2, 7, 3), (0, 2, 8, 1), (0, 2, 8, 3)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 8, 2), (0, 2, 7, 3), (0, 2, 8, 3), (0, 2, 7, 2)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          ...,\n",
       "          [([(0, 2, 7, 7)], array([1.])),\n",
       "           ([(0, 2, 7, 7)], array([1.])),\n",
       "           ([(0, 2, 7, 7)], array([1.])), ..., None, None, None],\n",
       "          [([(0, 2, 6, 8), (0, 2, 7, 9), (0, 2, 6, 9), (0, 2, 7, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 8, 8), (0, 2, 7, 9), (0, 2, 8, 7), (0, 2, 8, 9), (0, 2, 7, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ([(0, 2, 8, 8), (0, 2, 6, 8), (0, 2, 7, 9), (0, 2, 8, 9), (0, 2, 6, 9)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 6, 9), (0, 2, 7, 8), (0, 2, 6, 8), (0, 2, 7, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 8, 9), (0, 2, 7, 8), (0, 2, 8, 8), (0, 2, 7, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 8, 9), (0, 2, 6, 9), (0, 2, 7, 9)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ..., None, None, None]],\n",
       "\n",
       "         [[([(0, 2, 7, 0), (0, 2, 8, 1), (0, 2, 7, 1), (0, 2, 8, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 9, 0), (0, 2, 8, 1), (0, 2, 9, 1), (0, 2, 8, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 9, 0), (0, 2, 7, 0), (0, 2, 8, 1), (0, 2, 9, 1), (0, 2, 7, 1)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 7, 1), (0, 2, 8, 0), (0, 2, 8, 2), (0, 2, 7, 0), (0, 2, 7, 2)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 9, 1), (0, 2, 8, 0), (0, 2, 8, 2), (0, 2, 9, 0), (0, 2, 9, 2)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 9, 1), (0, 2, 7, 1), (0, 2, 8, 2), (0, 2, 9, 2), (0, 2, 7, 2)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 7, 2), (0, 2, 8, 1), (0, 2, 8, 3), (0, 2, 7, 1), (0, 2, 7, 3)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 9, 2), (0, 2, 8, 1), (0, 2, 8, 3), (0, 2, 9, 1), (0, 2, 9, 3)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 9, 2), (0, 2, 7, 2), (0, 2, 8, 3), (0, 2, 9, 3), (0, 2, 7, 3)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          ...,\n",
       "          [([(0, 2, 8, 6), (0, 2, 8, 8), (0, 2, 7, 8), (0, 2, 8, 7)], [0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.925]),\n",
       "           ([(0, 2, 9, 7), (0, 2, 8, 6), (0, 2, 8, 8), (0, 2, 9, 6), (0, 2, 9, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 9, 7), (0, 2, 8, 8), (0, 2, 9, 8), (0, 2, 7, 8), (0, 2, 8, 7)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 7, 8), (0, 2, 8, 7), (0, 2, 8, 9), (0, 2, 7, 9), (0, 2, 8, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.02499999999999991]),\n",
       "           ([(0, 2, 9, 8), (0, 2, 8, 7), (0, 2, 8, 9), (0, 2, 9, 7), (0, 2, 9, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 9, 8), (0, 2, 7, 8), (0, 2, 8, 9), (0, 2, 9, 9), (0, 2, 7, 9)], [0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 7, 9), (0, 2, 8, 8), (0, 2, 7, 8), (0, 2, 8, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 9, 9), (0, 2, 8, 8), (0, 2, 9, 8), (0, 2, 8, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 9, 9), (0, 2, 7, 9), (0, 2, 8, 9)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ..., None, None, None]],\n",
       "\n",
       "         [[([(0, 2, 8, 0), (0, 2, 9, 1), (0, 2, 8, 1), (0, 2, 9, 0)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 9, 1), (0, 2, 9, 0)], [0.024999999999999994, 0.975]),\n",
       "           ([(0, 2, 8, 0), (0, 2, 9, 1), (0, 2, 8, 1), (0, 2, 9, 0)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 8, 1), (0, 2, 9, 0), (0, 2, 9, 2), (0, 2, 8, 0), (0, 2, 8, 2)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 9, 0), (0, 2, 9, 2), (0, 2, 9, 1)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 2, 8, 1), (0, 2, 9, 2), (0, 2, 8, 2), (0, 2, 9, 1)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 8, 2), (0, 2, 9, 1), (0, 2, 9, 3), (0, 2, 8, 1), (0, 2, 8, 3)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 9, 1), (0, 2, 9, 3), (0, 2, 9, 2)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 2, 8, 2), (0, 2, 9, 3), (0, 2, 8, 3), (0, 2, 9, 2)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          ...,\n",
       "          [([(0, 2, 8, 7), (0, 2, 9, 6), (0, 2, 9, 8), (0, 2, 8, 6), (0, 2, 8, 8)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 9, 6), (0, 2, 9, 8), (0, 2, 9, 7)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 2, 8, 7), (0, 2, 9, 8), (0, 2, 8, 8), (0, 2, 9, 7)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 8, 8), (0, 2, 9, 7), (0, 2, 9, 9), (0, 2, 8, 7), (0, 2, 8, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]),\n",
       "           ([(0, 2, 9, 7), (0, 2, 9, 9), (0, 2, 9, 8)], [0.024999999999999994, 0.024999999999999994, 0.95]),\n",
       "           ([(0, 2, 8, 8), (0, 2, 9, 9), (0, 2, 8, 9), (0, 2, 9, 8)], [0.024999999999999994, 0.9, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ..., None, None, None],\n",
       "          [([(0, 2, 8, 9), (0, 2, 9, 8), (0, 2, 8, 8), (0, 2, 9, 9)], [0.9, 0.024999999999999994, 0.024999999999999994, 0.04999999999999993]),\n",
       "           ([(0, 2, 9, 8), (0, 2, 9, 9)], [0.024999999999999994, 0.975]),\n",
       "           ([(0, 2, 8, 9), (0, 2, 9, 9)], [0.024999999999999994, 0.975]),\n",
       "           ..., None, None, None]]]]], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csrl.transition_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee8ea50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START state: (0, 0, 3, 8)\n",
      "episode: 0/25000, steps: 52, e: 1.0\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1/25000, steps: 100, e: 1.0\n",
      "accumulated_rewards_per_episode: -1.0000000000000007\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 2/25000, steps: 64, e: 1.0\n",
      "accumulated_rewards_per_episode: -0.6400000000000003\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3/25000, steps: 52, e: 1.0\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 4/25000, steps: 52, e: 1.0\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5/25000, steps: 52, e: 1.0\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 6/25000, steps: 110, e: 1.0\n",
      "accumulated_rewards_per_episode: -0.10000000000000052\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 7/25000, steps: 97, e: 1.0\n",
      "accumulated_rewards_per_episode: 0.02999999999999925\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 8/25000, steps: 52, e: 1.0\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 9/25000, steps: 52, e: 1.0\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 10/25000, steps: 80, e: 1.0\n",
      "accumulated_rewards_per_episode: 0.1999999999999993\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 11/25000, steps: 304, e: 0.99\n",
      "accumulated_rewards_per_episode: -2.040000000000001\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 12/25000, steps: 61, e: 0.99\n",
      "accumulated_rewards_per_episode: -0.6100000000000003\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 13/25000, steps: 52, e: 0.99\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 14/25000, steps: 52, e: 0.99\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 15/25000, steps: 143, e: 0.99\n",
      "accumulated_rewards_per_episode: -0.4300000000000008\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 16/25000, steps: 52, e: 0.99\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 17/25000, steps: 154, e: 0.99\n",
      "accumulated_rewards_per_episode: -1.5400000000000011\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 18/25000, steps: 61, e: 0.99\n",
      "accumulated_rewards_per_episode: -0.6100000000000003\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 19/25000, steps: 52, e: 0.99\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 20/25000, steps: 57, e: 0.99\n",
      "accumulated_rewards_per_episode: -0.5700000000000003\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 21/25000, steps: 426, e: 0.99\n",
      "accumulated_rewards_per_episode: -3.2599999999999754\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 22/25000, steps: 175, e: 0.99\n",
      "accumulated_rewards_per_episode: -0.7500000000000012\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 23/25000, steps: 75, e: 0.99\n",
      "accumulated_rewards_per_episode: -0.7500000000000004\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 24/25000, steps: 52, e: 0.99\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 25/25000, steps: 52, e: 0.99\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 26/25000, steps: 52, e: 0.99\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 27/25000, steps: 52, e: 0.99\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 28/25000, steps: 215, e: 0.99\n",
      "accumulated_rewards_per_episode: -2.149999999999998\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 29/25000, steps: 52, e: 0.99\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 30/25000, steps: 125, e: 0.99\n",
      "accumulated_rewards_per_episode: -1.2500000000000009\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 31/25000, steps: 52, e: 0.99\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 32/25000, steps: 52, e: 0.98\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 33/25000, steps: 139, e: 0.98\n",
      "accumulated_rewards_per_episode: -1.390000000000001\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 34/25000, steps: 52, e: 0.98\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 35/25000, steps: 95, e: 0.98\n",
      "accumulated_rewards_per_episode: 0.049999999999999475\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 36/25000, steps: 52, e: 0.98\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 37/25000, steps: 86, e: 0.98\n",
      "accumulated_rewards_per_episode: 0.13999999999999935\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 38/25000, steps: 74, e: 0.98\n",
      "accumulated_rewards_per_episode: -0.7400000000000004\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 39/25000, steps: 67, e: 0.98\n",
      "accumulated_rewards_per_episode: -0.6700000000000004\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 40/25000, steps: 52, e: 0.98\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 41/25000, steps: 87, e: 0.98\n",
      "accumulated_rewards_per_episode: -0.8700000000000006\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 42/25000, steps: 52, e: 0.98\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 43/25000, steps: 116, e: 0.98\n",
      "accumulated_rewards_per_episode: -1.1600000000000008\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 44/25000, steps: 52, e: 0.98\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 45/25000, steps: 52, e: 0.98\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 46/25000, steps: 52, e: 0.98\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 47/25000, steps: 52, e: 0.98\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 48/25000, steps: 52, e: 0.98\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 49/25000, steps: 52, e: 0.98\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 50/25000, steps: 52, e: 0.98\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 51/25000, steps: 304, e: 0.98\n",
      "accumulated_rewards_per_episode: -2.0400000000000014\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 52/25000, steps: 201, e: 0.98\n",
      "accumulated_rewards_per_episode: -2.010000000000001\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 53/25000, steps: 52, e: 0.97\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 54/25000, steps: 199, e: 0.97\n",
      "accumulated_rewards_per_episode: -1.9900000000000015\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 55/25000, steps: 52, e: 0.97\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 56/25000, steps: 52, e: 0.97\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 57/25000, steps: 323, e: 0.97\n",
      "accumulated_rewards_per_episode: -2.229999999999997\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 58/25000, steps: 185, e: 0.97\n",
      "accumulated_rewards_per_episode: -0.8500000000000014\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 59/25000, steps: 141, e: 0.97\n",
      "accumulated_rewards_per_episode: 0.5899999999999992\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 60/25000, steps: 103, e: 0.97\n",
      "accumulated_rewards_per_episode: 0.9699999999999991\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 61/25000, steps: 52, e: 0.97\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 62/25000, steps: 52, e: 0.97\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 63/25000, steps: 99, e: 0.97\n",
      "accumulated_rewards_per_episode: 0.009999999999999247\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 64/25000, steps: 78, e: 0.97\n",
      "accumulated_rewards_per_episode: -0.7800000000000005\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 65/25000, steps: 507, e: 0.97\n",
      "accumulated_rewards_per_episode: -4.0699999999999585\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 66/25000, steps: 52, e: 0.97\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 67/25000, steps: 252, e: 0.97\n",
      "accumulated_rewards_per_episode: -2.5199999999999902\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 68/25000, steps: 52, e: 0.97\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 69/25000, steps: 52, e: 0.97\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 70/25000, steps: 316, e: 0.97\n",
      "accumulated_rewards_per_episode: -2.159999999999977\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 71/25000, steps: 52, e: 0.97\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 72/25000, steps: 63, e: 0.97\n",
      "accumulated_rewards_per_episode: -0.6300000000000003\n",
      "START state: (0, 0, 6, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 73/25000, steps: 66, e: 0.97\n",
      "accumulated_rewards_per_episode: 0.3399999999999995\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 74/25000, steps: 52, e: 0.97\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 75/25000, steps: 52, e: 0.96\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 76/25000, steps: 52, e: 0.96\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 77/25000, steps: 110, e: 0.96\n",
      "accumulated_rewards_per_episode: -1.1000000000000008\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 78/25000, steps: 52, e: 0.96\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 79/25000, steps: 109, e: 0.96\n",
      "accumulated_rewards_per_episode: -1.0900000000000007\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 80/25000, steps: 52, e: 0.96\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 81/25000, steps: 151, e: 0.96\n",
      "accumulated_rewards_per_episode: -0.510000000000001\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 82/25000, steps: 52, e: 0.96\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 83/25000, steps: 52, e: 0.96\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 84/25000, steps: 52, e: 0.96\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 85/25000, steps: 69, e: 0.96\n",
      "accumulated_rewards_per_episode: 0.3099999999999995\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 86/25000, steps: 666, e: 0.96\n",
      "accumulated_rewards_per_episode: -5.659999999999924\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 87/25000, steps: 52, e: 0.96\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 88/25000, steps: 52, e: 0.96\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 89/25000, steps: 52, e: 0.96\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 90/25000, steps: 52, e: 0.96\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 91/25000, steps: 52, e: 0.96\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 92/25000, steps: 74, e: 0.96\n",
      "accumulated_rewards_per_episode: 0.25999999999999934\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 93/25000, steps: 52, e: 0.96\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 94/25000, steps: 52, e: 0.96\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 95/25000, steps: 513, e: 0.96\n",
      "accumulated_rewards_per_episode: -3.1299999999999573\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 96/25000, steps: 52, e: 0.95\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 97/25000, steps: 102, e: 0.95\n",
      "accumulated_rewards_per_episode: -1.0200000000000007\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 98/25000, steps: 186, e: 0.95\n",
      "accumulated_rewards_per_episode: -1.8600000000000014\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 99/25000, steps: 52, e: 0.95\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 100/25000, steps: 95, e: 0.95\n",
      "accumulated_rewards_per_episode: 0.04999999999999942\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 101/25000, steps: 52, e: 0.95\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 102/25000, steps: 54, e: 0.95\n",
      "accumulated_rewards_per_episode: 0.45999999999999963\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 103/25000, steps: 73, e: 0.95\n",
      "accumulated_rewards_per_episode: 0.26999999999999946\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 104/25000, steps: 366, e: 0.95\n",
      "accumulated_rewards_per_episode: -2.659999999999988\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 105/25000, steps: 52, e: 0.95\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 106/25000, steps: 52, e: 0.95\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 107/25000, steps: 52, e: 0.95\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 108/25000, steps: 66, e: 0.95\n",
      "accumulated_rewards_per_episode: 0.3399999999999995\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 109/25000, steps: 52, e: 0.95\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 110/25000, steps: 76, e: 0.95\n",
      "accumulated_rewards_per_episode: 0.23999999999999932\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 111/25000, steps: 52, e: 0.95\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 112/25000, steps: 100, e: 0.95\n",
      "accumulated_rewards_per_episode: -6.418476861114186e-16\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 113/25000, steps: 52, e: 0.95\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 114/25000, steps: 52, e: 0.95\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 115/25000, steps: 260, e: 0.95\n",
      "accumulated_rewards_per_episode: -1.6000000000000016\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 116/25000, steps: 109, e: 0.95\n",
      "accumulated_rewards_per_episode: -1.0900000000000007\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 117/25000, steps: 52, e: 0.95\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 118/25000, steps: 52, e: 0.94\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 119/25000, steps: 52, e: 0.94\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 120/25000, steps: 52, e: 0.94\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 121/25000, steps: 61, e: 0.94\n",
      "accumulated_rewards_per_episode: 0.38999999999999946\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 122/25000, steps: 52, e: 0.94\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 123/25000, steps: 57, e: 0.94\n",
      "accumulated_rewards_per_episode: -0.5700000000000003\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 124/25000, steps: 52, e: 0.94\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 125/25000, steps: 56, e: 0.94\n",
      "accumulated_rewards_per_episode: -0.5600000000000003\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 126/25000, steps: 71, e: 0.94\n",
      "accumulated_rewards_per_episode: -0.7100000000000004\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 127/25000, steps: 66, e: 0.94\n",
      "accumulated_rewards_per_episode: -0.6600000000000004\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 128/25000, steps: 52, e: 0.94\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 129/25000, steps: 52, e: 0.94\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 130/25000, steps: 293, e: 0.94\n",
      "accumulated_rewards_per_episode: -1.9300000000000022\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 131/25000, steps: 52, e: 0.94\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 132/25000, steps: 52, e: 0.94\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 133/25000, steps: 52, e: 0.94\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 134/25000, steps: 90, e: 0.94\n",
      "accumulated_rewards_per_episode: -0.9000000000000006\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 135/25000, steps: 52, e: 0.94\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 136/25000, steps: 99, e: 0.94\n",
      "accumulated_rewards_per_episode: -0.9900000000000007\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 137/25000, steps: 158, e: 0.94\n",
      "accumulated_rewards_per_episode: -0.580000000000001\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 138/25000, steps: 52, e: 0.94\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 139/25000, steps: 52, e: 0.94\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 140/25000, steps: 52, e: 0.94\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 141/25000, steps: 52, e: 0.93\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 142/25000, steps: 52, e: 0.93\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 143/25000, steps: 229, e: 0.93\n",
      "accumulated_rewards_per_episode: -1.2900000000000018\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 144/25000, steps: 52, e: 0.93\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 145/25000, steps: 52, e: 0.93\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 146/25000, steps: 248, e: 0.93\n",
      "accumulated_rewards_per_episode: -0.4800000000000016\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 147/25000, steps: 52, e: 0.93\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 148/25000, steps: 52, e: 0.93\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 149/25000, steps: 52, e: 0.93\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 150/25000, steps: 52, e: 0.93\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 151/25000, steps: 52, e: 0.93\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 152/25000, steps: 130, e: 0.93\n",
      "accumulated_rewards_per_episode: -0.30000000000000066\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 153/25000, steps: 149, e: 0.93\n",
      "accumulated_rewards_per_episode: -1.490000000000001\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 154/25000, steps: 131, e: 0.93\n",
      "accumulated_rewards_per_episode: -1.310000000000001\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 155/25000, steps: 52, e: 0.93\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 156/25000, steps: 233, e: 0.93\n",
      "accumulated_rewards_per_episode: -1.3300000000000014\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 157/25000, steps: 56, e: 0.93\n",
      "accumulated_rewards_per_episode: -0.5600000000000003\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 158/25000, steps: 52, e: 0.93\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 159/25000, steps: 99, e: 0.93\n",
      "accumulated_rewards_per_episode: 0.00999999999999947\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 160/25000, steps: 72, e: 0.93\n",
      "accumulated_rewards_per_episode: -0.7200000000000004\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 161/25000, steps: 52, e: 0.93\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 162/25000, steps: 52, e: 0.93\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 163/25000, steps: 52, e: 0.92\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 164/25000, steps: 52, e: 0.92\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 165/25000, steps: 800, e: 0.92\n",
      "accumulated_rewards_per_episode: -4.999999999999933\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 166/25000, steps: 100, e: 0.92\n",
      "accumulated_rewards_per_episode: -6.418476861114186e-16\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 167/25000, steps: 217, e: 0.92\n",
      "accumulated_rewards_per_episode: -0.17000000000000118\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 168/25000, steps: 52, e: 0.92\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 169/25000, steps: 52, e: 0.92\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 170/25000, steps: 145, e: 0.92\n",
      "accumulated_rewards_per_episode: -1.450000000000001\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 171/25000, steps: 52, e: 0.92\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 172/25000, steps: 364, e: 0.92\n",
      "accumulated_rewards_per_episode: -1.6399999999999906\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 173/25000, steps: 100, e: 0.92\n",
      "accumulated_rewards_per_episode: -1.0000000000000007\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 174/25000, steps: 67, e: 0.92\n",
      "accumulated_rewards_per_episode: -0.6700000000000004\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 175/25000, steps: 426, e: 0.92\n",
      "accumulated_rewards_per_episode: -4.259999999999954\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 176/25000, steps: 64, e: 0.92\n",
      "accumulated_rewards_per_episode: -0.6400000000000003\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 177/25000, steps: 62, e: 0.92\n",
      "accumulated_rewards_per_episode: -0.6200000000000003\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 178/25000, steps: 52, e: 0.92\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 179/25000, steps: 77, e: 0.92\n",
      "accumulated_rewards_per_episode: -0.7700000000000005\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 180/25000, steps: 100, e: 0.92\n",
      "accumulated_rewards_per_episode: -6.418476861114186e-16\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 181/25000, steps: 52, e: 0.92\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 182/25000, steps: 52, e: 0.92\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 183/25000, steps: 52, e: 0.92\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 184/25000, steps: 79, e: 0.92\n",
      "accumulated_rewards_per_episode: -0.7900000000000005\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 185/25000, steps: 52, e: 0.92\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 186/25000, steps: 52, e: 0.91\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 187/25000, steps: 70, e: 0.91\n",
      "accumulated_rewards_per_episode: -0.7000000000000004\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 188/25000, steps: 112, e: 0.91\n",
      "accumulated_rewards_per_episode: -0.12000000000000062\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 189/25000, steps: 52, e: 0.91\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 190/25000, steps: 61, e: 0.91\n",
      "accumulated_rewards_per_episode: -0.6100000000000003\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 191/25000, steps: 52, e: 0.91\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 192/25000, steps: 52, e: 0.91\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 193/25000, steps: 52, e: 0.91\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 194/25000, steps: 52, e: 0.91\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 195/25000, steps: 304, e: 0.91\n",
      "accumulated_rewards_per_episode: -1.040000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 196/25000, steps: 52, e: 0.91\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 197/25000, steps: 52, e: 0.91\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 198/25000, steps: 276, e: 0.91\n",
      "accumulated_rewards_per_episode: -2.759999999999985\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 199/25000, steps: 82, e: 0.91\n",
      "accumulated_rewards_per_episode: -0.8200000000000005\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 200/25000, steps: 52, e: 0.91\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 201/25000, steps: 52, e: 0.91\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 202/25000, steps: 52, e: 0.91\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 203/25000, steps: 52, e: 0.91\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 204/25000, steps: 52, e: 0.91\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 205/25000, steps: 52, e: 0.91\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 206/25000, steps: 52, e: 0.91\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 207/25000, steps: 52, e: 0.91\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 208/25000, steps: 52, e: 0.9\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 209/25000, steps: 52, e: 0.9\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 210/25000, steps: 52, e: 0.9\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 211/25000, steps: 95, e: 0.9\n",
      "accumulated_rewards_per_episode: -0.9500000000000006\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 212/25000, steps: 52, e: 0.9\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 213/25000, steps: 235, e: 0.9\n",
      "accumulated_rewards_per_episode: -0.35000000000000137\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 214/25000, steps: 67, e: 0.9\n",
      "accumulated_rewards_per_episode: -0.6700000000000004\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 215/25000, steps: 106, e: 0.9\n",
      "accumulated_rewards_per_episode: -0.06000000000000054\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 216/25000, steps: 52, e: 0.9\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 217/25000, steps: 52, e: 0.9\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 218/25000, steps: 362, e: 0.9\n",
      "accumulated_rewards_per_episode: -0.6200000000000022\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 219/25000, steps: 52, e: 0.9\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 220/25000, steps: 106, e: 0.9\n",
      "accumulated_rewards_per_episode: -0.06000000000000054\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 221/25000, steps: 73, e: 0.9\n",
      "accumulated_rewards_per_episode: 0.2699999999999996\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 222/25000, steps: 52, e: 0.9\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 223/25000, steps: 52, e: 0.9\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 224/25000, steps: 52, e: 0.9\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 225/25000, steps: 52, e: 0.9\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 226/25000, steps: 52, e: 0.9\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 227/25000, steps: 87, e: 0.9\n",
      "accumulated_rewards_per_episode: 0.12999999999999923\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 228/25000, steps: 432, e: 0.9\n",
      "accumulated_rewards_per_episode: -3.319999999999974\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 229/25000, steps: 52, e: 0.9\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 230/25000, steps: 233, e: 0.9\n",
      "accumulated_rewards_per_episode: -0.3300000000000016\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 231/25000, steps: 52, e: 0.9\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 232/25000, steps: 113, e: 0.89\n",
      "accumulated_rewards_per_episode: -1.1300000000000008\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 233/25000, steps: 53, e: 0.89\n",
      "accumulated_rewards_per_episode: -0.5300000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 234/25000, steps: 153, e: 0.89\n",
      "accumulated_rewards_per_episode: -1.5300000000000011\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 235/25000, steps: 52, e: 0.89\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 236/25000, steps: 52, e: 0.89\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 237/25000, steps: 52, e: 0.89\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 238/25000, steps: 52, e: 0.89\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 239/25000, steps: 52, e: 0.89\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 240/25000, steps: 52, e: 0.89\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 241/25000, steps: 138, e: 0.89\n",
      "accumulated_rewards_per_episode: -1.380000000000001\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 242/25000, steps: 52, e: 0.89\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 243/25000, steps: 52, e: 0.89\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 244/25000, steps: 52, e: 0.89\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 245/25000, steps: 52, e: 0.89\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 246/25000, steps: 52, e: 0.89\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 247/25000, steps: 64, e: 0.89\n",
      "accumulated_rewards_per_episode: 0.35999999999999954\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 248/25000, steps: 224, e: 0.89\n",
      "accumulated_rewards_per_episode: -0.24000000000000152\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 249/25000, steps: 89, e: 0.89\n",
      "accumulated_rewards_per_episode: 0.10999999999999924\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 250/25000, steps: 52, e: 0.89\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 251/25000, steps: 52, e: 0.89\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 252/25000, steps: 100, e: 0.89\n",
      "accumulated_rewards_per_episode: -1.0000000000000007\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 253/25000, steps: 52, e: 0.89\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 254/25000, steps: 69, e: 0.89\n",
      "accumulated_rewards_per_episode: -0.6900000000000004\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 255/25000, steps: 52, e: 0.88\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 256/25000, steps: 121, e: 0.88\n",
      "accumulated_rewards_per_episode: -0.21000000000000058\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 257/25000, steps: 86, e: 0.88\n",
      "accumulated_rewards_per_episode: -0.8600000000000005\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 258/25000, steps: 295, e: 0.88\n",
      "accumulated_rewards_per_episode: -0.950000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 259/25000, steps: 248, e: 0.88\n",
      "accumulated_rewards_per_episode: -2.479999999999991\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 260/25000, steps: 156, e: 0.88\n",
      "accumulated_rewards_per_episode: -1.5600000000000012\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 261/25000, steps: 59, e: 0.88\n",
      "accumulated_rewards_per_episode: -0.5900000000000003\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 262/25000, steps: 95, e: 0.88\n",
      "accumulated_rewards_per_episode: 0.049999999999999475\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 263/25000, steps: 112, e: 0.88\n",
      "accumulated_rewards_per_episode: -1.1200000000000008\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 264/25000, steps: 120, e: 0.88\n",
      "accumulated_rewards_per_episode: -1.2000000000000008\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 265/25000, steps: 307, e: 0.88\n",
      "accumulated_rewards_per_episode: -1.070000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 266/25000, steps: 52, e: 0.88\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 267/25000, steps: 190, e: 0.88\n",
      "accumulated_rewards_per_episode: -1.9000000000000015\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 268/25000, steps: 138, e: 0.88\n",
      "accumulated_rewards_per_episode: -0.3800000000000008\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 269/25000, steps: 52, e: 0.88\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 270/25000, steps: 52, e: 0.88\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 271/25000, steps: 279, e: 0.88\n",
      "accumulated_rewards_per_episode: -0.7900000000000018\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 272/25000, steps: 52, e: 0.88\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 273/25000, steps: 52, e: 0.88\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 274/25000, steps: 269, e: 0.88\n",
      "accumulated_rewards_per_episode: -1.690000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 275/25000, steps: 126, e: 0.88\n",
      "accumulated_rewards_per_episode: -0.2600000000000007\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 276/25000, steps: 52, e: 0.88\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 277/25000, steps: 52, e: 0.88\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 278/25000, steps: 52, e: 0.88\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 279/25000, steps: 123, e: 0.87\n",
      "accumulated_rewards_per_episode: -0.2300000000000006\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 280/25000, steps: 68, e: 0.87\n",
      "accumulated_rewards_per_episode: -0.6800000000000004\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 281/25000, steps: 157, e: 0.87\n",
      "accumulated_rewards_per_episode: -0.5700000000000008\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 282/25000, steps: 52, e: 0.87\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 283/25000, steps: 52, e: 0.87\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 284/25000, steps: 52, e: 0.87\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 285/25000, steps: 57, e: 0.87\n",
      "accumulated_rewards_per_episode: -0.5700000000000003\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 286/25000, steps: 52, e: 0.87\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 287/25000, steps: 243, e: 0.87\n",
      "accumulated_rewards_per_episode: -1.430000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 288/25000, steps: 183, e: 0.87\n",
      "accumulated_rewards_per_episode: 0.16999999999999882\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 289/25000, steps: 101, e: 0.87\n",
      "accumulated_rewards_per_episode: -0.010000000000000531\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 290/25000, steps: 247, e: 0.87\n",
      "accumulated_rewards_per_episode: -2.4699999999999913\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 291/25000, steps: 122, e: 0.87\n",
      "accumulated_rewards_per_episode: -0.22000000000000058\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 292/25000, steps: 82, e: 0.87\n",
      "accumulated_rewards_per_episode: 0.1799999999999995\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 293/25000, steps: 76, e: 0.87\n",
      "accumulated_rewards_per_episode: 0.23999999999999955\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 294/25000, steps: 56, e: 0.87\n",
      "accumulated_rewards_per_episode: 0.4399999999999995\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 295/25000, steps: 79, e: 0.87\n",
      "accumulated_rewards_per_episode: -0.7900000000000005\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 296/25000, steps: 66, e: 0.87\n",
      "accumulated_rewards_per_episode: -0.6600000000000004\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 297/25000, steps: 52, e: 0.87\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 298/25000, steps: 52, e: 0.87\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 299/25000, steps: 79, e: 0.87\n",
      "accumulated_rewards_per_episode: 0.20999999999999952\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 300/25000, steps: 52, e: 0.87\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 301/25000, steps: 52, e: 0.87\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 302/25000, steps: 97, e: 0.87\n",
      "accumulated_rewards_per_episode: -0.9700000000000006\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 303/25000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 304/25000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 305/25000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 306/25000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 307/25000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 308/25000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 309/25000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 310/25000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 311/25000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 312/25000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 313/25000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 314/25000, steps: 55, e: 0.86\n",
      "accumulated_rewards_per_episode: 0.4499999999999995\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 315/25000, steps: 424, e: 0.86\n",
      "accumulated_rewards_per_episode: -2.2399999999999975\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 316/25000, steps: 58, e: 0.86\n",
      "accumulated_rewards_per_episode: 0.4199999999999997\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 317/25000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 318/25000, steps: 56, e: 0.86\n",
      "accumulated_rewards_per_episode: -0.5600000000000003\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 319/25000, steps: 184, e: 0.86\n",
      "accumulated_rewards_per_episode: -0.8400000000000014\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 320/25000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 321/25000, steps: 63, e: 0.86\n",
      "accumulated_rewards_per_episode: 0.36999999999999966\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 322/25000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 323/25000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 324/25000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 325/25000, steps: 56, e: 0.86\n",
      "accumulated_rewards_per_episode: -0.5600000000000003\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 326/25000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 327/25000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 328/25000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 329/25000, steps: 60, e: 0.85\n",
      "accumulated_rewards_per_episode: -0.6000000000000003\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 330/25000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 331/25000, steps: 169, e: 0.85\n",
      "accumulated_rewards_per_episode: -1.6900000000000013\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 332/25000, steps: 98, e: 0.85\n",
      "accumulated_rewards_per_episode: 0.01999999999999936\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 333/25000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 334/25000, steps: 60, e: 0.85\n",
      "accumulated_rewards_per_episode: -0.6000000000000003\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 335/25000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 336/25000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 337/25000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 338/25000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 339/25000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 340/25000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 341/25000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 342/25000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 343/25000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 344/25000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 345/25000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 346/25000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 347/25000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 348/25000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 349/25000, steps: 56, e: 0.85\n",
      "accumulated_rewards_per_episode: -0.5600000000000003\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 350/25000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 351/25000, steps: 52, e: 0.84\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 352/25000, steps: 52, e: 0.84\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 353/25000, steps: 52, e: 0.84\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 354/25000, steps: 125, e: 0.84\n",
      "accumulated_rewards_per_episode: -1.2500000000000009\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 355/25000, steps: 52, e: 0.84\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 356/25000, steps: 52, e: 0.84\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 357/25000, steps: 52, e: 0.84\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 358/25000, steps: 52, e: 0.84\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 359/25000, steps: 122, e: 0.84\n",
      "accumulated_rewards_per_episode: -1.2200000000000009\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 360/25000, steps: 66, e: 0.84\n",
      "accumulated_rewards_per_episode: -0.6600000000000004\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 361/25000, steps: 52, e: 0.84\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 362/25000, steps: 52, e: 0.84\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 363/25000, steps: 147, e: 0.84\n",
      "accumulated_rewards_per_episode: 0.5299999999999991\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 364/25000, steps: 94, e: 0.84\n",
      "accumulated_rewards_per_episode: -0.9400000000000006\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 365/25000, steps: 116, e: 0.84\n",
      "accumulated_rewards_per_episode: -1.1600000000000008\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 366/25000, steps: 57, e: 0.84\n",
      "accumulated_rewards_per_episode: 0.4299999999999996\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 367/25000, steps: 67, e: 0.84\n",
      "accumulated_rewards_per_episode: -0.6700000000000004\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 368/25000, steps: 52, e: 0.84\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 369/25000, steps: 52, e: 0.84\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 370/25000, steps: 77, e: 0.84\n",
      "accumulated_rewards_per_episode: -0.7700000000000005\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 371/25000, steps: 52, e: 0.84\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 372/25000, steps: 52, e: 0.84\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 373/25000, steps: 52, e: 0.84\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 374/25000, steps: 52, e: 0.84\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 375/25000, steps: 52, e: 0.84\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 376/25000, steps: 108, e: 0.83\n",
      "accumulated_rewards_per_episode: -0.08000000000000053\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 377/25000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 378/25000, steps: 56, e: 0.83\n",
      "accumulated_rewards_per_episode: 0.4399999999999995\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 379/25000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 380/25000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 381/25000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 382/25000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 383/25000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 384/25000, steps: 208, e: 0.83\n",
      "accumulated_rewards_per_episode: -1.0800000000000012\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 385/25000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 386/25000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 387/25000, steps: 195, e: 0.83\n",
      "accumulated_rewards_per_episode: -1.9500000000000015\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 388/25000, steps: 189, e: 0.83\n",
      "accumulated_rewards_per_episode: -0.8900000000000015\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 389/25000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 390/25000, steps: 77, e: 0.83\n",
      "accumulated_rewards_per_episode: -0.7700000000000005\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 391/25000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 392/25000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 393/25000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 394/25000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 395/25000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 396/25000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 397/25000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 398/25000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 399/25000, steps: 188, e: 0.83\n",
      "accumulated_rewards_per_episode: -1.8800000000000014\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 400/25000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 401/25000, steps: 52, e: 0.82\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 402/25000, steps: 52, e: 0.82\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 403/25000, steps: 124, e: 0.82\n",
      "accumulated_rewards_per_episode: -0.2400000000000007\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 404/25000, steps: 52, e: 0.82\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 405/25000, steps: 78, e: 0.82\n",
      "accumulated_rewards_per_episode: -0.7800000000000005\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 406/25000, steps: 119, e: 0.82\n",
      "accumulated_rewards_per_episode: -1.1900000000000008\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 407/25000, steps: 52, e: 0.82\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 408/25000, steps: 140, e: 0.82\n",
      "accumulated_rewards_per_episode: -1.400000000000001\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 409/25000, steps: 189, e: 0.82\n",
      "accumulated_rewards_per_episode: -1.8900000000000015\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 410/25000, steps: 52, e: 0.82\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 411/25000, steps: 52, e: 0.82\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 412/25000, steps: 52, e: 0.82\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 413/25000, steps: 180, e: 0.82\n",
      "accumulated_rewards_per_episode: -0.8000000000000012\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 414/25000, steps: 58, e: 0.82\n",
      "accumulated_rewards_per_episode: 0.4199999999999995\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 415/25000, steps: 52, e: 0.82\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 416/25000, steps: 52, e: 0.82\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 417/25000, steps: 59, e: 0.82\n",
      "accumulated_rewards_per_episode: -0.5900000000000003\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 418/25000, steps: 119, e: 0.82\n",
      "accumulated_rewards_per_episode: -0.19000000000000067\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 419/25000, steps: 52, e: 0.82\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 420/25000, steps: 52, e: 0.82\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 421/25000, steps: 92, e: 0.82\n",
      "accumulated_rewards_per_episode: -0.9200000000000006\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 422/25000, steps: 52, e: 0.82\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 423/25000, steps: 128, e: 0.82\n",
      "accumulated_rewards_per_episode: -1.280000000000001\n",
      "START state: (0, 0, 7, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 424/25000, steps: 126, e: 0.82\n",
      "accumulated_rewards_per_episode: -1.260000000000001\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 425/25000, steps: 52, e: 0.82\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 426/25000, steps: 52, e: 0.82\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 427/25000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 428/25000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 429/25000, steps: 318, e: 0.81\n",
      "accumulated_rewards_per_episode: -1.1800000000000022\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 430/25000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 431/25000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 432/25000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 433/25000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 434/25000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 435/25000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 436/25000, steps: 53, e: 0.81\n",
      "accumulated_rewards_per_episode: -0.5300000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 437/25000, steps: 340, e: 0.81\n",
      "accumulated_rewards_per_episode: -3.3999999999999715\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 438/25000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 439/25000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 440/25000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 441/25000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 442/25000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 443/25000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 444/25000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 445/25000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 446/25000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 447/25000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 448/25000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 449/25000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 450/25000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 451/25000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 452/25000, steps: 52, e: 0.8\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 453/25000, steps: 59, e: 0.8\n",
      "accumulated_rewards_per_episode: -0.5900000000000003\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 454/25000, steps: 322, e: 0.8\n",
      "accumulated_rewards_per_episode: -3.2199999999999753\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 455/25000, steps: 97, e: 0.8\n",
      "accumulated_rewards_per_episode: -0.9700000000000006\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 456/25000, steps: 52, e: 0.8\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 457/25000, steps: 52, e: 0.8\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 458/25000, steps: 52, e: 0.8\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 459/25000, steps: 123, e: 0.8\n",
      "accumulated_rewards_per_episode: -1.2300000000000009\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 460/25000, steps: 52, e: 0.8\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 461/25000, steps: 52, e: 0.8\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 462/25000, steps: 52, e: 0.8\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 463/25000, steps: 55, e: 0.8\n",
      "accumulated_rewards_per_episode: -0.5500000000000003\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 464/25000, steps: 66, e: 0.8\n",
      "accumulated_rewards_per_episode: -0.6600000000000004\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 465/25000, steps: 52, e: 0.8\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 466/25000, steps: 72, e: 0.8\n",
      "accumulated_rewards_per_episode: -0.7200000000000004\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 467/25000, steps: 83, e: 0.8\n",
      "accumulated_rewards_per_episode: -0.8300000000000005\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 468/25000, steps: 52, e: 0.8\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 469/25000, steps: 52, e: 0.8\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 470/25000, steps: 54, e: 0.8\n",
      "accumulated_rewards_per_episode: 0.45999999999999963\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 471/25000, steps: 52, e: 0.8\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 472/25000, steps: 52, e: 0.8\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 473/25000, steps: 60, e: 0.8\n",
      "accumulated_rewards_per_episode: -0.6000000000000003\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 474/25000, steps: 89, e: 0.8\n",
      "accumulated_rewards_per_episode: -0.8900000000000006\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 475/25000, steps: 91, e: 0.8\n",
      "accumulated_rewards_per_episode: -0.9100000000000006\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 476/25000, steps: 52, e: 0.8\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 477/25000, steps: 79, e: 0.8\n",
      "accumulated_rewards_per_episode: -0.7900000000000005\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 478/25000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 479/25000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 480/25000, steps: 56, e: 0.79\n",
      "accumulated_rewards_per_episode: -0.5600000000000003\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 481/25000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 482/25000, steps: 118, e: 0.79\n",
      "accumulated_rewards_per_episode: -1.1800000000000008\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 483/25000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 484/25000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 485/25000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 486/25000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 487/25000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 488/25000, steps: 82, e: 0.79\n",
      "accumulated_rewards_per_episode: 0.1799999999999995\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 489/25000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 490/25000, steps: 223, e: 0.79\n",
      "accumulated_rewards_per_episode: -1.2300000000000015\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 491/25000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 492/25000, steps: 305, e: 0.79\n",
      "accumulated_rewards_per_episode: -3.049999999999979\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 493/25000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 494/25000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 495/25000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 496/25000, steps: 141, e: 0.79\n",
      "accumulated_rewards_per_episode: -1.410000000000001\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 497/25000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 498/25000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 499/25000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 500/25000, steps: 317, e: 0.79\n",
      "accumulated_rewards_per_episode: -2.1699999999999777\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 501/25000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 502/25000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 503/25000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 504/25000, steps: 81, e: 0.79\n",
      "accumulated_rewards_per_episode: -0.8100000000000005\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 505/25000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 506/25000, steps: 114, e: 0.78\n",
      "accumulated_rewards_per_episode: -1.1400000000000008\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 507/25000, steps: 228, e: 0.78\n",
      "accumulated_rewards_per_episode: -2.2799999999999954\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 508/25000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 509/25000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 510/25000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 511/25000, steps: 296, e: 0.78\n",
      "accumulated_rewards_per_episode: -2.959999999999981\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 512/25000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 513/25000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 514/25000, steps: 121, e: 0.78\n",
      "accumulated_rewards_per_episode: -1.2100000000000009\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 515/25000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 516/25000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 517/25000, steps: 96, e: 0.78\n",
      "accumulated_rewards_per_episode: -0.9600000000000006\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 518/25000, steps: 69, e: 0.78\n",
      "accumulated_rewards_per_episode: 0.3099999999999994\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 519/25000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 520/25000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 521/25000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 522/25000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 523/25000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 524/25000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 525/25000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 526/25000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 527/25000, steps: 94, e: 0.78\n",
      "accumulated_rewards_per_episode: 0.059999999999999255\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 528/25000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 529/25000, steps: 92, e: 0.78\n",
      "accumulated_rewards_per_episode: -0.9200000000000006\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 530/25000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 531/25000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 532/25000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 533/25000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 534/25000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 535/25000, steps: 87, e: 0.77\n",
      "accumulated_rewards_per_episode: -0.8700000000000006\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 536/25000, steps: 55, e: 0.77\n",
      "accumulated_rewards_per_episode: 0.4499999999999995\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 537/25000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 538/25000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 539/25000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 540/25000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 541/25000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 542/25000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 543/25000, steps: 55, e: 0.77\n",
      "accumulated_rewards_per_episode: -0.5500000000000003\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 544/25000, steps: 111, e: 0.77\n",
      "accumulated_rewards_per_episode: -1.1100000000000008\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 545/25000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 546/25000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 547/25000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 548/25000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 549/25000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 550/25000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 551/25000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 552/25000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 553/25000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 554/25000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 555/25000, steps: 167, e: 0.77\n",
      "accumulated_rewards_per_episode: -1.6700000000000013\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 556/25000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 557/25000, steps: 73, e: 0.77\n",
      "accumulated_rewards_per_episode: 0.2699999999999996\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 558/25000, steps: 149, e: 0.77\n",
      "accumulated_rewards_per_episode: -1.490000000000001\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 559/25000, steps: 52, e: 0.76\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 560/25000, steps: 92, e: 0.76\n",
      "accumulated_rewards_per_episode: 0.07999999999999947\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 561/25000, steps: 52, e: 0.76\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 562/25000, steps: 52, e: 0.76\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 563/25000, steps: 52, e: 0.76\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 564/25000, steps: 52, e: 0.76\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 565/25000, steps: 131, e: 0.76\n",
      "accumulated_rewards_per_episode: -1.310000000000001\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 566/25000, steps: 52, e: 0.76\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 567/25000, steps: 101, e: 0.76\n",
      "accumulated_rewards_per_episode: -1.0100000000000007\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 568/25000, steps: 134, e: 0.76\n",
      "accumulated_rewards_per_episode: -0.3400000000000007\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 569/25000, steps: 67, e: 0.76\n",
      "accumulated_rewards_per_episode: -0.6700000000000004\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 570/25000, steps: 52, e: 0.76\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 571/25000, steps: 52, e: 0.76\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 572/25000, steps: 52, e: 0.76\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 573/25000, steps: 52, e: 0.76\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 574/25000, steps: 118, e: 0.76\n",
      "accumulated_rewards_per_episode: -0.18000000000000055\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 575/25000, steps: 128, e: 0.76\n",
      "accumulated_rewards_per_episode: -0.28000000000000064\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 576/25000, steps: 108, e: 0.76\n",
      "accumulated_rewards_per_episode: -0.0800000000000006\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 577/25000, steps: 78, e: 0.76\n",
      "accumulated_rewards_per_episode: -0.7800000000000005\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 578/25000, steps: 113, e: 0.76\n",
      "accumulated_rewards_per_episode: -0.1300000000000005\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 579/25000, steps: 172, e: 0.76\n",
      "accumulated_rewards_per_episode: -1.7200000000000013\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 580/25000, steps: 52, e: 0.76\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 581/25000, steps: 52, e: 0.76\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 582/25000, steps: 73, e: 0.76\n",
      "accumulated_rewards_per_episode: -0.7300000000000004\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 583/25000, steps: 101, e: 0.76\n",
      "accumulated_rewards_per_episode: -1.0100000000000007\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 584/25000, steps: 99, e: 0.76\n",
      "accumulated_rewards_per_episode: -0.9900000000000007\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 585/25000, steps: 52, e: 0.76\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 586/25000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 587/25000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 588/25000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 589/25000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 590/25000, steps: 63, e: 0.75\n",
      "accumulated_rewards_per_episode: -0.6300000000000003\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 591/25000, steps: 101, e: 0.75\n",
      "accumulated_rewards_per_episode: -1.0100000000000007\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 592/25000, steps: 212, e: 0.75\n",
      "accumulated_rewards_per_episode: -2.1199999999999988\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 593/25000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 594/25000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 595/25000, steps: 101, e: 0.75\n",
      "accumulated_rewards_per_episode: -1.0100000000000007\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 596/25000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 597/25000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 598/25000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 599/25000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 600/25000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 601/25000, steps: 79, e: 0.75\n",
      "accumulated_rewards_per_episode: 0.20999999999999952\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 602/25000, steps: 73, e: 0.75\n",
      "accumulated_rewards_per_episode: -0.7300000000000004\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 603/25000, steps: 88, e: 0.75\n",
      "accumulated_rewards_per_episode: -0.8800000000000006\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 604/25000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 605/25000, steps: 64, e: 0.75\n",
      "accumulated_rewards_per_episode: -0.6400000000000003\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 606/25000, steps: 75, e: 0.75\n",
      "accumulated_rewards_per_episode: -0.7500000000000004\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 607/25000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 608/25000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 609/25000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 610/25000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 611/25000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 612/25000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 613/25000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 614/25000, steps: 56, e: 0.74\n",
      "accumulated_rewards_per_episode: -0.5600000000000003\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 615/25000, steps: 288, e: 0.74\n",
      "accumulated_rewards_per_episode: -1.8800000000000021\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 616/25000, steps: 57, e: 0.74\n",
      "accumulated_rewards_per_episode: 0.4299999999999996\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 617/25000, steps: 52, e: 0.74\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 618/25000, steps: 52, e: 0.74\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 619/25000, steps: 52, e: 0.74\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 620/25000, steps: 52, e: 0.74\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 621/25000, steps: 52, e: 0.74\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 622/25000, steps: 58, e: 0.74\n",
      "accumulated_rewards_per_episode: -0.5800000000000003\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 623/25000, steps: 52, e: 0.74\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 624/25000, steps: 59, e: 0.74\n",
      "accumulated_rewards_per_episode: 0.4099999999999995\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 625/25000, steps: 52, e: 0.74\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 626/25000, steps: 52, e: 0.74\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 627/25000, steps: 52, e: 0.74\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 628/25000, steps: 52, e: 0.74\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 629/25000, steps: 52, e: 0.74\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 630/25000, steps: 52, e: 0.74\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 631/25000, steps: 52, e: 0.74\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 632/25000, steps: 52, e: 0.74\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 633/25000, steps: 52, e: 0.74\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 634/25000, steps: 57, e: 0.74\n",
      "accumulated_rewards_per_episode: -0.5700000000000003\n",
      "START state: (0, 0, 7, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 635/25000, steps: 52, e: 0.74\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 636/25000, steps: 52, e: 0.74\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 637/25000, steps: 69, e: 0.74\n",
      "accumulated_rewards_per_episode: 0.3099999999999996\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 638/25000, steps: 52, e: 0.74\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 639/25000, steps: 52, e: 0.74\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 640/25000, steps: 52, e: 0.74\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 641/25000, steps: 52, e: 0.74\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 642/25000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 643/25000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 644/25000, steps: 85, e: 0.73\n",
      "accumulated_rewards_per_episode: -0.8500000000000005\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 645/25000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 646/25000, steps: 105, e: 0.73\n",
      "accumulated_rewards_per_episode: -1.0500000000000007\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 647/25000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 648/25000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 649/25000, steps: 57, e: 0.73\n",
      "accumulated_rewards_per_episode: -0.5700000000000003\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 650/25000, steps: 183, e: 0.73\n",
      "accumulated_rewards_per_episode: -0.8300000000000014\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 651/25000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 652/25000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 653/25000, steps: 55, e: 0.73\n",
      "accumulated_rewards_per_episode: -0.5500000000000003\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 654/25000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 655/25000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 656/25000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 657/25000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 658/25000, steps: 55, e: 0.73\n",
      "accumulated_rewards_per_episode: -0.5500000000000003\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 659/25000, steps: 55, e: 0.73\n",
      "accumulated_rewards_per_episode: 0.4499999999999996\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 660/25000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 661/25000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 662/25000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 663/25000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 664/25000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 665/25000, steps: 65, e: 0.73\n",
      "accumulated_rewards_per_episode: -0.6500000000000004\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 666/25000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 667/25000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 668/25000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 669/25000, steps: 59, e: 0.73\n",
      "accumulated_rewards_per_episode: -0.5900000000000003\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 670/25000, steps: 111, e: 0.72\n",
      "accumulated_rewards_per_episode: -1.1100000000000008\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 671/25000, steps: 85, e: 0.72\n",
      "accumulated_rewards_per_episode: -0.8500000000000005\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 672/25000, steps: 52, e: 0.72\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 673/25000, steps: 65, e: 0.72\n",
      "accumulated_rewards_per_episode: -0.6500000000000004\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 674/25000, steps: 52, e: 0.72\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 675/25000, steps: 52, e: 0.72\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 676/25000, steps: 52, e: 0.72\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 677/25000, steps: 52, e: 0.72\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 678/25000, steps: 65, e: 0.72\n",
      "accumulated_rewards_per_episode: 0.3499999999999994\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 679/25000, steps: 52, e: 0.72\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 680/25000, steps: 139, e: 0.72\n",
      "accumulated_rewards_per_episode: -0.39000000000000073\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 681/25000, steps: 110, e: 0.72\n",
      "accumulated_rewards_per_episode: -0.10000000000000052\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 682/25000, steps: 52, e: 0.72\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 683/25000, steps: 52, e: 0.72\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 684/25000, steps: 86, e: 0.72\n",
      "accumulated_rewards_per_episode: -0.8600000000000005\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 685/25000, steps: 52, e: 0.72\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 686/25000, steps: 55, e: 0.72\n",
      "accumulated_rewards_per_episode: -0.5500000000000003\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 687/25000, steps: 87, e: 0.72\n",
      "accumulated_rewards_per_episode: 0.12999999999999923\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 688/25000, steps: 52, e: 0.72\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 689/25000, steps: 52, e: 0.72\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 690/25000, steps: 52, e: 0.72\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 691/25000, steps: 165, e: 0.72\n",
      "accumulated_rewards_per_episode: -1.6500000000000012\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 692/25000, steps: 52, e: 0.72\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 693/25000, steps: 89, e: 0.72\n",
      "accumulated_rewards_per_episode: 0.10999999999999946\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 694/25000, steps: 78, e: 0.72\n",
      "accumulated_rewards_per_episode: -0.7800000000000005\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 695/25000, steps: 432, e: 0.72\n",
      "accumulated_rewards_per_episode: -4.319999999999952\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 696/25000, steps: 97, e: 0.72\n",
      "accumulated_rewards_per_episode: 0.02999999999999936\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 697/25000, steps: 52, e: 0.72\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 698/25000, steps: 70, e: 0.72\n",
      "accumulated_rewards_per_episode: -0.7000000000000004\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 699/25000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 700/25000, steps: 59, e: 0.71\n",
      "accumulated_rewards_per_episode: -0.5900000000000003\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 701/25000, steps: 61, e: 0.71\n",
      "accumulated_rewards_per_episode: -0.6100000000000003\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 702/25000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 703/25000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 704/25000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 705/25000, steps: 63, e: 0.71\n",
      "accumulated_rewards_per_episode: -0.6300000000000003\n",
      "START state: (0, 0, 2, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 706/25000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 707/25000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 708/25000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 709/25000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 710/25000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 711/25000, steps: 68, e: 0.71\n",
      "accumulated_rewards_per_episode: 0.3199999999999995\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 712/25000, steps: 68, e: 0.71\n",
      "accumulated_rewards_per_episode: 0.3199999999999996\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 713/25000, steps: 301, e: 0.71\n",
      "accumulated_rewards_per_episode: -3.00999999999998\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 714/25000, steps: 78, e: 0.71\n",
      "accumulated_rewards_per_episode: 0.21999999999999953\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 715/25000, steps: 93, e: 0.71\n",
      "accumulated_rewards_per_episode: -0.9300000000000006\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 716/25000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 717/25000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 718/25000, steps: 56, e: 0.71\n",
      "accumulated_rewards_per_episode: -0.5600000000000003\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 719/25000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 720/25000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 721/25000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 722/25000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 723/25000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 724/25000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 725/25000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 726/25000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 727/25000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 728/25000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 729/25000, steps: 83, e: 0.7\n",
      "accumulated_rewards_per_episode: -0.8300000000000005\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 730/25000, steps: 113, e: 0.7\n",
      "accumulated_rewards_per_episode: 0.8699999999999992\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 731/25000, steps: 99, e: 0.7\n",
      "accumulated_rewards_per_episode: -0.9900000000000007\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 732/25000, steps: 56, e: 0.7\n",
      "accumulated_rewards_per_episode: -0.5600000000000003\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 733/25000, steps: 97, e: 0.7\n",
      "accumulated_rewards_per_episode: -0.9700000000000006\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 734/25000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 735/25000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 736/25000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 737/25000, steps: 57, e: 0.7\n",
      "accumulated_rewards_per_episode: 0.4299999999999997\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 738/25000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 739/25000, steps: 57, e: 0.7\n",
      "accumulated_rewards_per_episode: -0.5700000000000003\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 740/25000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 741/25000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 742/25000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 743/25000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 744/25000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 745/25000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 746/25000, steps: 100, e: 0.7\n",
      "accumulated_rewards_per_episode: 0.9999999999999991\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 747/25000, steps: 55, e: 0.7\n",
      "accumulated_rewards_per_episode: -0.5500000000000003\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 748/25000, steps: 120, e: 0.7\n",
      "accumulated_rewards_per_episode: -1.2000000000000008\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 749/25000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 750/25000, steps: 59, e: 0.7\n",
      "accumulated_rewards_per_episode: -0.5900000000000003\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 751/25000, steps: 66, e: 0.7\n",
      "accumulated_rewards_per_episode: 0.3399999999999996\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 752/25000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 753/25000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 754/25000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 755/25000, steps: 84, e: 0.7\n",
      "accumulated_rewards_per_episode: 0.15999999999999948\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 756/25000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 757/25000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 758/25000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 759/25000, steps: 105, e: 0.69\n",
      "accumulated_rewards_per_episode: -1.0500000000000007\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 760/25000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 761/25000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 762/25000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 763/25000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 764/25000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 765/25000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 766/25000, steps: 75, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.7500000000000004\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 767/25000, steps: 69, e: 0.69\n",
      "accumulated_rewards_per_episode: 0.3099999999999996\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 768/25000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 769/25000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 770/25000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 771/25000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 772/25000, steps: 58, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.5800000000000003\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 773/25000, steps: 125, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.25000000000000083\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 774/25000, steps: 197, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.9700000000000012\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 775/25000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 776/25000, steps: 72, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.7200000000000004\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 777/25000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 778/25000, steps: 171, e: 0.69\n",
      "accumulated_rewards_per_episode: -1.7100000000000013\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 779/25000, steps: 63, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.6300000000000003\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 780/25000, steps: 215, e: 0.69\n",
      "accumulated_rewards_per_episode: -1.1500000000000017\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 781/25000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 782/25000, steps: 111, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.11000000000000074\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 783/25000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 784/25000, steps: 70, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.7000000000000004\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 785/25000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 786/25000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 787/25000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 788/25000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 789/25000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 790/25000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 791/25000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 792/25000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 793/25000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 794/25000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 795/25000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 796/25000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 797/25000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 798/25000, steps: 73, e: 0.68\n",
      "accumulated_rewards_per_episode: 0.2699999999999996\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 799/25000, steps: 60, e: 0.68\n",
      "accumulated_rewards_per_episode: 0.39999999999999947\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 800/25000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 801/25000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 802/25000, steps: 66, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.6600000000000004\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 803/25000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 804/25000, steps: 64, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.6400000000000003\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 805/25000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 806/25000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 807/25000, steps: 174, e: 0.68\n",
      "accumulated_rewards_per_episode: -1.7400000000000013\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 808/25000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 809/25000, steps: 61, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.6100000000000003\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 810/25000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 811/25000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 812/25000, steps: 119, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.19000000000000078\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 813/25000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 814/25000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 815/25000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 816/25000, steps: 99, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.9900000000000007\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 817/25000, steps: 64, e: 0.68\n",
      "accumulated_rewards_per_episode: 0.35999999999999965\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 818/25000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 819/25000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 820/25000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 821/25000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 822/25000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 823/25000, steps: 147, e: 0.67\n",
      "accumulated_rewards_per_episode: -1.470000000000001\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 824/25000, steps: 214, e: 0.67\n",
      "accumulated_rewards_per_episode: -2.1399999999999983\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 825/25000, steps: 203, e: 0.67\n",
      "accumulated_rewards_per_episode: -2.0300000000000007\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 826/25000, steps: 72, e: 0.67\n",
      "accumulated_rewards_per_episode: -0.7200000000000004\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 827/25000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 828/25000, steps: 111, e: 0.67\n",
      "accumulated_rewards_per_episode: -1.1100000000000008\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 829/25000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 830/25000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 831/25000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 832/25000, steps: 75, e: 0.67\n",
      "accumulated_rewards_per_episode: 0.24999999999999956\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 833/25000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 834/25000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 835/25000, steps: 61, e: 0.67\n",
      "accumulated_rewards_per_episode: -0.6100000000000003\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 836/25000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 837/25000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 838/25000, steps: 62, e: 0.67\n",
      "accumulated_rewards_per_episode: -0.6200000000000003\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 839/25000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 840/25000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 841/25000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 842/25000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 843/25000, steps: 98, e: 0.67\n",
      "accumulated_rewards_per_episode: -0.9800000000000006\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 844/25000, steps: 56, e: 0.67\n",
      "accumulated_rewards_per_episode: 0.4399999999999997\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 845/25000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 846/25000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 847/25000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 848/25000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 849/25000, steps: 536, e: 0.67\n",
      "accumulated_rewards_per_episode: -4.359999999999952\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 850/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 851/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 852/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 853/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 854/25000, steps: 56, e: 0.66\n",
      "accumulated_rewards_per_episode: 0.4399999999999996\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 855/25000, steps: 58, e: 0.66\n",
      "accumulated_rewards_per_episode: -0.5800000000000003\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 856/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 857/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 858/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 859/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 860/25000, steps: 60, e: 0.66\n",
      "accumulated_rewards_per_episode: 0.39999999999999947\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 861/25000, steps: 88, e: 0.66\n",
      "accumulated_rewards_per_episode: -0.8800000000000006\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 862/25000, steps: 494, e: 0.66\n",
      "accumulated_rewards_per_episode: -3.939999999999939\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 863/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 864/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 865/25000, steps: 117, e: 0.66\n",
      "accumulated_rewards_per_episode: -1.1700000000000008\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 866/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 867/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 868/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 869/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 870/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 871/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 872/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 873/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 874/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 875/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 876/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 877/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 878/25000, steps: 55, e: 0.66\n",
      "accumulated_rewards_per_episode: -0.5500000000000003\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 879/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 880/25000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 881/25000, steps: 53, e: 0.66\n",
      "accumulated_rewards_per_episode: 0.46999999999999964\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 882/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 883/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 884/25000, steps: 197, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.9700000000000012\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 885/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 886/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 887/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 888/25000, steps: 94, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.9400000000000006\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 889/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 890/25000, steps: 94, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.9400000000000006\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 891/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 892/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 893/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 894/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 895/25000, steps: 154, e: 0.65\n",
      "accumulated_rewards_per_episode: -1.5400000000000011\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 896/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 897/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 898/25000, steps: 164, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.6400000000000009\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 899/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 900/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 901/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 902/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 903/25000, steps: 418, e: 0.65\n",
      "accumulated_rewards_per_episode: -4.179999999999955\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 904/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 905/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 906/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 907/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 908/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 909/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 910/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 911/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 912/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 913/25000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 914/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 915/25000, steps: 135, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.35000000000000075\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 916/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 917/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 918/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 919/25000, steps: 173, e: 0.64\n",
      "accumulated_rewards_per_episode: -1.7300000000000013\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 920/25000, steps: 92, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.9200000000000006\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 921/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 922/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 923/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 924/25000, steps: 228, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.2800000000000013\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 925/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 926/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 927/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 928/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 929/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 930/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 931/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 932/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 933/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 934/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 935/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 936/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 937/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 938/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 939/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 940/25000, steps: 55, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5500000000000003\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 941/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 942/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 943/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 944/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 945/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 946/25000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 947/25000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 948/25000, steps: 169, e: 0.63\n",
      "accumulated_rewards_per_episode: -1.6900000000000013\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 949/25000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 950/25000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 951/25000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 952/25000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 953/25000, steps: 54, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5400000000000003\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 954/25000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 955/25000, steps: 150, e: 0.63\n",
      "accumulated_rewards_per_episode: -1.500000000000001\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 956/25000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 957/25000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 958/25000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 959/25000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 960/25000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 961/25000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 962/25000, steps: 56, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5600000000000003\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 963/25000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 964/25000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 965/25000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 966/25000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 967/25000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 968/25000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 969/25000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 970/25000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 971/25000, steps: 130, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.30000000000000066\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 972/25000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 973/25000, steps: 202, e: 0.63\n",
      "accumulated_rewards_per_episode: -1.0200000000000016\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 974/25000, steps: 327, e: 0.63\n",
      "accumulated_rewards_per_episode: -3.2699999999999743\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 975/25000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 976/25000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 977/25000, steps: 56, e: 0.63\n",
      "accumulated_rewards_per_episode: 0.4399999999999995\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 978/25000, steps: 185, e: 0.63\n",
      "accumulated_rewards_per_episode: -1.8500000000000014\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 979/25000, steps: 53, e: 0.63\n",
      "accumulated_rewards_per_episode: -0.5300000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 980/25000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 981/25000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 982/25000, steps: 64, e: 0.62\n",
      "accumulated_rewards_per_episode: -0.6400000000000003\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 983/25000, steps: 102, e: 0.62\n",
      "accumulated_rewards_per_episode: -0.020000000000000753\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 984/25000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 985/25000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 986/25000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 987/25000, steps: 205, e: 0.62\n",
      "accumulated_rewards_per_episode: -2.0500000000000003\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 988/25000, steps: 139, e: 0.62\n",
      "accumulated_rewards_per_episode: -1.390000000000001\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 989/25000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 990/25000, steps: 72, e: 0.62\n",
      "accumulated_rewards_per_episode: -0.7200000000000004\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 991/25000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 992/25000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 993/25000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 994/25000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 995/25000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 996/25000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 997/25000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 998/25000, steps: 61, e: 0.62\n",
      "accumulated_rewards_per_episode: -0.6100000000000003\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 999/25000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1000/25000, steps: 62, e: 0.62\n",
      "accumulated_rewards_per_episode: -0.6200000000000003\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1001/25000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 1002/25000, steps: 108, e: 0.62\n",
      "accumulated_rewards_per_episode: -1.0800000000000007\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 1003/25000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1004/25000, steps: 74, e: 0.62\n",
      "accumulated_rewards_per_episode: 0.25999999999999934\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 1005/25000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 1006/25000, steps: 55, e: 0.62\n",
      "accumulated_rewards_per_episode: -0.5500000000000003\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1007/25000, steps: 70, e: 0.62\n",
      "accumulated_rewards_per_episode: 0.2999999999999995\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1008/25000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 1009/25000, steps: 68, e: 0.62\n",
      "accumulated_rewards_per_episode: 0.3199999999999995\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 1010/25000, steps: 120, e: 0.62\n",
      "accumulated_rewards_per_episode: -1.2000000000000008\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1011/25000, steps: 117, e: 0.62\n",
      "accumulated_rewards_per_episode: 0.8299999999999992\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1012/25000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 1013/25000, steps: 62, e: 0.61\n",
      "accumulated_rewards_per_episode: 0.37999999999999967\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 1014/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1015/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 1016/25000, steps: 68, e: 0.61\n",
      "accumulated_rewards_per_episode: 0.3199999999999995\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 1017/25000, steps: 114, e: 0.61\n",
      "accumulated_rewards_per_episode: -1.1400000000000008\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1018/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 1019/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1020/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1021/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 1022/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1023/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 1024/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 1025/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 1026/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1027/25000, steps: 104, e: 0.61\n",
      "accumulated_rewards_per_episode: -0.040000000000000535\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1028/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 1029/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1030/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1031/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1032/25000, steps: 101, e: 0.61\n",
      "accumulated_rewards_per_episode: -1.0100000000000007\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1033/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1034/25000, steps: 165, e: 0.61\n",
      "accumulated_rewards_per_episode: -0.6500000000000012\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1035/25000, steps: 199, e: 0.61\n",
      "accumulated_rewards_per_episode: -1.9900000000000015\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 1036/25000, steps: 186, e: 0.61\n",
      "accumulated_rewards_per_episode: 0.1399999999999988\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 1037/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1038/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1039/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 1040/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1041/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 1042/25000, steps: 95, e: 0.61\n",
      "accumulated_rewards_per_episode: 0.049999999999999475\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1043/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1044/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 1045/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1046/25000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1047/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 1048/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1049/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 1050/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 1051/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1052/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 1053/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 1054/25000, steps: 138, e: 0.6\n",
      "accumulated_rewards_per_episode: -0.3800000000000007\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 1055/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1056/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 1057/25000, steps: 141, e: 0.6\n",
      "accumulated_rewards_per_episode: -0.41000000000000075\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1058/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 1059/25000, steps: 83, e: 0.6\n",
      "accumulated_rewards_per_episode: 0.16999999999999948\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1060/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1061/25000, steps: 114, e: 0.6\n",
      "accumulated_rewards_per_episode: -1.1400000000000008\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 1062/25000, steps: 284, e: 0.6\n",
      "accumulated_rewards_per_episode: -1.8399999999999859\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1063/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 1064/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1065/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1066/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 1067/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1068/25000, steps: 493, e: 0.6\n",
      "accumulated_rewards_per_episode: -2.9299999999999606\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 1069/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 1070/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1071/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 1072/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1073/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1074/25000, steps: 116, e: 0.6\n",
      "accumulated_rewards_per_episode: -1.1600000000000008\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 1075/25000, steps: 240, e: 0.6\n",
      "accumulated_rewards_per_episode: -1.4000000000000017\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1076/25000, steps: 233, e: 0.6\n",
      "accumulated_rewards_per_episode: -0.3300000000000014\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 1077/25000, steps: 222, e: 0.6\n",
      "accumulated_rewards_per_episode: -2.2199999999999966\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 1078/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1079/25000, steps: 205, e: 0.6\n",
      "accumulated_rewards_per_episode: -0.050000000000001425\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 1080/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1081/25000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1082/25000, steps: 55, e: 0.59\n",
      "accumulated_rewards_per_episode: -0.5500000000000003\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 1083/25000, steps: 486, e: 0.59\n",
      "accumulated_rewards_per_episode: -1.8599999999999808\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1084/25000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 1085/25000, steps: 113, e: 0.59\n",
      "accumulated_rewards_per_episode: -1.1300000000000008\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 1086/25000, steps: 197, e: 0.59\n",
      "accumulated_rewards_per_episode: -0.9700000000000015\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 1087/25000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1088/25000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 1089/25000, steps: 113, e: 0.59\n",
      "accumulated_rewards_per_episode: -1.1300000000000008\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 1090/25000, steps: 71, e: 0.59\n",
      "accumulated_rewards_per_episode: -0.7100000000000004\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 1091/25000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 1092/25000, steps: 167, e: 0.59\n",
      "accumulated_rewards_per_episode: 0.32999999999999896\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 1093/25000, steps: 256, e: 0.59\n",
      "accumulated_rewards_per_episode: -1.5600000000000016\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1094/25000, steps: 115, e: 0.59\n",
      "accumulated_rewards_per_episode: -0.15000000000000063\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1095/25000, steps: 76, e: 0.59\n",
      "accumulated_rewards_per_episode: 0.23999999999999944\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1096/25000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1097/25000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1098/25000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1099/25000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 1100/25000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 1101/25000, steps: 53, e: 0.59\n",
      "accumulated_rewards_per_episode: 0.46999999999999975\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 1102/25000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1103/25000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1104/25000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1105/25000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 1106/25000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 1107/25000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1108/25000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 1109/25000, steps: 55, e: 0.59\n",
      "accumulated_rewards_per_episode: -0.5500000000000003\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1110/25000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1111/25000, steps: 800, e: 0.59\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1112/25000, steps: 413, e: 0.59\n",
      "accumulated_rewards_per_episode: -3.129999999999978\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1113/25000, steps: 99, e: 0.59\n",
      "accumulated_rewards_per_episode: -0.9900000000000007\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 1114/25000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 1115/25000, steps: 89, e: 0.59\n",
      "accumulated_rewards_per_episode: -0.8900000000000006\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 1116/25000, steps: 563, e: 0.59\n",
      "accumulated_rewards_per_episode: -4.629999999999924\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 1117/25000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 1118/25000, steps: 144, e: 0.58\n",
      "accumulated_rewards_per_episode: -1.440000000000001\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1119/25000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1120/25000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 1121/25000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 1122/25000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1123/25000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1124/25000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1125/25000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1126/25000, steps: 103, e: 0.58\n",
      "accumulated_rewards_per_episode: -1.0300000000000007\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1127/25000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1128/25000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 1129/25000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1130/25000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 1131/25000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 1132/25000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1133/25000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1134/25000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1135/25000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 1136/25000, steps: 414, e: 0.58\n",
      "accumulated_rewards_per_episode: -3.139999999999978\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 1137/25000, steps: 84, e: 0.58\n",
      "accumulated_rewards_per_episode: 0.15999999999999936\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 1138/25000, steps: 73, e: 0.58\n",
      "accumulated_rewards_per_episode: 0.2699999999999996\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1139/25000, steps: 68, e: 0.58\n",
      "accumulated_rewards_per_episode: -0.6800000000000004\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 1140/25000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1141/25000, steps: 140, e: 0.58\n",
      "accumulated_rewards_per_episode: 0.599999999999999\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1142/25000, steps: 93, e: 0.58\n",
      "accumulated_rewards_per_episode: -0.9300000000000006\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 1143/25000, steps: 62, e: 0.58\n",
      "accumulated_rewards_per_episode: -0.6200000000000003\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1144/25000, steps: 469, e: 0.58\n",
      "accumulated_rewards_per_episode: -3.689999999999966\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 1145/25000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1146/25000, steps: 94, e: 0.58\n",
      "accumulated_rewards_per_episode: -0.9400000000000006\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 1147/25000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1148/25000, steps: 79, e: 0.58\n",
      "accumulated_rewards_per_episode: 0.20999999999999952\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1149/25000, steps: 122, e: 0.58\n",
      "accumulated_rewards_per_episode: -1.2200000000000009\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1150/25000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1151/25000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 1152/25000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1153/25000, steps: 97, e: 0.57\n",
      "accumulated_rewards_per_episode: 0.02999999999999947\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1154/25000, steps: 355, e: 0.57\n",
      "accumulated_rewards_per_episode: -1.5499999999999963\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1155/25000, steps: 251, e: 0.57\n",
      "accumulated_rewards_per_episode: -1.5099999999999991\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 1156/25000, steps: 75, e: 0.57\n",
      "accumulated_rewards_per_episode: -0.7500000000000004\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1157/25000, steps: 52, e: 0.57\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1158/25000, steps: 52, e: 0.57\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1159/25000, steps: 523, e: 0.57\n",
      "accumulated_rewards_per_episode: -4.229999999999955\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1160/25000, steps: 52, e: 0.57\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1161/25000, steps: 52, e: 0.57\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1162/25000, steps: 52, e: 0.57\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1163/25000, steps: 54, e: 0.57\n",
      "accumulated_rewards_per_episode: 0.45999999999999974\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1164/25000, steps: 52, e: 0.57\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 1165/25000, steps: 52, e: 0.57\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1166/25000, steps: 52, e: 0.57\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1167/25000, steps: 800, e: 0.57\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 1168/25000, steps: 52, e: 0.57\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1169/25000, steps: 800, e: 0.57\n",
      "accumulated_rewards_per_episode: -4.99999999999994\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1170/25000, steps: 232, e: 0.57\n",
      "accumulated_rewards_per_episode: -2.3199999999999945\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1171/25000, steps: 86, e: 0.57\n",
      "accumulated_rewards_per_episode: -0.8600000000000005\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1172/25000, steps: 102, e: 0.57\n",
      "accumulated_rewards_per_episode: -1.0200000000000007\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1173/25000, steps: 52, e: 0.57\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 1174/25000, steps: 105, e: 0.57\n",
      "accumulated_rewards_per_episode: -0.05000000000000054\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 1175/25000, steps: 269, e: 0.57\n",
      "accumulated_rewards_per_episode: -1.6900000000000017\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 1176/25000, steps: 78, e: 0.57\n",
      "accumulated_rewards_per_episode: -0.7800000000000005\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1177/25000, steps: 106, e: 0.57\n",
      "accumulated_rewards_per_episode: -1.0600000000000007\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1178/25000, steps: 406, e: 0.57\n",
      "accumulated_rewards_per_episode: -4.059999999999958\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1179/25000, steps: 52, e: 0.57\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1180/25000, steps: 514, e: 0.57\n",
      "accumulated_rewards_per_episode: -4.139999999999951\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 1181/25000, steps: 206, e: 0.57\n",
      "accumulated_rewards_per_episode: -1.0600000000000012\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1182/25000, steps: 800, e: 0.57\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 1183/25000, steps: 272, e: 0.57\n",
      "accumulated_rewards_per_episode: -1.7200000000000017\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1184/25000, steps: 52, e: 0.57\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1185/25000, steps: 511, e: 0.57\n",
      "accumulated_rewards_per_episode: -5.1099999999999355\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 1186/25000, steps: 52, e: 0.57\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1187/25000, steps: 89, e: 0.57\n",
      "accumulated_rewards_per_episode: -0.8900000000000006\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1188/25000, steps: 52, e: 0.57\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1189/25000, steps: 232, e: 0.57\n",
      "accumulated_rewards_per_episode: -1.3200000000000018\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1190/25000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 1191/25000, steps: 800, e: 0.56\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1192/25000, steps: 264, e: 0.56\n",
      "accumulated_rewards_per_episode: -1.640000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 1193/25000, steps: 58, e: 0.56\n",
      "accumulated_rewards_per_episode: 0.4199999999999996\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1194/25000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1195/25000, steps: 384, e: 0.56\n",
      "accumulated_rewards_per_episode: -2.839999999999984\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 1196/25000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 1197/25000, steps: 453, e: 0.56\n",
      "accumulated_rewards_per_episode: -2.529999999999972\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 1198/25000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 1199/25000, steps: 167, e: 0.56\n",
      "accumulated_rewards_per_episode: -0.6700000000000009\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1200/25000, steps: 430, e: 0.56\n",
      "accumulated_rewards_per_episode: -3.2999999999999745\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 1201/25000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1202/25000, steps: 125, e: 0.56\n",
      "accumulated_rewards_per_episode: -0.2500000000000006\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 1203/25000, steps: 67, e: 0.56\n",
      "accumulated_rewards_per_episode: 0.3299999999999996\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1204/25000, steps: 334, e: 0.56\n",
      "accumulated_rewards_per_episode: -2.339999999999995\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1205/25000, steps: 175, e: 0.56\n",
      "accumulated_rewards_per_episode: -0.7500000000000012\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1206/25000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 1207/25000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 1208/25000, steps: 258, e: 0.56\n",
      "accumulated_rewards_per_episode: -1.5799999999999932\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1209/25000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1210/25000, steps: 56, e: 0.56\n",
      "accumulated_rewards_per_episode: 0.4399999999999995\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 1211/25000, steps: 67, e: 0.56\n",
      "accumulated_rewards_per_episode: 0.3299999999999996\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1212/25000, steps: 661, e: 0.56\n",
      "accumulated_rewards_per_episode: -5.609999999999925\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1213/25000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1214/25000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 1215/25000, steps: 800, e: 0.56\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1216/25000, steps: 270, e: 0.56\n",
      "accumulated_rewards_per_episode: -1.7000000000000017\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 1217/25000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1218/25000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 1219/25000, steps: 320, e: 0.56\n",
      "accumulated_rewards_per_episode: -2.199999999999998\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 1220/25000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1221/25000, steps: 110, e: 0.56\n",
      "accumulated_rewards_per_episode: -1.1000000000000008\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 1222/25000, steps: 475, e: 0.56\n",
      "accumulated_rewards_per_episode: -3.7499999999999645\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1223/25000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1224/25000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 1225/25000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1226/25000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 1227/25000, steps: 52, e: 0.55\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1228/25000, steps: 52, e: 0.55\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 1229/25000, steps: 52, e: 0.55\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 1230/25000, steps: 456, e: 0.55\n",
      "accumulated_rewards_per_episode: -3.559999999999969\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1231/25000, steps: 52, e: 0.55\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 1232/25000, steps: 52, e: 0.55\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1233/25000, steps: 52, e: 0.55\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 1234/25000, steps: 52, e: 0.55\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 1235/25000, steps: 255, e: 0.55\n",
      "accumulated_rewards_per_episode: -1.5500000000000018\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1236/25000, steps: 126, e: 0.55\n",
      "accumulated_rewards_per_episode: -0.2600000000000009\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 1237/25000, steps: 52, e: 0.55\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 1238/25000, steps: 52, e: 0.55\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 1239/25000, steps: 260, e: 0.55\n",
      "accumulated_rewards_per_episode: -1.6000000000000019\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 1240/25000, steps: 60, e: 0.55\n",
      "accumulated_rewards_per_episode: 0.39999999999999947\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1241/25000, steps: 61, e: 0.55\n",
      "accumulated_rewards_per_episode: 0.3899999999999997\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1242/25000, steps: 114, e: 0.55\n",
      "accumulated_rewards_per_episode: -1.1400000000000008\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1243/25000, steps: 84, e: 0.55\n",
      "accumulated_rewards_per_episode: 0.15999999999999936\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1244/25000, steps: 520, e: 0.55\n",
      "accumulated_rewards_per_episode: -4.199999999999956\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 1245/25000, steps: 60, e: 0.55\n",
      "accumulated_rewards_per_episode: 0.3999999999999996\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1246/25000, steps: 52, e: 0.55\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1247/25000, steps: 52, e: 0.55\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1248/25000, steps: 52, e: 0.55\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1249/25000, steps: 553, e: 0.55\n",
      "accumulated_rewards_per_episode: -3.529999999999948\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1250/25000, steps: 558, e: 0.55\n",
      "accumulated_rewards_per_episode: -4.579999999999947\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1251/25000, steps: 52, e: 0.55\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 1252/25000, steps: 501, e: 0.55\n",
      "accumulated_rewards_per_episode: -4.00999999999996\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 1253/25000, steps: 83, e: 0.55\n",
      "accumulated_rewards_per_episode: 0.16999999999999948\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 1254/25000, steps: 198, e: 0.55\n",
      "accumulated_rewards_per_episode: 1.0199999999999987\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 1255/25000, steps: 110, e: 0.55\n",
      "accumulated_rewards_per_episode: -0.10000000000000052\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 1256/25000, steps: 403, e: 0.55\n",
      "accumulated_rewards_per_episode: -2.0299999999999825\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1257/25000, steps: 214, e: 0.55\n",
      "accumulated_rewards_per_episode: -1.1400000000000015\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 1258/25000, steps: 52, e: 0.55\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1259/25000, steps: 52, e: 0.55\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1260/25000, steps: 65, e: 0.55\n",
      "accumulated_rewards_per_episode: 0.34999999999999953\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1261/25000, steps: 91, e: 0.55\n",
      "accumulated_rewards_per_episode: 0.08999999999999947\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1262/25000, steps: 52, e: 0.55\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1263/25000, steps: 52, e: 0.55\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1264/25000, steps: 52, e: 0.55\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1265/25000, steps: 319, e: 0.54\n",
      "accumulated_rewards_per_episode: -2.189999999999998\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1266/25000, steps: 105, e: 0.54\n",
      "accumulated_rewards_per_episode: -1.0500000000000007\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1267/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1268/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 1269/25000, steps: 312, e: 0.54\n",
      "accumulated_rewards_per_episode: -2.1199999999999997\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 1270/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 1271/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1272/25000, steps: 88, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.8800000000000006\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1273/25000, steps: 293, e: 0.54\n",
      "accumulated_rewards_per_episode: -1.9299999999999848\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 1274/25000, steps: 79, e: 0.54\n",
      "accumulated_rewards_per_episode: 0.2099999999999994\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1275/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1276/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1277/25000, steps: 61, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.6100000000000003\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1278/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1279/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 1280/25000, steps: 265, e: 0.54\n",
      "accumulated_rewards_per_episode: -1.6500000000000017\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 1281/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1282/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1283/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 1284/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 1285/25000, steps: 53, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.5300000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1286/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 1287/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 1288/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1289/25000, steps: 99, e: 0.54\n",
      "accumulated_rewards_per_episode: 0.00999999999999947\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 1290/25000, steps: 332, e: 0.54\n",
      "accumulated_rewards_per_episode: -1.320000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 1291/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 1292/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 1293/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 1294/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 1295/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 1296/25000, steps: 136, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.3600000000000008\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1297/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 1298/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1299/25000, steps: 109, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.09000000000000052\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1300/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 1301/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1302/25000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 1303/25000, steps: 54, e: 0.54\n",
      "accumulated_rewards_per_episode: -0.5400000000000003\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1304/25000, steps: 54, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.5400000000000003\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 1305/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1306/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 1307/25000, steps: 77, e: 0.53\n",
      "accumulated_rewards_per_episode: 0.22999999999999954\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 1308/25000, steps: 99, e: 0.53\n",
      "accumulated_rewards_per_episode: 0.00999999999999947\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1309/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1310/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1311/25000, steps: 180, e: 0.53\n",
      "accumulated_rewards_per_episode: -1.8000000000000014\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 1312/25000, steps: 84, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.8400000000000005\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1313/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 1314/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 1315/25000, steps: 58, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.5800000000000003\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 1316/25000, steps: 124, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.2400000000000007\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 1317/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1318/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1319/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1320/25000, steps: 271, e: 0.53\n",
      "accumulated_rewards_per_episode: -1.7100000000000022\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1321/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1322/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1323/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1324/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 1325/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 1326/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1327/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 1328/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 1329/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 1330/25000, steps: 182, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.8200000000000014\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1331/25000, steps: 82, e: 0.53\n",
      "accumulated_rewards_per_episode: 0.17999999999999927\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 1332/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 1333/25000, steps: 69, e: 0.53\n",
      "accumulated_rewards_per_episode: 0.3099999999999995\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1334/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1335/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1336/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 1337/25000, steps: 64, e: 0.53\n",
      "accumulated_rewards_per_episode: 0.35999999999999965\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 1338/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1339/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 1340/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1341/25000, steps: 59, e: 0.53\n",
      "accumulated_rewards_per_episode: 0.4099999999999995\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 1342/25000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 1343/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 1344/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1345/25000, steps: 77, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.22999999999999954\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1346/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1347/25000, steps: 74, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.25999999999999956\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1348/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1349/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1350/25000, steps: 134, e: 0.52\n",
      "accumulated_rewards_per_episode: -1.340000000000001\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1351/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1352/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1353/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1354/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1355/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 1356/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 1357/25000, steps: 95, e: 0.52\n",
      "accumulated_rewards_per_episode: -0.9500000000000006\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1358/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1359/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1360/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 1361/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1362/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1363/25000, steps: 160, e: 0.52\n",
      "accumulated_rewards_per_episode: -0.6000000000000011\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1364/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 1365/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 1366/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1367/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1368/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 1369/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1370/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 1371/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 1372/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1373/25000, steps: 67, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.3299999999999996\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 1374/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1375/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1376/25000, steps: 78, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.21999999999999953\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1377/25000, steps: 135, e: 0.52\n",
      "accumulated_rewards_per_episode: -0.350000000000001\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1378/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1379/25000, steps: 67, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.3299999999999996\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 1380/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 1381/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1382/25000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1383/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1384/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1385/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1386/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1387/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 1388/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1389/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1390/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 1391/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 1392/25000, steps: 56, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5600000000000003\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 1393/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 1394/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1395/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1396/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 1397/25000, steps: 374, e: 0.51\n",
      "accumulated_rewards_per_episode: -2.739999999999986\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 1398/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1399/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 1400/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1401/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1402/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1403/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1404/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1405/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1406/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1407/25000, steps: 68, e: 0.51\n",
      "accumulated_rewards_per_episode: 0.3199999999999996\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1408/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 1409/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 1410/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1411/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1412/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1413/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1414/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1415/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1416/25000, steps: 76, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.7600000000000005\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 1417/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1418/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 1419/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1420/25000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1421/25000, steps: 79, e: 0.51\n",
      "accumulated_rewards_per_episode: -0.7900000000000005\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1422/25000, steps: 282, e: 0.51\n",
      "accumulated_rewards_per_episode: -1.8200000000000018\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 1423/25000, steps: 66, e: 0.51\n",
      "accumulated_rewards_per_episode: 0.3399999999999994\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 1424/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1425/25000, steps: 74, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.7400000000000004\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1426/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1427/25000, steps: 167, e: 0.5\n",
      "accumulated_rewards_per_episode: -1.6700000000000013\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1428/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 1429/25000, steps: 77, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.7700000000000005\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 1430/25000, steps: 121, e: 0.5\n",
      "accumulated_rewards_per_episode: -1.2100000000000009\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1431/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 1432/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 1433/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1434/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1435/25000, steps: 68, e: 0.5\n",
      "accumulated_rewards_per_episode: 0.3199999999999995\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1436/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 1437/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1438/25000, steps: 145, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.45000000000000084\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1439/25000, steps: 70, e: 0.5\n",
      "accumulated_rewards_per_episode: 0.2999999999999994\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 1440/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1441/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1442/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 1443/25000, steps: 56, e: 0.5\n",
      "accumulated_rewards_per_episode: 0.4399999999999996\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 1444/25000, steps: 227, e: 0.5\n",
      "accumulated_rewards_per_episode: -1.2700000000000016\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1445/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 1446/25000, steps: 84, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.8400000000000005\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1447/25000, steps: 234, e: 0.5\n",
      "accumulated_rewards_per_episode: -2.339999999999994\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1448/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1449/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 1450/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 1451/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 1452/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1453/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1454/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1455/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1456/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1457/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1458/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1459/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1460/25000, steps: 68, e: 0.5\n",
      "accumulated_rewards_per_episode: 0.3199999999999996\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 1461/25000, steps: 75, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.7500000000000004\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1462/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1463/25000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 1464/25000, steps: 58, e: 0.5\n",
      "accumulated_rewards_per_episode: -0.5800000000000003\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 1465/25000, steps: 109, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.09000000000000052\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1466/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 1467/25000, steps: 152, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5200000000000009\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 1468/25000, steps: 58, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5800000000000003\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 1469/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 1470/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1471/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 1472/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1473/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1474/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1475/25000, steps: 53, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5300000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1476/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 1477/25000, steps: 187, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.8700000000000014\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1478/25000, steps: 53, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5300000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1479/25000, steps: 120, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.20000000000000068\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 1480/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 1481/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1482/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1483/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 1484/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 1485/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1486/25000, steps: 108, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.08000000000000053\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 1487/25000, steps: 77, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.7700000000000005\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1488/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 1489/25000, steps: 57, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5700000000000003\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 1490/25000, steps: 103, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.030000000000000644\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 1491/25000, steps: 64, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.6400000000000003\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 1492/25000, steps: 56, e: 0.49\n",
      "accumulated_rewards_per_episode: 0.4399999999999996\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 1493/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 1494/25000, steps: 66, e: 0.49\n",
      "accumulated_rewards_per_episode: 0.3399999999999995\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1495/25000, steps: 99, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.9900000000000007\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 1496/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 1497/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1498/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 1499/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1500/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1501/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 1502/25000, steps: 77, e: 0.49\n",
      "accumulated_rewards_per_episode: 1.2299999999999995\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 1503/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1504/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 1505/25000, steps: 67, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.6700000000000004\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1506/25000, steps: 240, e: 0.49\n",
      "accumulated_rewards_per_episode: -2.399999999999993\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 1507/25000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1508/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1509/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 1510/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1511/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 1512/25000, steps: 62, e: 0.48\n",
      "accumulated_rewards_per_episode: 0.37999999999999956\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 1513/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 1514/25000, steps: 109, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.09000000000000052\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1515/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 1516/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 1517/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 1518/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 1519/25000, steps: 113, e: 0.48\n",
      "accumulated_rewards_per_episode: -1.1300000000000008\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1520/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 1521/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 1522/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 1523/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 1524/25000, steps: 58, e: 0.48\n",
      "accumulated_rewards_per_episode: 0.4199999999999996\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1525/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1526/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 1527/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1528/25000, steps: 69, e: 0.48\n",
      "accumulated_rewards_per_episode: 0.3099999999999996\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 1529/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1530/25000, steps: 58, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5800000000000003\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1531/25000, steps: 61, e: 0.48\n",
      "accumulated_rewards_per_episode: 0.3899999999999997\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 1532/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 1533/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1534/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1535/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1536/25000, steps: 54, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5400000000000003\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 1537/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 1538/25000, steps: 118, e: 0.48\n",
      "accumulated_rewards_per_episode: -1.1800000000000008\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 1539/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 1540/25000, steps: 59, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5900000000000003\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 1541/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 1542/25000, steps: 58, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5800000000000003\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1543/25000, steps: 67, e: 0.48\n",
      "accumulated_rewards_per_episode: 0.3299999999999996\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 1544/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1545/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1546/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1547/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1548/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1549/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 1550/25000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1551/25000, steps: 76, e: 0.47\n",
      "accumulated_rewards_per_episode: -0.7600000000000005\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1552/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1553/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1554/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 1555/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 1556/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1557/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1558/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1559/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1560/25000, steps: 74, e: 0.47\n",
      "accumulated_rewards_per_episode: -0.7400000000000004\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1561/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1562/25000, steps: 298, e: 0.47\n",
      "accumulated_rewards_per_episode: -1.9799999999999924\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 1563/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 1564/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1565/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1566/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1567/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1568/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 1569/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1570/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1571/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 1572/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 1573/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1574/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1575/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 1576/25000, steps: 110, e: 0.47\n",
      "accumulated_rewards_per_episode: -1.1000000000000008\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1577/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 1578/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 1579/25000, steps: 91, e: 0.47\n",
      "accumulated_rewards_per_episode: -0.9100000000000006\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 1580/25000, steps: 66, e: 0.47\n",
      "accumulated_rewards_per_episode: 0.33999999999999964\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1581/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1582/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 1583/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 1584/25000, steps: 81, e: 0.47\n",
      "accumulated_rewards_per_episode: 0.1899999999999994\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 1585/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1586/25000, steps: 299, e: 0.47\n",
      "accumulated_rewards_per_episode: -1.9900000000000022\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1587/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 1588/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 1589/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 1590/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1591/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1592/25000, steps: 204, e: 0.47\n",
      "accumulated_rewards_per_episode: -2.0400000000000005\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 1593/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 1594/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 1595/25000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1596/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1597/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 1598/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 1599/25000, steps: 56, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5600000000000003\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1600/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1601/25000, steps: 63, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.6300000000000003\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1602/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1603/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 1604/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1605/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 1606/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 1607/25000, steps: 58, e: 0.46\n",
      "accumulated_rewards_per_episode: 0.4199999999999996\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 1608/25000, steps: 55, e: 0.46\n",
      "accumulated_rewards_per_episode: 0.44999999999999973\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1609/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 1610/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1611/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 1612/25000, steps: 246, e: 0.46\n",
      "accumulated_rewards_per_episode: -2.4599999999999915\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 1613/25000, steps: 163, e: 0.46\n",
      "accumulated_rewards_per_episode: -1.6300000000000012\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 1614/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1615/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1616/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 1617/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1618/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 1619/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 1620/25000, steps: 102, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.020000000000000573\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1621/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1622/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1623/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1624/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1625/25000, steps: 65, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.6500000000000004\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1626/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1627/25000, steps: 76, e: 0.46\n",
      "accumulated_rewards_per_episode: 0.23999999999999955\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 1628/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1629/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 1630/25000, steps: 61, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.6100000000000003\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1631/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 1632/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1633/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 1634/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1635/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 1636/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1637/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 1638/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 1639/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1640/25000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 1641/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 1642/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1643/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1644/25000, steps: 54, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5400000000000003\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 1645/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1646/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 1647/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 1648/25000, steps: 109, e: 0.45\n",
      "accumulated_rewards_per_episode: -1.0900000000000007\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1649/25000, steps: 69, e: 0.45\n",
      "accumulated_rewards_per_episode: 0.3099999999999995\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1650/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 1651/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 1652/25000, steps: 105, e: 0.45\n",
      "accumulated_rewards_per_episode: -1.0500000000000007\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 1653/25000, steps: 63, e: 0.45\n",
      "accumulated_rewards_per_episode: 0.36999999999999966\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1654/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1655/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1656/25000, steps: 73, e: 0.45\n",
      "accumulated_rewards_per_episode: 0.26999999999999946\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 1657/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 1658/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 1659/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 1660/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 1661/25000, steps: 58, e: 0.45\n",
      "accumulated_rewards_per_episode: 0.4199999999999996\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1662/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 1663/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1664/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 1665/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1666/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 1667/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1668/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 1669/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 1670/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 1671/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 1672/25000, steps: 68, e: 0.45\n",
      "accumulated_rewards_per_episode: 0.3199999999999995\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1673/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 1674/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1675/25000, steps: 67, e: 0.45\n",
      "accumulated_rewards_per_episode: 0.3299999999999994\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1676/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1677/25000, steps: 60, e: 0.45\n",
      "accumulated_rewards_per_episode: 0.39999999999999947\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1678/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1679/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1680/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 1681/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 1682/25000, steps: 73, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.7300000000000004\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1683/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1684/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1685/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1686/25000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 1687/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 1688/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 1689/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1690/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 1691/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1692/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1693/25000, steps: 439, e: 0.44\n",
      "accumulated_rewards_per_episode: -3.3899999999999726\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1694/25000, steps: 129, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.29000000000000087\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 1695/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 1696/25000, steps: 213, e: 0.44\n",
      "accumulated_rewards_per_episode: -1.1300000000000012\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1697/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 1698/25000, steps: 363, e: 0.44\n",
      "accumulated_rewards_per_episode: -2.629999999999989\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 1699/25000, steps: 79, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.2099999999999994\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1700/25000, steps: 108, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.08000000000000075\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 1701/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 1702/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 1703/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 1704/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1705/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1706/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 1707/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1708/25000, steps: 63, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.36999999999999966\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1709/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1710/25000, steps: 70, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.2999999999999994\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 1711/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 1712/25000, steps: 62, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.37999999999999967\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1713/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1714/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1715/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1716/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 1717/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1718/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1719/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1720/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1721/25000, steps: 92, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.07999999999999936\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1722/25000, steps: 292, e: 0.44\n",
      "accumulated_rewards_per_episode: -1.9200000000000021\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 1723/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1724/25000, steps: 91, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.08999999999999936\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1725/25000, steps: 69, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.6900000000000004\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 1726/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 1727/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1728/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1729/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 1730/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1731/25000, steps: 254, e: 0.44\n",
      "accumulated_rewards_per_episode: -1.5400000000000018\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 1732/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1733/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1734/25000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 1735/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1736/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1737/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 1738/25000, steps: 79, e: 0.43\n",
      "accumulated_rewards_per_episode: 0.2099999999999994\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1739/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 1740/25000, steps: 86, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.8600000000000005\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 1741/25000, steps: 523, e: 0.43\n",
      "accumulated_rewards_per_episode: -5.229999999999933\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1742/25000, steps: 181, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.8100000000000012\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1743/25000, steps: 60, e: 0.43\n",
      "accumulated_rewards_per_episode: 0.3999999999999997\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 1744/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 1745/25000, steps: 116, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.16000000000000064\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1746/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 1747/25000, steps: 70, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.7000000000000004\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1748/25000, steps: 65, e: 0.43\n",
      "accumulated_rewards_per_episode: 0.34999999999999964\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 1749/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1750/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1751/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1752/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1753/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1754/25000, steps: 75, e: 0.43\n",
      "accumulated_rewards_per_episode: 0.24999999999999933\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 1755/25000, steps: 118, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.18000000000000066\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1756/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1757/25000, steps: 430, e: 0.43\n",
      "accumulated_rewards_per_episode: -3.2999999999999745\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1758/25000, steps: 129, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.29000000000000065\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 1759/25000, steps: 477, e: 0.43\n",
      "accumulated_rewards_per_episode: -3.7699999999999645\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 1760/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1761/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1762/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1763/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 1764/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1765/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1766/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 1767/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 1768/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1769/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1770/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1771/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1772/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1773/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1774/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 1775/25000, steps: 800, e: 0.43\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1776/25000, steps: 524, e: 0.43\n",
      "accumulated_rewards_per_episode: -4.239999999999955\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1777/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1778/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1779/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1780/25000, steps: 54, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5400000000000003\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 1781/25000, steps: 53, e: 0.43\n",
      "accumulated_rewards_per_episode: 0.46999999999999964\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 1782/25000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 1783/25000, steps: 144, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.4400000000000009\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 1784/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 1785/25000, steps: 176, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.760000000000001\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1786/25000, steps: 123, e: 0.42\n",
      "accumulated_rewards_per_episode: -1.2300000000000009\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1787/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 1788/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1789/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 1790/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1791/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 1792/25000, steps: 100, e: 0.42\n",
      "accumulated_rewards_per_episode: -5.30825383648903e-16\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1793/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1794/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1795/25000, steps: 660, e: 0.42\n",
      "accumulated_rewards_per_episode: -5.599999999999926\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 1796/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1797/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1798/25000, steps: 74, e: 0.42\n",
      "accumulated_rewards_per_episode: 0.25999999999999956\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 1799/25000, steps: 229, e: 0.42\n",
      "accumulated_rewards_per_episode: -1.2900000000000014\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 1800/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 1801/25000, steps: 229, e: 0.42\n",
      "accumulated_rewards_per_episode: -1.2900000000000014\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1802/25000, steps: 337, e: 0.42\n",
      "accumulated_rewards_per_episode: -2.369999999999994\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1803/25000, steps: 59, e: 0.42\n",
      "accumulated_rewards_per_episode: 0.4099999999999996\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 1804/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1805/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 1806/25000, steps: 114, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.1400000000000005\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1807/25000, steps: 91, e: 0.42\n",
      "accumulated_rewards_per_episode: 0.08999999999999936\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1808/25000, steps: 62, e: 0.42\n",
      "accumulated_rewards_per_episode: 0.37999999999999967\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1809/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1810/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 1811/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 1812/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1813/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1814/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 1815/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 1816/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1817/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1818/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1819/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 1820/25000, steps: 278, e: 0.42\n",
      "accumulated_rewards_per_episode: -1.780000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 1821/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1822/25000, steps: 73, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.7300000000000004\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 1823/25000, steps: 89, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.8900000000000006\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 1824/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 1825/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 1826/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 1827/25000, steps: 91, e: 0.42\n",
      "accumulated_rewards_per_episode: 0.08999999999999947\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 1828/25000, steps: 121, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.2100000000000006\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 1829/25000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 1830/25000, steps: 68, e: 0.42\n",
      "accumulated_rewards_per_episode: 0.3199999999999994\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1831/25000, steps: 388, e: 0.42\n",
      "accumulated_rewards_per_episode: -2.8799999999999835\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 1832/25000, steps: 535, e: 0.42\n",
      "accumulated_rewards_per_episode: -5.34999999999993\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1833/25000, steps: 73, e: 0.41\n",
      "accumulated_rewards_per_episode: 0.2699999999999996\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 1834/25000, steps: 166, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.6600000000000009\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 1835/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1836/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 1837/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1838/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1839/25000, steps: 319, e: 0.41\n",
      "accumulated_rewards_per_episode: -2.189999999999998\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 1840/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 1841/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 1842/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1843/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 1844/25000, steps: 72, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.7200000000000004\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 1845/25000, steps: 58, e: 0.41\n",
      "accumulated_rewards_per_episode: 0.4199999999999997\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 1846/25000, steps: 59, e: 0.41\n",
      "accumulated_rewards_per_episode: 0.4099999999999997\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 1847/25000, steps: 83, e: 0.41\n",
      "accumulated_rewards_per_episode: 0.16999999999999948\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1848/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1849/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1850/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 1851/25000, steps: 597, e: 0.41\n",
      "accumulated_rewards_per_episode: -4.969999999999939\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1852/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1853/25000, steps: 800, e: 0.41\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 1854/25000, steps: 462, e: 0.41\n",
      "accumulated_rewards_per_episode: -3.6199999999999672\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 1855/25000, steps: 73, e: 0.41\n",
      "accumulated_rewards_per_episode: 0.2699999999999996\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1856/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 1857/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 1858/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1859/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1860/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1861/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1862/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 1863/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 1864/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 1865/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1866/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1867/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1868/25000, steps: 781, e: 0.41\n",
      "accumulated_rewards_per_episode: -6.809999999999899\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 1869/25000, steps: 230, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.30000000000000127\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 1870/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 1871/25000, steps: 93, e: 0.41\n",
      "accumulated_rewards_per_episode: 1.0699999999999994\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1872/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1873/25000, steps: 64, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.6400000000000003\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1874/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1875/25000, steps: 520, e: 0.41\n",
      "accumulated_rewards_per_episode: -4.199999999999956\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1876/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 1877/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1878/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1879/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 1880/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1881/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 1882/25000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 1883/25000, steps: 358, e: 0.41\n",
      "accumulated_rewards_per_episode: -2.5799999999999894\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1884/25000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 1885/25000, steps: 73, e: 0.4\n",
      "accumulated_rewards_per_episode: 0.2699999999999996\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 1886/25000, steps: 223, e: 0.4\n",
      "accumulated_rewards_per_episode: -1.2300000000000018\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1887/25000, steps: 156, e: 0.4\n",
      "accumulated_rewards_per_episode: 0.4399999999999986\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 1888/25000, steps: 184, e: 0.4\n",
      "accumulated_rewards_per_episode: -0.8400000000000011\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 1889/25000, steps: 75, e: 0.4\n",
      "accumulated_rewards_per_episode: 0.24999999999999944\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 1890/25000, steps: 56, e: 0.4\n",
      "accumulated_rewards_per_episode: 1.4399999999999995\n",
      "START state: (0, 0, 4, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1891/25000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 1892/25000, steps: 596, e: 0.4\n",
      "accumulated_rewards_per_episode: -5.959999999999917\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1893/25000, steps: 146, e: 0.4\n",
      "accumulated_rewards_per_episode: -1.460000000000001\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 1894/25000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1895/25000, steps: 800, e: 0.4\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1896/25000, steps: 293, e: 0.4\n",
      "accumulated_rewards_per_episode: -1.929999999999997\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1897/25000, steps: 596, e: 0.4\n",
      "accumulated_rewards_per_episode: -4.959999999999939\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1898/25000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 1899/25000, steps: 57, e: 0.4\n",
      "accumulated_rewards_per_episode: -0.5700000000000003\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1900/25000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 1901/25000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1902/25000, steps: 148, e: 0.4\n",
      "accumulated_rewards_per_episode: -1.480000000000001\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 1903/25000, steps: 141, e: 0.4\n",
      "accumulated_rewards_per_episode: -1.410000000000001\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 1904/25000, steps: 61, e: 0.4\n",
      "accumulated_rewards_per_episode: 0.38999999999999957\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1905/25000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1906/25000, steps: 136, e: 0.4\n",
      "accumulated_rewards_per_episode: 0.6399999999999992\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 1907/25000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 1908/25000, steps: 646, e: 0.4\n",
      "accumulated_rewards_per_episode: -4.459999999999928\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1909/25000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1910/25000, steps: 251, e: 0.4\n",
      "accumulated_rewards_per_episode: -0.5100000000000016\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1911/25000, steps: 129, e: 0.4\n",
      "accumulated_rewards_per_episode: -0.2900000000000007\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 1912/25000, steps: 181, e: 0.4\n",
      "accumulated_rewards_per_episode: -0.810000000000001\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1913/25000, steps: 347, e: 0.4\n",
      "accumulated_rewards_per_episode: -3.46999999999997\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 1914/25000, steps: 150, e: 0.4\n",
      "accumulated_rewards_per_episode: 0.4999999999999989\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1915/25000, steps: 800, e: 0.4\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 1916/25000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 1917/25000, steps: 800, e: 0.4\n",
      "accumulated_rewards_per_episode: -5.999999999999895\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 1918/25000, steps: 106, e: 0.4\n",
      "accumulated_rewards_per_episode: -1.0600000000000007\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 1919/25000, steps: 140, e: 0.4\n",
      "accumulated_rewards_per_episode: -0.4000000000000008\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1920/25000, steps: 53, e: 0.4\n",
      "accumulated_rewards_per_episode: -0.5300000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 1921/25000, steps: 307, e: 0.4\n",
      "accumulated_rewards_per_episode: -2.0699999999999825\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 1922/25000, steps: 170, e: 0.4\n",
      "accumulated_rewards_per_episode: -0.7000000000000012\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1923/25000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1924/25000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 1925/25000, steps: 70, e: 0.4\n",
      "accumulated_rewards_per_episode: -0.7000000000000004\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1926/25000, steps: 133, e: 0.4\n",
      "accumulated_rewards_per_episode: -0.3300000000000008\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1927/25000, steps: 62, e: 0.4\n",
      "accumulated_rewards_per_episode: 0.37999999999999956\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 1928/25000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1929/25000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 1930/25000, steps: 792, e: 0.4\n",
      "accumulated_rewards_per_episode: -6.919999999999898\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1931/25000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1932/25000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1933/25000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1934/25000, steps: 67, e: 0.4\n",
      "accumulated_rewards_per_episode: 0.3299999999999996\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 1935/25000, steps: 66, e: 0.4\n",
      "accumulated_rewards_per_episode: 0.3399999999999994\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1936/25000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 1937/25000, steps: 58, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.5800000000000003\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 1938/25000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1939/25000, steps: 203, e: 0.39\n",
      "accumulated_rewards_per_episode: -1.0300000000000014\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1940/25000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1941/25000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 1942/25000, steps: 800, e: 0.39\n",
      "accumulated_rewards_per_episode: -6.999999999999875\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1943/25000, steps: 459, e: 0.39\n",
      "accumulated_rewards_per_episode: -2.5899999999999688\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1944/25000, steps: 364, e: 0.39\n",
      "accumulated_rewards_per_episode: -2.6399999999999886\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1945/25000, steps: 83, e: 0.39\n",
      "accumulated_rewards_per_episode: 0.16999999999999937\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1946/25000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 1947/25000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1948/25000, steps: 72, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.7200000000000004\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1949/25000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1950/25000, steps: 54, e: 0.39\n",
      "accumulated_rewards_per_episode: 0.45999999999999974\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1951/25000, steps: 81, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.8100000000000005\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1952/25000, steps: 346, e: 0.39\n",
      "accumulated_rewards_per_episode: -2.4599999999999924\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1953/25000, steps: 607, e: 0.39\n",
      "accumulated_rewards_per_episode: -5.069999999999915\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1954/25000, steps: 167, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.670000000000001\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1955/25000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 1956/25000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 1957/25000, steps: 73, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.7300000000000004\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1958/25000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1959/25000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1960/25000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1961/25000, steps: 146, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.4600000000000009\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1962/25000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 1963/25000, steps: 129, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.29000000000000076\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1964/25000, steps: 133, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.3300000000000008\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1965/25000, steps: 378, e: 0.39\n",
      "accumulated_rewards_per_episode: -2.7799999999999856\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1966/25000, steps: 760, e: 0.39\n",
      "accumulated_rewards_per_episode: -4.599999999999948\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1967/25000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 1968/25000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1969/25000, steps: 200, e: 0.39\n",
      "accumulated_rewards_per_episode: -1.0000000000000013\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1970/25000, steps: 800, e: 0.39\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 1971/25000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1972/25000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1973/25000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1974/25000, steps: 800, e: 0.39\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 1975/25000, steps: 64, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.6400000000000003\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1976/25000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1977/25000, steps: 800, e: 0.39\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 1978/25000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1979/25000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1980/25000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1981/25000, steps: 234, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.34000000000000163\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 1982/25000, steps: 205, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.050000000000001314\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1983/25000, steps: 202, e: 0.39\n",
      "accumulated_rewards_per_episode: -1.0200000000000016\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 1984/25000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1985/25000, steps: 373, e: 0.39\n",
      "accumulated_rewards_per_episode: -1.7299999999999955\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1986/25000, steps: 452, e: 0.39\n",
      "accumulated_rewards_per_episode: -3.51999999999997\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1987/25000, steps: 374, e: 0.39\n",
      "accumulated_rewards_per_episode: -2.739999999999986\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1988/25000, steps: 367, e: 0.39\n",
      "accumulated_rewards_per_episode: -3.6699999999999657\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 1989/25000, steps: 105, e: 0.38\n",
      "accumulated_rewards_per_episode: 0.9499999999999993\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 1990/25000, steps: 433, e: 0.38\n",
      "accumulated_rewards_per_episode: -3.329999999999974\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1991/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1992/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1993/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 1994/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1995/25000, steps: 352, e: 0.38\n",
      "accumulated_rewards_per_episode: -1.519999999999995\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 1996/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 1997/25000, steps: 800, e: 0.38\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1998/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1999/25000, steps: 220, e: 0.38\n",
      "accumulated_rewards_per_episode: -1.2000000000000015\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 2000/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2001/25000, steps: 66, e: 0.38\n",
      "accumulated_rewards_per_episode: -0.6600000000000004\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 2002/25000, steps: 95, e: 0.38\n",
      "accumulated_rewards_per_episode: 0.049999999999999475\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2003/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 2004/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2005/25000, steps: 90, e: 0.38\n",
      "accumulated_rewards_per_episode: 0.09999999999999946\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2006/25000, steps: 800, e: 0.38\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2007/25000, steps: 620, e: 0.38\n",
      "accumulated_rewards_per_episode: -4.1999999999999345\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2008/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 2009/25000, steps: 800, e: 0.38\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 2010/25000, steps: 800, e: 0.38\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2011/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2012/25000, steps: 137, e: 0.38\n",
      "accumulated_rewards_per_episode: -0.370000000000001\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2013/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2014/25000, steps: 337, e: 0.38\n",
      "accumulated_rewards_per_episode: -2.3699999999999943\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 2015/25000, steps: 130, e: 0.38\n",
      "accumulated_rewards_per_episode: -0.3000000000000009\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 2016/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 2017/25000, steps: 800, e: 0.38\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 2018/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2019/25000, steps: 665, e: 0.38\n",
      "accumulated_rewards_per_episode: -5.649999999999925\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2020/25000, steps: 218, e: 0.38\n",
      "accumulated_rewards_per_episode: -1.1800000000000017\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2021/25000, steps: 800, e: 0.38\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2022/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2023/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 2024/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2025/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2026/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2027/25000, steps: 127, e: 0.38\n",
      "accumulated_rewards_per_episode: -0.27000000000000063\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2028/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2029/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2030/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2031/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 2032/25000, steps: 800, e: 0.38\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2033/25000, steps: 335, e: 0.38\n",
      "accumulated_rewards_per_episode: -2.3499999999999948\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 2034/25000, steps: 115, e: 0.38\n",
      "accumulated_rewards_per_episode: -0.15000000000000066\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 2035/25000, steps: 690, e: 0.38\n",
      "accumulated_rewards_per_episode: -5.899999999999919\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2036/25000, steps: 69, e: 0.38\n",
      "accumulated_rewards_per_episode: 0.3099999999999996\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2037/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 2038/25000, steps: 110, e: 0.38\n",
      "accumulated_rewards_per_episode: 0.8999999999999992\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2039/25000, steps: 526, e: 0.38\n",
      "accumulated_rewards_per_episode: -4.259999999999954\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 2040/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2041/25000, steps: 75, e: 0.38\n",
      "accumulated_rewards_per_episode: 0.24999999999999956\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 2042/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2043/25000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 2044/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 2045/25000, steps: 90, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.9000000000000006\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 2046/25000, steps: 109, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.09000000000000064\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2047/25000, steps: 523, e: 0.37\n",
      "accumulated_rewards_per_episode: -4.229999999999955\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2048/25000, steps: 82, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.1799999999999995\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 2049/25000, steps: 109, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.09000000000000075\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 2050/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2051/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2052/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 2053/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2054/25000, steps: 83, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.16999999999999948\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2055/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 2056/25000, steps: 66, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.6600000000000004\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2057/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 2058/25000, steps: 162, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.6200000000000009\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2059/25000, steps: 112, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.12000000000000062\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2060/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 2061/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2062/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 2063/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 2064/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 2065/25000, steps: 518, e: 0.37\n",
      "accumulated_rewards_per_episode: -3.1799999999999553\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2066/25000, steps: 128, e: 0.37\n",
      "accumulated_rewards_per_episode: -1.280000000000001\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2067/25000, steps: 387, e: 0.37\n",
      "accumulated_rewards_per_episode: -2.8699999999999832\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 2068/25000, steps: 86, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.13999999999999935\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 2069/25000, steps: 86, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.13999999999999946\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2070/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 2071/25000, steps: 108, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.08000000000000064\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2072/25000, steps: 165, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.349999999999999\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2073/25000, steps: 143, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.5699999999999988\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 2074/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2075/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 2076/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2077/25000, steps: 800, e: 0.37\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2078/25000, steps: 98, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.9800000000000006\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2079/25000, steps: 66, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.3399999999999994\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2080/25000, steps: 96, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.03999999999999936\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2081/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2082/25000, steps: 590, e: 0.37\n",
      "accumulated_rewards_per_episode: -4.89999999999994\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 2083/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2084/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2085/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2086/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2087/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 2088/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 2089/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2090/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 2091/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 2092/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2093/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2094/25000, steps: 412, e: 0.37\n",
      "accumulated_rewards_per_episode: -3.119999999999978\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 2095/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2096/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 2097/25000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 2098/25000, steps: 182, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.8200000000000013\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2099/25000, steps: 63, e: 0.37\n",
      "accumulated_rewards_per_episode: -0.6300000000000003\n",
      "START state: (0, 0, 8, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2100/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 2101/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2102/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2103/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 2104/25000, steps: 776, e: 0.36\n",
      "accumulated_rewards_per_episode: -5.759999999999901\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2105/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2106/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 2107/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2108/25000, steps: 90, e: 0.36\n",
      "accumulated_rewards_per_episode: 0.09999999999999935\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2109/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 2110/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 2111/25000, steps: 473, e: 0.36\n",
      "accumulated_rewards_per_episode: -3.7299999999999653\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 2112/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: 1.4799999999999998\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2113/25000, steps: 119, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.19000000000000056\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 2114/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 2115/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2116/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 2117/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2118/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2119/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 2120/25000, steps: 163, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.6300000000000009\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 2121/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 2122/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 2123/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2124/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2125/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2126/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2127/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2128/25000, steps: 91, e: 0.36\n",
      "accumulated_rewards_per_episode: 0.08999999999999936\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2129/25000, steps: 61, e: 0.36\n",
      "accumulated_rewards_per_episode: 0.38999999999999946\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 2130/25000, steps: 142, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.42000000000000076\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2131/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2132/25000, steps: 246, e: 0.36\n",
      "accumulated_rewards_per_episode: -1.4600000000000017\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2133/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 2134/25000, steps: 333, e: 0.36\n",
      "accumulated_rewards_per_episode: -1.3299999999999963\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 2135/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2136/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 2137/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2138/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 2139/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2140/25000, steps: 58, e: 0.36\n",
      "accumulated_rewards_per_episode: 0.4199999999999997\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2141/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 2142/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2143/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2144/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2145/25000, steps: 69, e: 0.36\n",
      "accumulated_rewards_per_episode: 0.3099999999999996\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2146/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 2147/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 2148/25000, steps: 193, e: 0.36\n",
      "accumulated_rewards_per_episode: 0.06999999999999892\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 2149/25000, steps: 436, e: 0.36\n",
      "accumulated_rewards_per_episode: -3.359999999999973\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 2150/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2151/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 2152/25000, steps: 157, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5700000000000011\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2153/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 2154/25000, steps: 263, e: 0.36\n",
      "accumulated_rewards_per_episode: -1.630000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2155/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 2156/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 2157/25000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 2158/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2159/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2160/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2161/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 2162/25000, steps: 63, e: 0.35\n",
      "accumulated_rewards_per_episode: 0.36999999999999955\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 2163/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2164/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 2165/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 2166/25000, steps: 106, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.06000000000000065\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 2167/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 2168/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2169/25000, steps: 94, e: 0.35\n",
      "accumulated_rewards_per_episode: 0.059999999999999255\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 2170/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 2171/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2172/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2173/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2174/25000, steps: 96, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.9600000000000006\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 2175/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 2176/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 2177/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 2178/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 2179/25000, steps: 310, e: 0.35\n",
      "accumulated_rewards_per_episode: -3.099999999999978\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 2180/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2181/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2182/25000, steps: 107, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.07000000000000076\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 2183/25000, steps: 78, e: 0.35\n",
      "accumulated_rewards_per_episode: 0.21999999999999953\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2184/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 2185/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2186/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2187/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2188/25000, steps: 84, e: 0.35\n",
      "accumulated_rewards_per_episode: 0.15999999999999936\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 2189/25000, steps: 113, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.1300000000000005\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2190/25000, steps: 221, e: 0.35\n",
      "accumulated_rewards_per_episode: -1.2100000000000013\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 2191/25000, steps: 92, e: 0.35\n",
      "accumulated_rewards_per_episode: 0.07999999999999936\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 2192/25000, steps: 193, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.9300000000000013\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2193/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2194/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2195/25000, steps: 800, e: 0.35\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 2196/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 2197/25000, steps: 427, e: 0.35\n",
      "accumulated_rewards_per_episode: -3.2699999999999747\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2198/25000, steps: 68, e: 0.35\n",
      "accumulated_rewards_per_episode: 0.3199999999999995\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2199/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2200/25000, steps: 481, e: 0.35\n",
      "accumulated_rewards_per_episode: -3.8099999999999636\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 2201/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2202/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2203/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2204/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 2205/25000, steps: 92, e: 0.35\n",
      "accumulated_rewards_per_episode: 0.07999999999999947\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2206/25000, steps: 223, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.23000000000000115\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2207/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2208/25000, steps: 111, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.11000000000000063\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2209/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2210/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2211/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2212/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2213/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 2214/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2215/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2216/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 2217/25000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 2218/25000, steps: 113, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.13000000000000056\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2219/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 2220/25000, steps: 372, e: 0.34\n",
      "accumulated_rewards_per_episode: -2.7199999999999864\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 2221/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 2222/25000, steps: 79, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.2099999999999994\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 2223/25000, steps: 233, e: 0.34\n",
      "accumulated_rewards_per_episode: -1.3300000000000014\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 2224/25000, steps: 242, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.4200000000000013\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2225/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2226/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 2227/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2228/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 2229/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 2230/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 2231/25000, steps: 79, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.2099999999999993\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 2232/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2233/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2234/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 2235/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 2236/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2237/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2238/25000, steps: 429, e: 0.34\n",
      "accumulated_rewards_per_episode: -4.289999999999953\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 2239/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 2240/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2241/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2242/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 2243/25000, steps: 717, e: 0.34\n",
      "accumulated_rewards_per_episode: -6.169999999999892\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 2244/25000, steps: 97, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.02999999999999936\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2245/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2246/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 2247/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 2248/25000, steps: 74, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.25999999999999934\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2249/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 2250/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2251/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 2252/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2253/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 2254/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 2255/25000, steps: 56, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.4399999999999996\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2256/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 2257/25000, steps: 112, e: 0.34\n",
      "accumulated_rewards_per_episode: -1.1200000000000008\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 2258/25000, steps: 187, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.8700000000000014\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2259/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2260/25000, steps: 652, e: 0.34\n",
      "accumulated_rewards_per_episode: -4.519999999999928\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 2261/25000, steps: 69, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.3099999999999995\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2262/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2263/25000, steps: 142, e: 0.34\n",
      "accumulated_rewards_per_episode: -1.420000000000001\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 2264/25000, steps: 65, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.34999999999999953\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 2265/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2266/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2267/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2268/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2269/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 2270/25000, steps: 180, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.19999999999999862\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 2271/25000, steps: 85, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.14999999999999947\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2272/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 2273/25000, steps: 109, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.09000000000000064\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2274/25000, steps: 53, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.46999999999999953\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2275/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 2276/25000, steps: 71, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.2899999999999996\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2277/25000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2278/25000, steps: 62, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.37999999999999967\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2279/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 2280/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2281/25000, steps: 58, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.4199999999999997\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 2282/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 2283/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 2284/25000, steps: 53, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.46999999999999964\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 2285/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2286/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 2287/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 2288/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2289/25000, steps: 515, e: 0.33\n",
      "accumulated_rewards_per_episode: -3.149999999999957\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 2290/25000, steps: 106, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.06000000000000065\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 2291/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 2292/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 2293/25000, steps: 87, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.12999999999999945\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 2294/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2295/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2296/25000, steps: 72, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.27999999999999936\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2297/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2298/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2299/25000, steps: 124, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.24000000000000066\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 2300/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 2301/25000, steps: 56, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.4399999999999997\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 2302/25000, steps: 58, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.4199999999999996\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2303/25000, steps: 77, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.22999999999999954\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2304/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2305/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 2306/25000, steps: 53, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.46999999999999975\n",
      "START state: (0, 0, 9, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2307/25000, steps: 106, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.06000000000000054\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 2308/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 2309/25000, steps: 70, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.2999999999999994\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2310/25000, steps: 218, e: 0.33\n",
      "accumulated_rewards_per_episode: -1.1800000000000015\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2311/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 2312/25000, steps: 519, e: 0.33\n",
      "accumulated_rewards_per_episode: -4.189999999999955\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2313/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 2314/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2315/25000, steps: 304, e: 0.33\n",
      "accumulated_rewards_per_episode: -1.0400000000000018\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 2316/25000, steps: 384, e: 0.33\n",
      "accumulated_rewards_per_episode: -2.8399999999999843\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 2317/25000, steps: 66, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.33999999999999964\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2318/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 2319/25000, steps: 90, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.09999999999999946\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2320/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2321/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2322/25000, steps: 367, e: 0.33\n",
      "accumulated_rewards_per_episode: -2.669999999999988\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2323/25000, steps: 77, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.22999999999999954\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 2324/25000, steps: 800, e: 0.33\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2325/25000, steps: 230, e: 0.33\n",
      "accumulated_rewards_per_episode: -1.3000000000000016\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2326/25000, steps: 85, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.14999999999999947\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 2327/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2328/25000, steps: 130, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.30000000000000066\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 2329/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 2330/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2331/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 2332/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 2333/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2334/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2335/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 2336/25000, steps: 800, e: 0.33\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 2337/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2338/25000, steps: 330, e: 0.33\n",
      "accumulated_rewards_per_episode: -3.2999999999999736\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 2339/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2340/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2341/25000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2342/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2343/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2344/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 2345/25000, steps: 800, e: 0.32\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2346/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2347/25000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: -4.999999999999939\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2348/25000, steps: 119, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.19000000000000067\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 2349/25000, steps: 135, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.3500000000000009\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 2350/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2351/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2352/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 2353/25000, steps: 594, e: 0.32\n",
      "accumulated_rewards_per_episode: -4.939999999999939\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 2354/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2355/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 2356/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2357/25000, steps: 70, e: 0.32\n",
      "accumulated_rewards_per_episode: 0.2999999999999995\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2358/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2359/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2360/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 2361/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2362/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 2363/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2364/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2365/25000, steps: 646, e: 0.32\n",
      "accumulated_rewards_per_episode: -5.459999999999928\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 2366/25000, steps: 210, e: 0.32\n",
      "accumulated_rewards_per_episode: -1.1000000000000014\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 2367/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2368/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 2369/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 2370/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 2371/25000, steps: 128, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.28000000000000064\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 2372/25000, steps: 243, e: 0.32\n",
      "accumulated_rewards_per_episode: -1.4300000000000017\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 2373/25000, steps: 153, e: 0.32\n",
      "accumulated_rewards_per_episode: 0.46999999999999864\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 2374/25000, steps: 148, e: 0.32\n",
      "accumulated_rewards_per_episode: -1.480000000000001\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2375/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2376/25000, steps: 572, e: 0.32\n",
      "accumulated_rewards_per_episode: -4.719999999999945\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2377/25000, steps: 83, e: 0.32\n",
      "accumulated_rewards_per_episode: 0.16999999999999937\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 2378/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2379/25000, steps: 591, e: 0.32\n",
      "accumulated_rewards_per_episode: -4.909999999999918\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2380/25000, steps: 79, e: 0.32\n",
      "accumulated_rewards_per_episode: 0.2099999999999993\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2381/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2382/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2383/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 2384/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2385/25000, steps: 62, e: 0.32\n",
      "accumulated_rewards_per_episode: 0.37999999999999967\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2386/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 2387/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 2388/25000, steps: 707, e: 0.32\n",
      "accumulated_rewards_per_episode: -6.069999999999916\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2389/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 2390/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2391/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 2392/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2393/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 2394/25000, steps: 61, e: 0.32\n",
      "accumulated_rewards_per_episode: 1.3899999999999997\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 2395/25000, steps: 65, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.6500000000000004\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 2396/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2397/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 2398/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2399/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 2400/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 2401/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2402/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2403/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2404/25000, steps: 194, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.9400000000000012\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 2405/25000, steps: 61, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.6100000000000003\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2406/25000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 2407/25000, steps: 567, e: 0.31\n",
      "accumulated_rewards_per_episode: -4.669999999999946\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 2408/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 2409/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2410/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2411/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2412/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 2413/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2414/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 2415/25000, steps: 63, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.36999999999999966\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2416/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 2417/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 2418/25000, steps: 165, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.650000000000001\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2419/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 2420/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2421/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2422/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2423/25000, steps: 309, e: 0.31\n",
      "accumulated_rewards_per_episode: -2.0900000000000003\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2424/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2425/25000, steps: 112, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.12000000000000052\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2426/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 2427/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2428/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2429/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2430/25000, steps: 55, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5500000000000003\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 2431/25000, steps: 472, e: 0.31\n",
      "accumulated_rewards_per_episode: -3.7199999999999656\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 2432/25000, steps: 123, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.2300000000000007\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2433/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2434/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2435/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 2436/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2437/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2438/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2439/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 2440/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 2441/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2442/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2443/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2444/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 2445/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2446/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 2447/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 2448/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 2449/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2450/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2451/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2452/25000, steps: 376, e: 0.31\n",
      "accumulated_rewards_per_episode: -2.759999999999986\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2453/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 2454/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2455/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 2456/25000, steps: 64, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.35999999999999965\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 2457/25000, steps: 78, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.21999999999999953\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2458/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 2459/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 2460/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2461/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2462/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2463/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 2464/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2465/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2466/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 2467/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 2468/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2469/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 2470/25000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 2471/25000, steps: 411, e: 0.31\n",
      "accumulated_rewards_per_episode: -2.1100000000000003\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 2472/25000, steps: 251, e: 0.31\n",
      "accumulated_rewards_per_episode: -1.5100000000000018\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 2473/25000, steps: 357, e: 0.31\n",
      "accumulated_rewards_per_episode: -1.5700000000000027\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 2474/25000, steps: 129, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.29000000000000065\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2475/25000, steps: 89, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.10999999999999946\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 2476/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2477/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2478/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2479/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2480/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2481/25000, steps: 91, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.9100000000000006\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 2482/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2483/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 2484/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2485/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2486/25000, steps: 224, e: 0.3\n",
      "accumulated_rewards_per_episode: -1.2400000000000018\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2487/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2488/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2489/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2490/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 2491/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 2492/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 2493/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2494/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 2495/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 2496/25000, steps: 58, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.4199999999999997\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2497/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 2498/25000, steps: 316, e: 0.3\n",
      "accumulated_rewards_per_episode: -2.1599999999999984\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 2499/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 2500/25000, steps: 101, e: 0.3\n",
      "accumulated_rewards_per_episode: -1.0100000000000007\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 2501/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 2502/25000, steps: 61, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.38999999999999957\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 2503/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2504/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 2505/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 2506/25000, steps: 800, e: 0.3\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2507/25000, steps: 124, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.24000000000000074\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 2508/25000, steps: 123, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.2300000000000007\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 2509/25000, steps: 605, e: 0.3\n",
      "accumulated_rewards_per_episode: -5.049999999999938\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2510/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 2511/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2512/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 2513/25000, steps: 123, e: 0.3\n",
      "accumulated_rewards_per_episode: -1.2300000000000009\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 2514/25000, steps: 60, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.3999999999999996\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2515/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 2516/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2517/25000, steps: 68, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.3199999999999995\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 2518/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2519/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 2520/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2521/25000, steps: 155, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5500000000000008\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 2522/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2523/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 2524/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 2525/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 2526/25000, steps: 362, e: 0.3\n",
      "accumulated_rewards_per_episode: -2.6199999999999886\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2527/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2528/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 2529/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 2530/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 2531/25000, steps: 55, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5500000000000003\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 2532/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2533/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2534/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 2535/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 2536/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2537/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 2538/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2539/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 2540/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2541/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2542/25000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2543/25000, steps: 62, e: 0.3\n",
      "accumulated_rewards_per_episode: 1.3799999999999994\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 2544/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2545/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 2546/25000, steps: 661, e: 0.29\n",
      "accumulated_rewards_per_episode: -5.609999999999926\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 2547/25000, steps: 58, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.4199999999999997\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2548/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2549/25000, steps: 73, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.2699999999999996\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 2550/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2551/25000, steps: 185, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.1499999999999988\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 2552/25000, steps: 313, e: 0.29\n",
      "accumulated_rewards_per_episode: -1.1300000000000021\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2553/25000, steps: 226, e: 0.29\n",
      "accumulated_rewards_per_episode: -1.2600000000000013\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2554/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2555/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 2556/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 2557/25000, steps: 126, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.26000000000000073\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2558/25000, steps: 359, e: 0.29\n",
      "accumulated_rewards_per_episode: -2.5899999999999896\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 2559/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 2560/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2561/25000, steps: 66, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.33999999999999964\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 2562/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 2563/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 2564/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 2565/25000, steps: 800, e: 0.29\n",
      "accumulated_rewards_per_episode: -5.999999999999917\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2566/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 2567/25000, steps: 73, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.2699999999999996\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2568/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 2569/25000, steps: 72, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.7200000000000004\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 2570/25000, steps: 94, e: 0.29\n",
      "accumulated_rewards_per_episode: 1.0599999999999992\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 2571/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 2572/25000, steps: 797, e: 0.29\n",
      "accumulated_rewards_per_episode: -6.969999999999896\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 2573/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 2574/25000, steps: 134, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.3400000000000007\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2575/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2576/25000, steps: 104, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.040000000000000605\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2577/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 2578/25000, steps: 113, e: 0.29\n",
      "accumulated_rewards_per_episode: -1.1300000000000008\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 2579/25000, steps: 721, e: 0.29\n",
      "accumulated_rewards_per_episode: -6.209999999999913\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2580/25000, steps: 95, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.049999999999999364\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 2581/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 2582/25000, steps: 320, e: 0.29\n",
      "accumulated_rewards_per_episode: -2.199999999999998\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 2583/25000, steps: 56, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5600000000000003\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2584/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2585/25000, steps: 735, e: 0.29\n",
      "accumulated_rewards_per_episode: -6.34999999999991\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2586/25000, steps: 454, e: 0.29\n",
      "accumulated_rewards_per_episode: -3.539999999999969\n",
      "START state: (0, 0, 4, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2587/25000, steps: 475, e: 0.29\n",
      "accumulated_rewards_per_episode: -3.749999999999943\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 2588/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2589/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2590/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 2591/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2592/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2593/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2594/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2595/25000, steps: 106, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.06000000000000065\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2596/25000, steps: 519, e: 0.29\n",
      "accumulated_rewards_per_episode: -3.189999999999956\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2597/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2598/25000, steps: 68, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.3199999999999994\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 2599/25000, steps: 101, e: 0.29\n",
      "accumulated_rewards_per_episode: -1.0100000000000007\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 2600/25000, steps: 114, e: 0.29\n",
      "accumulated_rewards_per_episode: -1.1400000000000008\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2601/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2602/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 2603/25000, steps: 76, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.23999999999999955\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2604/25000, steps: 154, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5400000000000009\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 2605/25000, steps: 75, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.24999999999999944\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2606/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 2607/25000, steps: 800, e: 0.29\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 2608/25000, steps: 68, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.6800000000000004\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2609/25000, steps: 77, e: 0.29\n",
      "accumulated_rewards_per_episode: 1.2299999999999993\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2610/25000, steps: 126, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.26000000000000073\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2611/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 2612/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2613/25000, steps: 59, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.4099999999999997\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 2614/25000, steps: 305, e: 0.29\n",
      "accumulated_rewards_per_episode: -1.050000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2615/25000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 2616/25000, steps: 786, e: 0.28\n",
      "accumulated_rewards_per_episode: -6.859999999999877\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 2617/25000, steps: 94, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.9400000000000006\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 2618/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 2619/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 2620/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2621/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2622/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 2623/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 2624/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2625/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 2626/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 2627/25000, steps: 114, e: 0.28\n",
      "accumulated_rewards_per_episode: -1.1400000000000008\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 2628/25000, steps: 172, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.7200000000000011\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2629/25000, steps: 243, e: 0.28\n",
      "accumulated_rewards_per_episode: -1.4300000000000017\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2630/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 2631/25000, steps: 105, e: 0.28\n",
      "accumulated_rewards_per_episode: -1.0500000000000007\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 2632/25000, steps: 571, e: 0.28\n",
      "accumulated_rewards_per_episode: -4.709999999999945\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2633/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2634/25000, steps: 133, e: 0.28\n",
      "accumulated_rewards_per_episode: -1.330000000000001\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2635/25000, steps: 330, e: 0.28\n",
      "accumulated_rewards_per_episode: -3.2999999999999736\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 2636/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 2637/25000, steps: 234, e: 0.28\n",
      "accumulated_rewards_per_episode: -1.3400000000000014\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 2638/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 2639/25000, steps: 111, e: 0.28\n",
      "accumulated_rewards_per_episode: 0.889999999999999\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2640/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 2641/25000, steps: 209, e: 0.28\n",
      "accumulated_rewards_per_episode: -1.0900000000000014\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 2642/25000, steps: 115, e: 0.28\n",
      "accumulated_rewards_per_episode: -1.1500000000000008\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2643/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 2644/25000, steps: 329, e: 0.28\n",
      "accumulated_rewards_per_episode: -3.289999999999974\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2645/25000, steps: 373, e: 0.28\n",
      "accumulated_rewards_per_episode: -2.7299999999999716\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2646/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2647/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2648/25000, steps: 355, e: 0.28\n",
      "accumulated_rewards_per_episode: -1.5500000000000025\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 2649/25000, steps: 419, e: 0.28\n",
      "accumulated_rewards_per_episode: -2.189999999999994\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 2650/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 2651/25000, steps: 255, e: 0.28\n",
      "accumulated_rewards_per_episode: -1.5500000000000018\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 2652/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 2653/25000, steps: 335, e: 0.28\n",
      "accumulated_rewards_per_episode: -2.3499999999999748\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2654/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2655/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2656/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2657/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2658/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 2659/25000, steps: 190, e: 0.28\n",
      "accumulated_rewards_per_episode: -1.9000000000000015\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2660/25000, steps: 158, e: 0.28\n",
      "accumulated_rewards_per_episode: -1.5800000000000012\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2661/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 2662/25000, steps: 409, e: 0.28\n",
      "accumulated_rewards_per_episode: -3.0899999999999785\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2663/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 2664/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 2665/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 2666/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 2667/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 2668/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 2669/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2670/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 2671/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2672/25000, steps: 124, e: 0.28\n",
      "accumulated_rewards_per_episode: -1.2400000000000009\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2673/25000, steps: 192, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.9200000000000013\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 2674/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2675/25000, steps: 106, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.06000000000000076\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2676/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2677/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 2678/25000, steps: 67, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.6700000000000004\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 2679/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 2680/25000, steps: 153, e: 0.28\n",
      "accumulated_rewards_per_episode: -1.5300000000000011\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 2681/25000, steps: 120, e: 0.28\n",
      "accumulated_rewards_per_episode: -1.2000000000000008\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2682/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2683/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2684/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2685/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 2686/25000, steps: 137, e: 0.28\n",
      "accumulated_rewards_per_episode: -1.370000000000001\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2687/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2688/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2689/25000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2690/25000, steps: 64, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.6400000000000003\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 2691/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2692/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 2693/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2694/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 2695/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2696/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 2697/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 2698/25000, steps: 109, e: 0.27\n",
      "accumulated_rewards_per_episode: -1.0900000000000007\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 2699/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2700/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 2701/25000, steps: 322, e: 0.27\n",
      "accumulated_rewards_per_episode: -2.219999999999997\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2702/25000, steps: 163, e: 0.27\n",
      "accumulated_rewards_per_episode: -1.6300000000000012\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2703/25000, steps: 167, e: 0.27\n",
      "accumulated_rewards_per_episode: -1.6700000000000013\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 2704/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2705/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2706/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 2707/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2708/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2709/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2710/25000, steps: 57, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5700000000000003\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2711/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2712/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2713/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 2714/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2715/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2716/25000, steps: 161, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.6100000000000012\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2717/25000, steps: 304, e: 0.27\n",
      "accumulated_rewards_per_episode: -3.039999999999979\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 2718/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 2719/25000, steps: 53, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5300000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 2720/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2721/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2722/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 2723/25000, steps: 120, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.20000000000000068\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 2724/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2725/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2726/25000, steps: 219, e: 0.27\n",
      "accumulated_rewards_per_episode: -2.1899999999999973\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2727/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 2728/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 2729/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2730/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2731/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2732/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 2733/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 2734/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2735/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 2736/25000, steps: 57, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5700000000000003\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2737/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2738/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2739/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2740/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2741/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 2742/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 2743/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 2744/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2745/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 2746/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 2747/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 2748/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 2749/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2750/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2751/25000, steps: 184, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.8400000000000011\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 2752/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 2753/25000, steps: 114, e: 0.27\n",
      "accumulated_rewards_per_episode: -1.1400000000000008\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2754/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 2755/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2756/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 2757/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 2758/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 2759/25000, steps: 114, e: 0.27\n",
      "accumulated_rewards_per_episode: -1.1400000000000008\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2760/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2761/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2762/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2763/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 2764/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2765/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2766/25000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 2767/25000, steps: 156, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5600000000000008\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 2768/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 2769/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2770/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 2771/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2772/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 2773/25000, steps: 100, e: 0.26\n",
      "accumulated_rewards_per_episode: -1.0000000000000007\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2774/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2775/25000, steps: 280, e: 0.26\n",
      "accumulated_rewards_per_episode: -1.7999999999999956\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 2776/25000, steps: 56, e: 0.26\n",
      "accumulated_rewards_per_episode: 0.4399999999999997\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2777/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2778/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 2779/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2780/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 2781/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 2782/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 2783/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2784/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 2785/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2786/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 2787/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 2788/25000, steps: 140, e: 0.26\n",
      "accumulated_rewards_per_episode: -1.400000000000001\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 2789/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 2790/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2791/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2792/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2793/25000, steps: 105, e: 0.26\n",
      "accumulated_rewards_per_episode: -1.0500000000000007\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2794/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2795/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 2796/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 2797/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2798/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2799/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2800/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2801/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2802/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2803/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 2804/25000, steps: 65, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.6500000000000004\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 2805/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2806/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2807/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2808/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 2809/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2810/25000, steps: 87, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.8700000000000006\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2811/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2812/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2813/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2814/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 2815/25000, steps: 72, e: 0.26\n",
      "accumulated_rewards_per_episode: 0.2799999999999996\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2816/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 2817/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2818/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2819/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 2820/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 2821/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 2822/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2823/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 2824/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2825/25000, steps: 60, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.6000000000000003\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2826/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 2827/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2828/25000, steps: 229, e: 0.26\n",
      "accumulated_rewards_per_episode: -1.2900000000000014\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 2829/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 2830/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 2831/25000, steps: 119, e: 0.26\n",
      "accumulated_rewards_per_episode: -1.1900000000000008\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2832/25000, steps: 110, e: 0.26\n",
      "accumulated_rewards_per_episode: -1.1000000000000008\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2833/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 2834/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 2835/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2836/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2837/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2838/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2839/25000, steps: 110, e: 0.26\n",
      "accumulated_rewards_per_episode: -1.1000000000000008\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2840/25000, steps: 262, e: 0.26\n",
      "accumulated_rewards_per_episode: -2.619999999999988\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 2841/25000, steps: 107, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.07000000000000076\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2842/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2843/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2844/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 2845/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 2846/25000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2847/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2848/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 2849/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 2850/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2851/25000, steps: 444, e: 0.25\n",
      "accumulated_rewards_per_episode: -4.43999999999995\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2852/25000, steps: 95, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.9500000000000006\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2853/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 2854/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2855/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2856/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2857/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2858/25000, steps: 55, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5500000000000003\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2859/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 2860/25000, steps: 123, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.23000000000000081\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 2861/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 2862/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 2863/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2864/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2865/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 2866/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 2867/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 2868/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2869/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2870/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 2871/25000, steps: 255, e: 0.25\n",
      "accumulated_rewards_per_episode: -2.5499999999999896\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2872/25000, steps: 91, e: 0.25\n",
      "accumulated_rewards_per_episode: 0.08999999999999947\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2873/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 2874/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2875/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2876/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2877/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2878/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2879/25000, steps: 99, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.9900000000000007\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2880/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2881/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2882/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 2883/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2884/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 2885/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 2886/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2887/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 2888/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 2889/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 2890/25000, steps: 302, e: 0.25\n",
      "accumulated_rewards_per_episode: -3.0199999999999796\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2891/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2892/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2893/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 2894/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2895/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2896/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2897/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2898/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2899/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 2900/25000, steps: 128, e: 0.25\n",
      "accumulated_rewards_per_episode: -1.280000000000001\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 2901/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 2902/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2903/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2904/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 2905/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2906/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2907/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 2908/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 2909/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2910/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 2911/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 2912/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2913/25000, steps: 54, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5400000000000003\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 2914/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 2915/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2916/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 2917/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 2918/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2919/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 2920/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2921/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2922/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2923/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2924/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 2925/25000, steps: 68, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.6800000000000004\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 2926/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2927/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2928/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2929/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2930/25000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 2931/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2932/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2933/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2934/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 2935/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 2936/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 2937/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2938/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2939/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 2940/25000, steps: 67, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.6700000000000004\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2941/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 2942/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 2943/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 2944/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 2945/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2946/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 2947/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 2948/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 2949/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2950/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 2951/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 2952/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2953/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 2954/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 2955/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2956/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2957/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 2958/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2959/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 2960/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2961/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 2962/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2963/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2964/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2965/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2966/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2967/25000, steps: 77, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.7700000000000005\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2968/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2969/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 2970/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 2971/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 2972/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 2973/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2974/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2975/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 2976/25000, steps: 101, e: 0.24\n",
      "accumulated_rewards_per_episode: -1.0100000000000007\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 2977/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2978/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 2979/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2980/25000, steps: 121, e: 0.24\n",
      "accumulated_rewards_per_episode: -1.2100000000000009\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 2981/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 2982/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 2983/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 2984/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 2985/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 2986/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 2987/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 2988/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 2989/25000, steps: 104, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.040000000000000646\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 2990/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2991/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 2992/25000, steps: 255, e: 0.24\n",
      "accumulated_rewards_per_episode: -2.5499999999999896\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 2993/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 2994/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2995/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 2996/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 2997/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2998/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 2999/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3000/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 3001/25000, steps: 127, e: 0.24\n",
      "accumulated_rewards_per_episode: -1.270000000000001\n",
      "START state: (0, 0, 0, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3002/25000, steps: 105, e: 0.24\n",
      "accumulated_rewards_per_episode: -1.0500000000000007\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3003/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 3004/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3005/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3006/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3007/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 3008/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3009/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 3010/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 3011/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3012/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 3013/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3014/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3015/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3016/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 3017/25000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3018/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3019/25000, steps: 170, e: 0.23\n",
      "accumulated_rewards_per_episode: -1.7000000000000013\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 3020/25000, steps: 54, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5400000000000003\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3021/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 3022/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 3023/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 3024/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3025/25000, steps: 53, e: 0.23\n",
      "accumulated_rewards_per_episode: 0.46999999999999975\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 3026/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3027/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 3028/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3029/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 3030/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3031/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3032/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 3033/25000, steps: 71, e: 0.23\n",
      "accumulated_rewards_per_episode: 0.2899999999999996\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 3034/25000, steps: 66, e: 0.23\n",
      "accumulated_rewards_per_episode: 0.33999999999999964\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 3035/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 3036/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 3037/25000, steps: 66, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.6600000000000004\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 3038/25000, steps: 231, e: 0.23\n",
      "accumulated_rewards_per_episode: -2.3099999999999947\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3039/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3040/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 3041/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 3042/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 3043/25000, steps: 53, e: 0.23\n",
      "accumulated_rewards_per_episode: 0.46999999999999964\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 3044/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 3045/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3046/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 3047/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3048/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 3049/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 3050/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3051/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3052/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3053/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3054/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 3055/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3056/25000, steps: 119, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.19000000000000072\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3057/25000, steps: 98, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.9800000000000006\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3058/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3059/25000, steps: 77, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.7700000000000005\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3060/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3061/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3062/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 3063/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3064/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 3065/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 3066/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3067/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 3068/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3069/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3070/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3071/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3072/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 3073/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3074/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 3075/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3076/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3077/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 3078/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3079/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 3080/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 3081/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 3082/25000, steps: 112, e: 0.23\n",
      "accumulated_rewards_per_episode: -1.1200000000000008\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 3083/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3084/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3085/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3086/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3087/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 3088/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3089/25000, steps: 78, e: 0.23\n",
      "accumulated_rewards_per_episode: 0.2199999999999993\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3090/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 3091/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3092/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3093/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 3094/25000, steps: 66, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.6600000000000004\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 3095/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3096/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 3097/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 3098/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3099/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 3100/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 3101/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 3102/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 3103/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3104/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 3105/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 3106/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3107/25000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3108/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3109/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 3110/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 3111/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3112/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 3113/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3114/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3115/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3116/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3117/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3118/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3119/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 3120/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3121/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3122/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3123/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3124/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 3125/25000, steps: 105, e: 0.22\n",
      "accumulated_rewards_per_episode: -1.0500000000000007\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 3126/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3127/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3128/25000, steps: 83, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.16999999999999948\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 3129/25000, steps: 108, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.08000000000000053\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3130/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3131/25000, steps: 108, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.08000000000000053\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3132/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3133/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 3134/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 3135/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 3136/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 3137/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3138/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3139/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3140/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3141/25000, steps: 70, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.2999999999999994\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 3142/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 3143/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 3144/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3145/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3146/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3147/25000, steps: 68, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.6800000000000004\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3148/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3149/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 3150/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 3151/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3152/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3153/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3154/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3155/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3156/25000, steps: 58, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5800000000000003\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 3157/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3158/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 3159/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 3160/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3161/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3162/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 3163/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 3164/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3165/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3166/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3167/25000, steps: 66, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.33999999999999964\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 3168/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3169/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3170/25000, steps: 116, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.16000000000000053\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3171/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 3172/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3173/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3174/25000, steps: 75, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.24999999999999956\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3175/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 3176/25000, steps: 116, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.16000000000000064\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3177/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 3178/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 3179/25000, steps: 70, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.2999999999999996\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3180/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3181/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3182/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 3183/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 3184/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 3185/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3186/25000, steps: 85, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.14999999999999947\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3187/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3188/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 3189/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 3190/25000, steps: 58, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5800000000000003\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 3191/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 3192/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3193/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3194/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3195/25000, steps: 88, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.8800000000000006\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3196/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 3197/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 3198/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3199/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3200/25000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3201/25000, steps: 136, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.3600000000000008\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3202/25000, steps: 119, e: 0.22\n",
      "accumulated_rewards_per_episode: -0.19000000000000067\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 3203/25000, steps: 55, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.4499999999999996\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 3204/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3205/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 3206/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3207/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 3208/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3209/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3210/25000, steps: 88, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.8800000000000006\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3211/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 3212/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3213/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 3214/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3215/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3216/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 3217/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3218/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3219/25000, steps: 67, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.3299999999999995\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3220/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 3221/25000, steps: 78, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.2199999999999993\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 3222/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3223/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 3224/25000, steps: 99, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.00999999999999947\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3225/25000, steps: 66, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.6600000000000004\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3226/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3227/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3228/25000, steps: 295, e: 0.21\n",
      "accumulated_rewards_per_episode: -1.9500000000000022\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 3229/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3230/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 3231/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 3232/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3233/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3234/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 3235/25000, steps: 82, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.8200000000000005\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 3236/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3237/25000, steps: 209, e: 0.21\n",
      "accumulated_rewards_per_episode: -1.0900000000000012\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3238/25000, steps: 367, e: 0.21\n",
      "accumulated_rewards_per_episode: -2.6699999999999875\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3239/25000, steps: 217, e: 0.21\n",
      "accumulated_rewards_per_episode: -1.1700000000000015\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3240/25000, steps: 135, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.3500000000000008\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3241/25000, steps: 132, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.3200000000000009\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3242/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3243/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 3244/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3245/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3246/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3247/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3248/25000, steps: 142, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.4200000000000009\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3249/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3250/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 3251/25000, steps: 68, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.3199999999999994\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3252/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3253/25000, steps: 551, e: 0.21\n",
      "accumulated_rewards_per_episode: -4.509999999999949\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3254/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3255/25000, steps: 164, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.640000000000001\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 3256/25000, steps: 117, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.17000000000000065\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 3257/25000, steps: 80, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.8000000000000005\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 3258/25000, steps: 124, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.7599999999999993\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 3259/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 3260/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3261/25000, steps: 70, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.2999999999999995\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3262/25000, steps: 93, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.06999999999999926\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3263/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 3264/25000, steps: 57, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.4299999999999996\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3265/25000, steps: 113, e: 0.21\n",
      "accumulated_rewards_per_episode: -1.1300000000000008\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 3266/25000, steps: 73, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.7300000000000004\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3267/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3268/25000, steps: 800, e: 0.21\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3269/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 3270/25000, steps: 196, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.9600000000000013\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 3271/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3272/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3273/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 3274/25000, steps: 800, e: 0.21\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 3275/25000, steps: 124, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.2400000000000007\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3276/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3277/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3278/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3279/25000, steps: 337, e: 0.21\n",
      "accumulated_rewards_per_episode: -3.369999999999972\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 3280/25000, steps: 133, e: 0.21\n",
      "accumulated_rewards_per_episode: -1.330000000000001\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3281/25000, steps: 800, e: 0.21\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 3282/25000, steps: 520, e: 0.21\n",
      "accumulated_rewards_per_episode: -3.1999999999999598\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3283/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 3284/25000, steps: 800, e: 0.21\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3285/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3286/25000, steps: 186, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.1399999999999988\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3287/25000, steps: 800, e: 0.21\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3288/25000, steps: 800, e: 0.21\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3289/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3290/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3291/25000, steps: 605, e: 0.21\n",
      "accumulated_rewards_per_episode: -6.0499999999999154\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3292/25000, steps: 800, e: 0.21\n",
      "accumulated_rewards_per_episode: -4.99999999999994\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3293/25000, steps: 800, e: 0.21\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 3294/25000, steps: 351, e: 0.21\n",
      "accumulated_rewards_per_episode: -3.509999999999969\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3295/25000, steps: 800, e: 0.21\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 3296/25000, steps: 800, e: 0.21\n",
      "accumulated_rewards_per_episode: -5.999999999999895\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 3297/25000, steps: 800, e: 0.21\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 3298/25000, steps: 402, e: 0.21\n",
      "accumulated_rewards_per_episode: -3.0199999999999587\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3299/25000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3300/25000, steps: 800, e: 0.21\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3301/25000, steps: 375, e: 0.21\n",
      "accumulated_rewards_per_episode: -2.749999999999986\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3302/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3303/25000, steps: 80, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.8000000000000005\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3304/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3305/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3306/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3307/25000, steps: 218, e: 0.2\n",
      "accumulated_rewards_per_episode: -2.1799999999999975\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3308/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3309/25000, steps: 754, e: 0.2\n",
      "accumulated_rewards_per_episode: -5.539999999999906\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3310/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3311/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3312/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3313/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3314/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3315/25000, steps: 422, e: 0.2\n",
      "accumulated_rewards_per_episode: -2.2199999999999984\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3316/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 3317/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 3318/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 3319/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 3320/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3321/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3322/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 3323/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 3324/25000, steps: 74, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.7400000000000004\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3325/25000, steps: 56, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.4399999999999996\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 3326/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3327/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3328/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3329/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 3330/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 3331/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3332/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3333/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3334/25000, steps: 575, e: 0.2\n",
      "accumulated_rewards_per_episode: -4.749999999999944\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3335/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3336/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 3337/25000, steps: 353, e: 0.2\n",
      "accumulated_rewards_per_episode: -2.529999999999991\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 3338/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -5.999999999999893\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3339/25000, steps: 676, e: 0.2\n",
      "accumulated_rewards_per_episode: -5.759999999999922\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3340/25000, steps: 101, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.010000000000000642\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3341/25000, steps: 96, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.03999999999999925\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3342/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3343/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3344/25000, steps: 53, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.46999999999999964\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3345/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -2.9999999999999396\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 3346/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3347/25000, steps: 783, e: 0.2\n",
      "accumulated_rewards_per_episode: -5.8299999999999\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3348/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 7, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3349/25000, steps: 67, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.6700000000000004\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3350/25000, steps: 63, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.36999999999999955\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 3351/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3352/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 3353/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3354/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 3355/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3356/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 3357/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3358/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3359/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3360/25000, steps: 665, e: 0.2\n",
      "accumulated_rewards_per_episode: -4.649999999999924\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 3361/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 3362/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3363/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3364/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 3365/25000, steps: 71, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.2899999999999995\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3366/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 3367/25000, steps: 74, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.7400000000000004\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3368/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 3369/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3370/25000, steps: 305, e: 0.2\n",
      "accumulated_rewards_per_episode: -2.0500000000000007\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3371/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -5.999999999999895\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 3372/25000, steps: 244, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.5599999999999984\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3373/25000, steps: 186, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.8600000000000012\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 3374/25000, steps: 471, e: 0.2\n",
      "accumulated_rewards_per_episode: -3.7099999999999658\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 3375/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3376/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3377/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3378/25000, steps: 336, e: 0.2\n",
      "accumulated_rewards_per_episode: -3.3599999999999723\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3379/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 3380/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3381/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 3382/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3383/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3384/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 3385/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3386/25000, steps: 661, e: 0.2\n",
      "accumulated_rewards_per_episode: -5.609999999999926\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3387/25000, steps: 800, e: 0.2\n",
      "accumulated_rewards_per_episode: -5.999999999999896\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3388/25000, steps: 230, e: 0.2\n",
      "accumulated_rewards_per_episode: -2.299999999999995\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 3389/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 3390/25000, steps: 92, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.07999999999999947\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3391/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3392/25000, steps: 56, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.4399999999999995\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3393/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3394/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3395/25000, steps: 575, e: 0.2\n",
      "accumulated_rewards_per_episode: -3.749999999999943\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3396/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 3397/25000, steps: 204, e: 0.2\n",
      "accumulated_rewards_per_episode: -1.0400000000000011\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3398/25000, steps: 297, e: 0.2\n",
      "accumulated_rewards_per_episode: -1.9700000000000022\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 3399/25000, steps: 85, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.14999999999999947\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3400/25000, steps: 100, e: 0.2\n",
      "accumulated_rewards_per_episode: -5.30825383648903e-16\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3401/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3402/25000, steps: 145, e: 0.2\n",
      "accumulated_rewards_per_episode: -1.450000000000001\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3403/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 3404/25000, steps: 709, e: 0.2\n",
      "accumulated_rewards_per_episode: -4.089999999999938\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 3405/25000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 3406/25000, steps: 66, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.6600000000000004\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 3407/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3408/25000, steps: 64, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.35999999999999965\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3409/25000, steps: 86, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.13999999999999924\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3410/25000, steps: 775, e: 0.19\n",
      "accumulated_rewards_per_episode: -5.749999999999879\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3411/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 3412/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3413/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 3414/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 3415/25000, steps: 86, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.8600000000000005\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 3416/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 3417/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3418/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3419/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 3420/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3421/25000, steps: 365, e: 0.19\n",
      "accumulated_rewards_per_episode: -2.649999999999988\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 3422/25000, steps: 800, e: 0.19\n",
      "accumulated_rewards_per_episode: -4.999999999999918\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3423/25000, steps: 703, e: 0.19\n",
      "accumulated_rewards_per_episode: -6.029999999999916\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3424/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3425/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3426/25000, steps: 125, e: 0.19\n",
      "accumulated_rewards_per_episode: -1.2500000000000009\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 3427/25000, steps: 99, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.9900000000000007\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3428/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 3429/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 3430/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3431/25000, steps: 567, e: 0.19\n",
      "accumulated_rewards_per_episode: -4.669999999999945\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 3432/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3433/25000, steps: 205, e: 0.19\n",
      "accumulated_rewards_per_episode: -2.0500000000000003\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 3434/25000, steps: 67, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.6700000000000004\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3435/25000, steps: 548, e: 0.19\n",
      "accumulated_rewards_per_episode: -2.4799999999999276\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3436/25000, steps: 129, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.29000000000000065\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3437/25000, steps: 214, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.1400000000000014\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 3438/25000, steps: 237, e: 0.19\n",
      "accumulated_rewards_per_episode: -1.3700000000000019\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 3439/25000, steps: 604, e: 0.19\n",
      "accumulated_rewards_per_episode: -2.0399999999999667\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 3440/25000, steps: 669, e: 0.19\n",
      "accumulated_rewards_per_episode: -4.689999999999946\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 3441/25000, steps: 800, e: 0.19\n",
      "accumulated_rewards_per_episode: -5.999999999999899\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3442/25000, steps: 271, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.7100000000000017\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 3443/25000, steps: 241, e: 0.19\n",
      "accumulated_rewards_per_episode: -1.410000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3444/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 3445/25000, steps: 800, e: 0.19\n",
      "accumulated_rewards_per_episode: -2.999999999999874\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3446/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 3447/25000, steps: 800, e: 0.19\n",
      "accumulated_rewards_per_episode: -4.999999999999896\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 3448/25000, steps: 161, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.6100000000000012\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3449/25000, steps: 58, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.4199999999999996\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3450/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 3451/25000, steps: 800, e: 0.19\n",
      "accumulated_rewards_per_episode: -2.999999999999944\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3452/25000, steps: 413, e: 0.19\n",
      "accumulated_rewards_per_episode: -4.129999999999956\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3453/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 3454/25000, steps: 156, e: 0.19\n",
      "accumulated_rewards_per_episode: -1.5600000000000012\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3455/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3456/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3457/25000, steps: 800, e: 0.19\n",
      "accumulated_rewards_per_episode: -5.999999999999895\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 3458/25000, steps: 385, e: 0.19\n",
      "accumulated_rewards_per_episode: -2.849999999999984\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3459/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3460/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3461/25000, steps: 185, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.8500000000000014\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3462/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 3463/25000, steps: 127, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.27000000000000085\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3464/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 3465/25000, steps: 590, e: 0.19\n",
      "accumulated_rewards_per_episode: -4.899999999999919\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3466/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 3467/25000, steps: 714, e: 0.19\n",
      "accumulated_rewards_per_episode: -5.139999999999914\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 3468/25000, steps: 185, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.8500000000000013\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3469/25000, steps: 134, e: 0.19\n",
      "accumulated_rewards_per_episode: -1.340000000000001\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3470/25000, steps: 152, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.4799999999999991\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 3471/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3472/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 3473/25000, steps: 490, e: 0.19\n",
      "accumulated_rewards_per_episode: -3.89999999999994\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3474/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3475/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3476/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 3477/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3478/25000, steps: 800, e: 0.19\n",
      "accumulated_rewards_per_episode: -4.999999999999917\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 3479/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3480/25000, steps: 800, e: 0.19\n",
      "accumulated_rewards_per_episode: -5.999999999999895\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 3481/25000, steps: 220, e: 0.19\n",
      "accumulated_rewards_per_episode: -1.2000000000000015\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 3482/25000, steps: 244, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.4400000000000019\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3483/25000, steps: 177, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.770000000000001\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3484/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3485/25000, steps: 94, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.05999999999999948\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3486/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 3487/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3488/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3489/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 3490/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 3491/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3492/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 3493/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 3494/25000, steps: 64, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.35999999999999965\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3495/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 3496/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3497/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 3498/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3499/25000, steps: 159, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5900000000000012\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 3500/25000, steps: 158, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.41999999999999904\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 3501/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3502/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3503/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3504/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3505/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3506/25000, steps: 384, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.8400000000000026\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3507/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3508/25000, steps: 800, e: 0.19\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 3509/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 3510/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3511/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3512/25000, steps: 409, e: 0.19\n",
      "accumulated_rewards_per_episode: -4.089999999999957\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3513/25000, steps: 779, e: 0.19\n",
      "accumulated_rewards_per_episode: -3.7899999999999006\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 3514/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 3515/25000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3516/25000, steps: 58, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.4199999999999996\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3517/25000, steps: 55, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.44999999999999973\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 3518/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 3519/25000, steps: 55, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.4499999999999996\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3520/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 3521/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3522/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 3523/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3524/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3525/25000, steps: 53, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5300000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 3526/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 3527/25000, steps: 102, e: 0.18\n",
      "accumulated_rewards_per_episode: -1.0200000000000007\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3528/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 3529/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 3530/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3531/25000, steps: 440, e: 0.18\n",
      "accumulated_rewards_per_episode: -1.3999999999999986\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 3532/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 3533/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 3534/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3535/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 3536/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3537/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3538/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 3539/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3540/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3541/25000, steps: 385, e: 0.18\n",
      "accumulated_rewards_per_episode: -3.849999999999962\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3542/25000, steps: 800, e: 0.18\n",
      "accumulated_rewards_per_episode: -5.999999999999896\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 3543/25000, steps: 486, e: 0.18\n",
      "accumulated_rewards_per_episode: -1.859999999999988\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 3544/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3545/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3546/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3547/25000, steps: 89, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.8900000000000006\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 3548/25000, steps: 472, e: 0.18\n",
      "accumulated_rewards_per_episode: -2.719999999999944\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3549/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3550/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 3551/25000, steps: 722, e: 0.18\n",
      "accumulated_rewards_per_episode: -5.219999999999913\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3552/25000, steps: 62, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.37999999999999967\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3553/25000, steps: 65, e: 0.18\n",
      "accumulated_rewards_per_episode: 1.3499999999999994\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3554/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3555/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3556/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 3557/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3558/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3559/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3560/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 3561/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3562/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 3563/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 3564/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3565/25000, steps: 57, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.4299999999999996\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 3566/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3567/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3568/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3569/25000, steps: 800, e: 0.18\n",
      "accumulated_rewards_per_episode: -4.999999999999917\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 3570/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 3571/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3572/25000, steps: 306, e: 0.18\n",
      "accumulated_rewards_per_episode: -1.0600000000000018\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3573/25000, steps: 109, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.9099999999999993\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3574/25000, steps: 196, e: 0.18\n",
      "accumulated_rewards_per_episode: 1.0399999999999987\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 3575/25000, steps: 61, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.38999999999999957\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 3576/25000, steps: 57, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.4299999999999997\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 3577/25000, steps: 99, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.9900000000000007\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3578/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 3579/25000, steps: 79, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.20999999999999946\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3580/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 3581/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3582/25000, steps: 89, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.10999999999999946\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3583/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3584/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3585/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3586/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 3587/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 3588/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3589/25000, steps: 73, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.2699999999999996\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3590/25000, steps: 609, e: 0.18\n",
      "accumulated_rewards_per_episode: -3.089999999999958\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3591/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 3592/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 3593/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 3594/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3595/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3596/25000, steps: 800, e: 0.18\n",
      "accumulated_rewards_per_episode: -4.999999999999918\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3597/25000, steps: 176, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.760000000000001\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 3598/25000, steps: 308, e: 0.18\n",
      "accumulated_rewards_per_episode: -1.080000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3599/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 3600/25000, steps: 244, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.559999999999998\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3601/25000, steps: 163, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.6300000000000011\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 3602/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 3603/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3604/25000, steps: 239, e: 0.18\n",
      "accumulated_rewards_per_episode: -1.3900000000000017\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3605/25000, steps: 56, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.4399999999999996\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3606/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3607/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3608/25000, steps: 54, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.4599999999999995\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3609/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3610/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 3611/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3612/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 3613/25000, steps: 154, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.45999999999999863\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3614/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3615/25000, steps: 74, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.25999999999999945\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3616/25000, steps: 67, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.3299999999999995\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3617/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3618/25000, steps: 91, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.08999999999999947\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3619/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3620/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3621/25000, steps: 75, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.24999999999999944\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3622/25000, steps: 93, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.06999999999999948\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 3623/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 3624/25000, steps: 76, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.23999999999999955\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3625/25000, steps: 87, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.12999999999999945\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3626/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 3627/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3628/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3629/25000, steps: 70, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.7000000000000004\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 3630/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3631/25000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3632/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3633/25000, steps: 305, e: 0.17\n",
      "accumulated_rewards_per_episode: -1.0500000000000023\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3634/25000, steps: 65, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.34999999999999964\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3635/25000, steps: 253, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5300000000000015\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 3636/25000, steps: 305, e: 0.17\n",
      "accumulated_rewards_per_episode: -3.049999999999979\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3637/25000, steps: 55, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.44999999999999973\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 3638/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3639/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3640/25000, steps: 62, e: 0.17\n",
      "accumulated_rewards_per_episode: 1.3799999999999994\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3641/25000, steps: 54, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.4599999999999995\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 3642/25000, steps: 264, e: 0.17\n",
      "accumulated_rewards_per_episode: -1.6400000000000017\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 3643/25000, steps: 128, e: 0.17\n",
      "accumulated_rewards_per_episode: -1.280000000000001\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3644/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 3645/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3646/25000, steps: 635, e: 0.17\n",
      "accumulated_rewards_per_episode: -3.3499999999999526\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3647/25000, steps: 155, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5500000000000008\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3648/25000, steps: 73, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.26999999999999935\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 3649/25000, steps: 84, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.15999999999999948\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3650/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3651/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 3652/25000, steps: 75, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.24999999999999956\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3653/25000, steps: 96, e: 0.17\n",
      "accumulated_rewards_per_episode: 1.0399999999999994\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 3654/25000, steps: 315, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.849999999999998\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3655/25000, steps: 416, e: 0.17\n",
      "accumulated_rewards_per_episode: -3.1599999999999775\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3656/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3657/25000, steps: 800, e: 0.17\n",
      "accumulated_rewards_per_episode: -5.999999999999917\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3658/25000, steps: 55, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.4499999999999996\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3659/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3660/25000, steps: 418, e: 0.17\n",
      "accumulated_rewards_per_episode: -3.1799999999999766\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 3661/25000, steps: 163, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.6300000000000009\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3662/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 3663/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 3664/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3665/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3666/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3667/25000, steps: 93, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.06999999999999948\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3668/25000, steps: 677, e: 0.17\n",
      "accumulated_rewards_per_episode: -3.769999999999933\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3669/25000, steps: 273, e: 0.17\n",
      "accumulated_rewards_per_episode: -1.730000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 3670/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3671/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3672/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3673/25000, steps: 175, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.7500000000000013\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 3674/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 3675/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 3676/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3677/25000, steps: 110, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.899999999999999\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3678/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3679/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3680/25000, steps: 88, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.11999999999999945\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 3681/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 3682/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3683/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3684/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3685/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 3686/25000, steps: 60, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.3999999999999997\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3687/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3688/25000, steps: 65, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.34999999999999964\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 3689/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3690/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3691/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3692/25000, steps: 87, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.12999999999999934\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 3693/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3694/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3695/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 3696/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 3697/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3698/25000, steps: 166, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.660000000000001\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 3699/25000, steps: 60, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.3999999999999996\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3700/25000, steps: 121, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.21000000000000058\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 3701/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 3702/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3703/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3704/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3705/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3706/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3707/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3708/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 3709/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 3710/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3711/25000, steps: 113, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.13000000000000062\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3712/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 3713/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3714/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 3715/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 3716/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 3717/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3718/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 3719/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3720/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 3721/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 3722/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3723/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3724/25000, steps: 206, e: 0.17\n",
      "accumulated_rewards_per_episode: -2.06\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 3725/25000, steps: 254, e: 0.17\n",
      "accumulated_rewards_per_episode: -1.540000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3726/25000, steps: 172, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.7200000000000011\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3727/25000, steps: 63, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.36999999999999966\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3728/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 3729/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3730/25000, steps: 85, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.8500000000000005\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3731/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3732/25000, steps: 55, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5500000000000003\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 3733/25000, steps: 105, e: 0.17\n",
      "accumulated_rewards_per_episode: -1.0500000000000007\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 3734/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 3735/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3736/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3737/25000, steps: 365, e: 0.17\n",
      "accumulated_rewards_per_episode: -2.649999999999988\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3738/25000, steps: 74, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.7400000000000004\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 3739/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 3740/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3741/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3742/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 3743/25000, steps: 72, e: 0.17\n",
      "accumulated_rewards_per_episode: 1.2799999999999994\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3744/25000, steps: 60, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.39999999999999947\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3745/25000, steps: 190, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.9000000000000012\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 3746/25000, steps: 104, e: 0.17\n",
      "accumulated_rewards_per_episode: -1.0400000000000007\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 3747/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 3748/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 3749/25000, steps: 536, e: 0.17\n",
      "accumulated_rewards_per_episode: -3.3599999999999737\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 3750/25000, steps: 202, e: 0.17\n",
      "accumulated_rewards_per_episode: -1.0200000000000014\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3751/25000, steps: 56, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5600000000000003\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3752/25000, steps: 88, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.8800000000000006\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 3753/25000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3754/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3755/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3756/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3757/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3758/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 3759/25000, steps: 652, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000048\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 3760/25000, steps: 631, e: 0.16\n",
      "accumulated_rewards_per_episode: -4.309999999999932\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 3761/25000, steps: 614, e: 0.16\n",
      "accumulated_rewards_per_episode: -5.139999999999935\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3762/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3763/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3764/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 3765/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3766/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3767/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 3768/25000, steps: 93, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.06999999999999937\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3769/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3770/25000, steps: 800, e: 0.16\n",
      "accumulated_rewards_per_episode: -5.999999999999896\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3771/25000, steps: 82, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.17999999999999938\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3772/25000, steps: 59, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.4099999999999995\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 3773/25000, steps: 675, e: 0.16\n",
      "accumulated_rewards_per_episode: -2.7499999999999525\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 3774/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 3775/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 3776/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3777/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 3778/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3779/25000, steps: 800, e: 0.16\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3780/25000, steps: 413, e: 0.16\n",
      "accumulated_rewards_per_episode: -3.1299999999999777\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 3781/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 3782/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3783/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 3784/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3785/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 3786/25000, steps: 800, e: 0.16\n",
      "accumulated_rewards_per_episode: -6.9999999999998925\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3787/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3788/25000, steps: 499, e: 0.16\n",
      "accumulated_rewards_per_episode: -3.9899999999999594\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 3789/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3790/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3791/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 3792/25000, steps: 800, e: 0.16\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3793/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 3794/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 3795/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3796/25000, steps: 544, e: 0.16\n",
      "accumulated_rewards_per_episode: -3.439999999999954\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3797/25000, steps: 796, e: 0.16\n",
      "accumulated_rewards_per_episode: -6.959999999999897\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 3798/25000, steps: 585, e: 0.16\n",
      "accumulated_rewards_per_episode: -2.8499999999999854\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3799/25000, steps: 112, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.12000000000000051\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 3800/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3801/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3802/25000, steps: 68, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.3199999999999995\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3803/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 3804/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3805/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 3806/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3807/25000, steps: 800, e: 0.16\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 3808/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 3809/25000, steps: 800, e: 0.16\n",
      "accumulated_rewards_per_episode: -1.999999999999955\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3810/25000, steps: 800, e: 0.16\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3811/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3812/25000, steps: 800, e: 0.16\n",
      "accumulated_rewards_per_episode: -2.999999999999983\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3813/25000, steps: 800, e: 0.16\n",
      "accumulated_rewards_per_episode: -4.99999999999994\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3814/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 3815/25000, steps: 800, e: 0.16\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3816/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3817/25000, steps: 800, e: 0.16\n",
      "accumulated_rewards_per_episode: -4.999999999999874\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3818/25000, steps: 794, e: 0.16\n",
      "accumulated_rewards_per_episode: -4.939999999999919\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3819/25000, steps: 141, e: 0.16\n",
      "accumulated_rewards_per_episode: -1.410000000000001\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 3820/25000, steps: 271, e: 0.16\n",
      "accumulated_rewards_per_episode: -1.710000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3821/25000, steps: 429, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.709999999999997\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 3822/25000, steps: 93, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.06999999999999948\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 3823/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 3824/25000, steps: 86, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.13999999999999946\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3825/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 3826/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3827/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3828/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3829/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 3830/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3831/25000, steps: 126, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.26000000000000073\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 3832/25000, steps: 57, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.4299999999999997\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3833/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3834/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 3835/25000, steps: 329, e: 0.16\n",
      "accumulated_rewards_per_episode: -1.290000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 3836/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3837/25000, steps: 88, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.11999999999999945\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 3838/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 3839/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3840/25000, steps: 666, e: 0.16\n",
      "accumulated_rewards_per_episode: -3.659999999999968\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3841/25000, steps: 800, e: 0.16\n",
      "accumulated_rewards_per_episode: -2.9999999999999836\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 3842/25000, steps: 800, e: 0.16\n",
      "accumulated_rewards_per_episode: -3.999999999999896\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3843/25000, steps: 290, e: 0.16\n",
      "accumulated_rewards_per_episode: 2.1000000000000076\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3844/25000, steps: 219, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.8099999999999988\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3845/25000, steps: 87, e: 0.16\n",
      "accumulated_rewards_per_episode: 1.1299999999999994\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3846/25000, steps: 83, e: 0.16\n",
      "accumulated_rewards_per_episode: 1.1699999999999993\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3847/25000, steps: 464, e: 0.16\n",
      "accumulated_rewards_per_episode: -1.639999999999992\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3848/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 3849/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3850/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3851/25000, steps: 62, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.37999999999999956\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3852/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 3853/25000, steps: 68, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.3199999999999995\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3854/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 3855/25000, steps: 62, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.37999999999999967\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3856/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3857/25000, steps: 155, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5500000000000008\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 3858/25000, steps: 115, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.8499999999999992\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3859/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3860/25000, steps: 137, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.629999999999999\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 3861/25000, steps: 370, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.2999999999999976\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3862/25000, steps: 56, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.4399999999999996\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3863/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 3864/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3865/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3866/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 3867/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3868/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3869/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 3870/25000, steps: 148, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.4800000000000009\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 3871/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3872/25000, steps: 439, e: 0.16\n",
      "accumulated_rewards_per_episode: -2.389999999999973\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3873/25000, steps: 178, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.21999999999999886\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 3874/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 3875/25000, steps: 162, e: 0.16\n",
      "accumulated_rewards_per_episode: 1.3800000000000026\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3876/25000, steps: 117, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.8299999999999994\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3877/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 3878/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3879/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 3880/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 3881/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 3882/25000, steps: 64, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.35999999999999954\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3883/25000, steps: 587, e: 0.16\n",
      "accumulated_rewards_per_episode: -2.8699999999999406\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 3884/25000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 3885/25000, steps: 426, e: 0.15\n",
      "accumulated_rewards_per_episode: -2.2599999999999563\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3886/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3887/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 3888/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3889/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3890/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 3891/25000, steps: 800, e: 0.15\n",
      "accumulated_rewards_per_episode: -6.999999999999894\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3892/25000, steps: 623, e: 0.15\n",
      "accumulated_rewards_per_episode: -2.2299999999999756\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3893/25000, steps: 305, e: 0.15\n",
      "accumulated_rewards_per_episode: -3.049999999999979\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3894/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3895/25000, steps: 331, e: 0.15\n",
      "accumulated_rewards_per_episode: -2.309999999999995\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 3896/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3897/25000, steps: 110, e: 0.15\n",
      "accumulated_rewards_per_episode: -1.1000000000000008\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3898/25000, steps: 90, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.9000000000000006\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 3899/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 3900/25000, steps: 800, e: 0.15\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3901/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 3902/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3903/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3904/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3905/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3906/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3907/25000, steps: 508, e: 0.15\n",
      "accumulated_rewards_per_episode: -3.0799999999999574\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3908/25000, steps: 81, e: 0.15\n",
      "accumulated_rewards_per_episode: 1.1899999999999993\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 3909/25000, steps: 70, e: 0.15\n",
      "accumulated_rewards_per_episode: 1.2999999999999996\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3910/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3911/25000, steps: 56, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5600000000000003\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3912/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 3913/25000, steps: 800, e: 0.15\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 3914/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3915/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3916/25000, steps: 121, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.21000000000000069\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 3917/25000, steps: 416, e: 0.15\n",
      "accumulated_rewards_per_episode: -2.1599999999999993\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3918/25000, steps: 231, e: 0.15\n",
      "accumulated_rewards_per_episode: -1.3100000000000014\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 3919/25000, steps: 182, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.8200000000000012\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3920/25000, steps: 140, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.40000000000000085\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3921/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3922/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 3923/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3924/25000, steps: 800, e: 0.15\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3925/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 3926/25000, steps: 800, e: 0.15\n",
      "accumulated_rewards_per_episode: -4.999999999999896\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 3927/25000, steps: 160, e: 0.15\n",
      "accumulated_rewards_per_episode: -1.6000000000000012\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3928/25000, steps: 800, e: 0.15\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3929/25000, steps: 156, e: 0.15\n",
      "accumulated_rewards_per_episode: -1.5600000000000012\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3930/25000, steps: 800, e: 0.15\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3931/25000, steps: 800, e: 0.15\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 3932/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3933/25000, steps: 800, e: 0.15\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3934/25000, steps: 313, e: 0.15\n",
      "accumulated_rewards_per_episode: -2.1299999999999994\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3935/25000, steps: 294, e: 0.15\n",
      "accumulated_rewards_per_episode: -1.93999999999999\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3936/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3937/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3938/25000, steps: 144, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.440000000000001\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3939/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3940/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 3941/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3942/25000, steps: 457, e: 0.15\n",
      "accumulated_rewards_per_episode: -4.569999999999947\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 3943/25000, steps: 470, e: 0.15\n",
      "accumulated_rewards_per_episode: -3.699999999999966\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 3944/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 3945/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 3946/25000, steps: 63, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.36999999999999955\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 3947/25000, steps: 121, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.7899999999999991\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3948/25000, steps: 800, e: 0.15\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 3949/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 3950/25000, steps: 192, e: 0.15\n",
      "accumulated_rewards_per_episode: -1.9200000000000015\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 3951/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 3952/25000, steps: 800, e: 0.15\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3953/25000, steps: 91, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.08999999999999936\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 3954/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 3955/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 3956/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3957/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 3958/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 3959/25000, steps: 59, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.4099999999999996\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 3960/25000, steps: 314, e: 0.15\n",
      "accumulated_rewards_per_episode: -2.1399999999999992\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 3961/25000, steps: 58, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.4199999999999995\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3962/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 3963/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3964/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3965/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3966/25000, steps: 452, e: 0.15\n",
      "accumulated_rewards_per_episode: -3.5199999999999694\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3967/25000, steps: 586, e: 0.15\n",
      "accumulated_rewards_per_episode: -3.8599999999999417\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 3968/25000, steps: 800, e: 0.15\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3969/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3970/25000, steps: 119, e: 0.15\n",
      "accumulated_rewards_per_episode: -1.1900000000000008\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3971/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 3972/25000, steps: 80, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.1999999999999994\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 3973/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 3974/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 3975/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3976/25000, steps: 233, e: 0.15\n",
      "accumulated_rewards_per_episode: -2.3299999999999943\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3977/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 3978/25000, steps: 72, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.2799999999999996\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3979/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3980/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3981/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3982/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3983/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3984/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 3985/25000, steps: 168, e: 0.15\n",
      "accumulated_rewards_per_episode: -1.6800000000000013\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3986/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 3987/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3988/25000, steps: 800, e: 0.15\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 3989/25000, steps: 800, e: 0.15\n",
      "accumulated_rewards_per_episode: -5.999999999999895\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3990/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 3991/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3992/25000, steps: 797, e: 0.15\n",
      "accumulated_rewards_per_episode: -5.969999999999897\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3993/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3994/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3995/25000, steps: 101, e: 0.15\n",
      "accumulated_rewards_per_episode: -1.0100000000000007\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 3996/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3997/25000, steps: 316, e: 0.15\n",
      "accumulated_rewards_per_episode: -1.160000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 3998/25000, steps: 800, e: 0.15\n",
      "accumulated_rewards_per_episode: -4.999999999999896\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3999/25000, steps: 800, e: 0.15\n",
      "accumulated_rewards_per_episode: -3.9999999999999396\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4000/25000, steps: 800, e: 0.15\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4001/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4002/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4003/25000, steps: 800, e: 0.15\n",
      "accumulated_rewards_per_episode: -4.999999999999896\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 4004/25000, steps: 666, e: 0.15\n",
      "accumulated_rewards_per_episode: -4.659999999999925\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4005/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 4006/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4007/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 4008/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 4009/25000, steps: 800, e: 0.15\n",
      "accumulated_rewards_per_episode: -4.999999999999895\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 4010/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 4011/25000, steps: 800, e: 0.15\n",
      "accumulated_rewards_per_episode: -4.999999999999898\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4012/25000, steps: 229, e: 0.15\n",
      "accumulated_rewards_per_episode: -1.2900000000000016\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 4013/25000, steps: 157, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5700000000000011\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4014/25000, steps: 137, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.3700000000000007\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 4015/25000, steps: 390, e: 0.15\n",
      "accumulated_rewards_per_episode: -1.9000000000000026\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 4016/25000, steps: 733, e: 0.15\n",
      "accumulated_rewards_per_episode: -5.329999999999932\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 4017/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4018/25000, steps: 226, e: 0.15\n",
      "accumulated_rewards_per_episode: -1.2600000000000005\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4019/25000, steps: 269, e: 0.15\n",
      "accumulated_rewards_per_episode: -2.6899999999999866\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 4020/25000, steps: 800, e: 0.15\n",
      "accumulated_rewards_per_episode: -4.9999999999999165\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4021/25000, steps: 640, e: 0.15\n",
      "accumulated_rewards_per_episode: -3.399999999999921\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4022/25000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 4023/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4024/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 4025/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4026/25000, steps: 79, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.2099999999999993\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4027/25000, steps: 355, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.4499999999999974\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 4028/25000, steps: 340, e: 0.14\n",
      "accumulated_rewards_per_episode: -1.399999999999995\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4029/25000, steps: 197, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.029999999999998583\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4030/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 4031/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 4032/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4033/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4034/25000, steps: 57, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5700000000000003\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 4035/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4036/25000, steps: 62, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.6200000000000003\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4037/25000, steps: 57, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5700000000000003\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 4038/25000, steps: 716, e: 0.14\n",
      "accumulated_rewards_per_episode: -2.1599999999999033\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 4039/25000, steps: 203, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.03000000000000142\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4040/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4041/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 4042/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 4043/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4044/25000, steps: 106, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.06000000000000054\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 4045/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4046/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 4047/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 4048/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4049/25000, steps: 226, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.7399999999999989\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 4050/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 4051/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4052/25000, steps: 67, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.3299999999999996\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 4053/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4054/25000, steps: 57, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.4299999999999996\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4055/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4056/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 4057/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 4058/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 4059/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4060/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 4061/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4062/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4063/25000, steps: 407, e: 0.14\n",
      "accumulated_rewards_per_episode: -3.069999999999977\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4064/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4065/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4066/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 4067/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 4068/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 4069/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4070/25000, steps: 578, e: 0.14\n",
      "accumulated_rewards_per_episode: -4.779999999999943\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 4071/25000, steps: 79, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.20999999999999952\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4072/25000, steps: 65, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.34999999999999964\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 4073/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4074/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 4075/25000, steps: 765, e: 0.14\n",
      "accumulated_rewards_per_episode: -5.6499999999999035\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 4076/25000, steps: 311, e: 0.14\n",
      "accumulated_rewards_per_episode: -2.11\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4077/25000, steps: 800, e: 0.14\n",
      "accumulated_rewards_per_episode: -3.9999999999999307\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 4078/25000, steps: 65, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.6500000000000004\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 4079/25000, steps: 57, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.4299999999999996\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 4080/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 4081/25000, steps: 752, e: 0.14\n",
      "accumulated_rewards_per_episode: -6.519999999999884\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4082/25000, steps: 287, e: 0.14\n",
      "accumulated_rewards_per_episode: -1.8699999999999892\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4083/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 4084/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4085/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 4086/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 4087/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4088/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4089/25000, steps: 98, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.019999999999999428\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4090/25000, steps: 800, e: 0.14\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4091/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4092/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4093/25000, steps: 800, e: 0.14\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 4094/25000, steps: 89, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.10999999999999946\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 4095/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 4096/25000, steps: 421, e: 0.14\n",
      "accumulated_rewards_per_episode: -3.209999999999976\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 4097/25000, steps: 583, e: 0.14\n",
      "accumulated_rewards_per_episode: -4.82999999999992\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 4098/25000, steps: 800, e: 0.14\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 4099/25000, steps: 800, e: 0.14\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4100/25000, steps: 800, e: 0.14\n",
      "accumulated_rewards_per_episode: -3.999999999999896\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 4101/25000, steps: 800, e: 0.14\n",
      "accumulated_rewards_per_episode: -5.999999999999917\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4102/25000, steps: 800, e: 0.14\n",
      "accumulated_rewards_per_episode: -3.9999999999999396\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4103/25000, steps: 800, e: 0.14\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 4104/25000, steps: 335, e: 0.14\n",
      "accumulated_rewards_per_episode: -2.3499999999999943\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4105/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4106/25000, steps: 800, e: 0.14\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 4107/25000, steps: 60, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.6000000000000003\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4108/25000, steps: 81, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.1899999999999994\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4109/25000, steps: 660, e: 0.14\n",
      "accumulated_rewards_per_episode: -6.599999999999904\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4110/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 4111/25000, steps: 800, e: 0.14\n",
      "accumulated_rewards_per_episode: -4.999999999999918\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4112/25000, steps: 460, e: 0.14\n",
      "accumulated_rewards_per_episode: -3.599999999999968\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4113/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4114/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 4115/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4116/25000, steps: 800, e: 0.14\n",
      "accumulated_rewards_per_episode: -4.999999999999896\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 4117/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4118/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 4119/25000, steps: 75, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.24999999999999944\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 4120/25000, steps: 114, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.14000000000000073\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4121/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4122/25000, steps: 330, e: 0.14\n",
      "accumulated_rewards_per_episode: -3.2999999999999736\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 4123/25000, steps: 66, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.6600000000000004\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 4124/25000, steps: 114, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.1400000000000005\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4125/25000, steps: 492, e: 0.14\n",
      "accumulated_rewards_per_episode: -2.9199999999999786\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4126/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4127/25000, steps: 639, e: 0.14\n",
      "accumulated_rewards_per_episode: -5.38999999999993\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4128/25000, steps: 800, e: 0.14\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4129/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4130/25000, steps: 744, e: 0.14\n",
      "accumulated_rewards_per_episode: -2.439999999999996\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4131/25000, steps: 800, e: 0.14\n",
      "accumulated_rewards_per_episode: -4.999999999999917\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4132/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4133/25000, steps: 687, e: 0.14\n",
      "accumulated_rewards_per_episode: -5.869999999999898\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4134/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4135/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 4136/25000, steps: 777, e: 0.14\n",
      "accumulated_rewards_per_episode: -4.769999999999922\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4137/25000, steps: 755, e: 0.14\n",
      "accumulated_rewards_per_episode: -7.5499999999998835\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4138/25000, steps: 800, e: 0.14\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4139/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4140/25000, steps: 547, e: 0.14\n",
      "accumulated_rewards_per_episode: -4.469999999999928\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4141/25000, steps: 525, e: 0.14\n",
      "accumulated_rewards_per_episode: -3.249999999999976\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 4142/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 4143/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4144/25000, steps: 63, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.36999999999999966\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4145/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 4146/25000, steps: 642, e: 0.14\n",
      "accumulated_rewards_per_episode: -5.419999999999908\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 4147/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 4148/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 4149/25000, steps: 476, e: 0.14\n",
      "accumulated_rewards_per_episode: -3.759999999999943\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4150/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 4151/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 4152/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4153/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 4154/25000, steps: 583, e: 0.14\n",
      "accumulated_rewards_per_episode: -3.829999999999943\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 4155/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4156/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 4157/25000, steps: 537, e: 0.14\n",
      "accumulated_rewards_per_episode: -3.369999999999969\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4158/25000, steps: 800, e: 0.14\n",
      "accumulated_rewards_per_episode: -3.9999999999999605\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 4159/25000, steps: 78, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.7800000000000005\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 4160/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4161/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4162/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4163/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 4164/25000, steps: 84, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.15999999999999925\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4165/25000, steps: 99, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.00999999999999947\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 4166/25000, steps: 800, e: 0.14\n",
      "accumulated_rewards_per_episode: -5.999999999999908\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4167/25000, steps: 382, e: 0.14\n",
      "accumulated_rewards_per_episode: -2.8199999999999847\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4168/25000, steps: 67, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.3299999999999996\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4169/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4170/25000, steps: 151, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.4899999999999989\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 4171/25000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 4172/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4173/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 4174/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 4175/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -4.999999999999915\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4176/25000, steps: 453, e: 0.13\n",
      "accumulated_rewards_per_episode: -3.5299999999999696\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 4177/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -4.999999999999918\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4178/25000, steps: 735, e: 0.13\n",
      "accumulated_rewards_per_episode: -3.349999999999925\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 4179/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -4.999999999999907\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 4180/25000, steps: 710, e: 0.13\n",
      "accumulated_rewards_per_episode: -2.0999999999999757\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4181/25000, steps: 74, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.25999999999999956\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 4182/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 4183/25000, steps: 55, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.44999999999999973\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4184/25000, steps: 215, e: 0.13\n",
      "accumulated_rewards_per_episode: -2.149999999999998\n",
      "START state: (0, 0, 5, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4185/25000, steps: 590, e: 0.13\n",
      "accumulated_rewards_per_episode: -4.899999999999919\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 4186/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -4.999999999999918\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 4187/25000, steps: 159, e: 0.13\n",
      "accumulated_rewards_per_episode: -1.5900000000000012\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4188/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 4189/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 4190/25000, steps: 507, e: 0.13\n",
      "accumulated_rewards_per_episode: -4.0699999999999585\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 4191/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4192/25000, steps: 160, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.600000000000001\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4193/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 4194/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -5.999999999999874\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4195/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 4196/25000, steps: 313, e: 0.13\n",
      "accumulated_rewards_per_episode: -2.129999999999993\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4197/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4198/25000, steps: 64, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.35999999999999965\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 4199/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4200/25000, steps: 200, e: 0.13\n",
      "accumulated_rewards_per_episode: -2.0000000000000013\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4201/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -4.999999999999916\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4202/25000, steps: 601, e: 0.13\n",
      "accumulated_rewards_per_episode: -5.0099999999999385\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 4203/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -4.9999999999999005\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4204/25000, steps: 445, e: 0.13\n",
      "accumulated_rewards_per_episode: -3.449999999999971\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4205/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4206/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 4207/25000, steps: 271, e: 0.13\n",
      "accumulated_rewards_per_episode: -1.7100000000000017\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 4208/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4209/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -5.999999999999896\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4210/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 4211/25000, steps: 773, e: 0.13\n",
      "accumulated_rewards_per_episode: -6.72999999999988\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4212/25000, steps: 91, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.08999999999999947\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4213/25000, steps: 447, e: 0.13\n",
      "accumulated_rewards_per_episode: -3.4699999999999704\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4214/25000, steps: 75, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.24999999999999956\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 4215/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 4216/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 4217/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4218/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4219/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4220/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 4221/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4222/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 4223/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -4.99999999999988\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4224/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4225/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 4226/25000, steps: 604, e: 0.13\n",
      "accumulated_rewards_per_episode: -5.039999999999916\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 4227/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 4228/25000, steps: 154, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5400000000000011\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 4229/25000, steps: 112, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.12000000000000062\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 4230/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4231/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -2.0000000000000058\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4232/25000, steps: 191, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.9100000000000014\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 4233/25000, steps: 128, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.28000000000000075\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4234/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -3.9999999999999334\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4235/25000, steps: 430, e: 0.13\n",
      "accumulated_rewards_per_episode: -1.300000000000003\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 4236/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4237/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4238/25000, steps: 204, e: 0.13\n",
      "accumulated_rewards_per_episode: -2.0400000000000005\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 4239/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 4240/25000, steps: 86, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.13999999999999935\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4241/25000, steps: 588, e: 0.13\n",
      "accumulated_rewards_per_episode: -4.879999999999919\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 4242/25000, steps: 296, e: 0.13\n",
      "accumulated_rewards_per_episode: -1.9599999999999835\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4243/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -2.9999999999999836\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4244/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4245/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4246/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 4247/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -2.9999999999999787\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4248/25000, steps: 467, e: 0.13\n",
      "accumulated_rewards_per_episode: -3.669999999999954\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4249/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4250/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4251/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4252/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 4253/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -5.999999999999918\n",
      "START state: (0, 0, 0, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4254/25000, steps: 144, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.4400000000000008\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 4255/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -5.999999999999895\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 4256/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -5.999999999999895\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4257/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4258/25000, steps: 713, e: 0.13\n",
      "accumulated_rewards_per_episode: -2.1300000000000026\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 4259/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -4.999999999999917\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 4260/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 4261/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4262/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -5.999999999999895\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4263/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4264/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 4265/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 4266/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 4267/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 4268/25000, steps: 431, e: 0.13\n",
      "accumulated_rewards_per_episode: -2.3099999999999747\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 4269/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 4270/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -3.9999999999999414\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 4271/25000, steps: 77, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.22999999999999954\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4272/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4273/25000, steps: 90, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.09999999999999935\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 4274/25000, steps: 263, e: 0.13\n",
      "accumulated_rewards_per_episode: -1.630000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4275/25000, steps: 58, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.4199999999999996\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4276/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4277/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 4278/25000, steps: 93, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.06999999999999937\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4279/25000, steps: 54, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.45999999999999974\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4280/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4281/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 4282/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4283/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -5.999999999999895\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4284/25000, steps: 53, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.46999999999999964\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4285/25000, steps: 243, e: 0.13\n",
      "accumulated_rewards_per_episode: -1.4300000000000015\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4286/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4287/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4288/25000, steps: 173, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.7300000000000012\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4289/25000, steps: 294, e: 0.13\n",
      "accumulated_rewards_per_episode: -1.9399999999999857\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4290/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -5.999999999999896\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 4291/25000, steps: 62, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.37999999999999956\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 4292/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4293/25000, steps: 62, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.37999999999999967\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4294/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 4295/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4296/25000, steps: 132, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.3200000000000008\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4297/25000, steps: 187, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.8700000000000011\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 4298/25000, steps: 605, e: 0.13\n",
      "accumulated_rewards_per_episode: -5.049999999999937\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 4299/25000, steps: 166, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.660000000000001\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4300/25000, steps: 92, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.07999999999999936\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 4301/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 4302/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 4303/25000, steps: 124, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.2400000000000007\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4304/25000, steps: 329, e: 0.13\n",
      "accumulated_rewards_per_episode: -2.289999999999994\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4305/25000, steps: 54, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5400000000000003\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4306/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4307/25000, steps: 120, e: 0.13\n",
      "accumulated_rewards_per_episode: -1.2000000000000008\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 4308/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 4309/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4310/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 4311/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4312/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 4313/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4314/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4315/25000, steps: 66, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.33999999999999964\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4316/25000, steps: 583, e: 0.13\n",
      "accumulated_rewards_per_episode: -2.829999999999986\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 4317/25000, steps: 800, e: 0.13\n",
      "accumulated_rewards_per_episode: -2.999999999999984\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 4318/25000, steps: 542, e: 0.13\n",
      "accumulated_rewards_per_episode: -4.419999999999951\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 4319/25000, steps: 425, e: 0.13\n",
      "accumulated_rewards_per_episode: -2.2499999999999973\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 4320/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 4321/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4322/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4323/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 4324/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 4325/25000, steps: 71, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.2899999999999995\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4326/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 4327/25000, steps: 591, e: 0.13\n",
      "accumulated_rewards_per_episode: -2.909999999999962\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4328/25000, steps: 78, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.21999999999999942\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 4329/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 4330/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4331/25000, steps: 63, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.6300000000000003\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 4332/25000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 4333/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4334/25000, steps: 720, e: 0.12\n",
      "accumulated_rewards_per_episode: -6.199999999999891\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 4335/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 4336/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 4337/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4338/25000, steps: 584, e: 0.12\n",
      "accumulated_rewards_per_episode: -4.83999999999993\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4339/25000, steps: 500, e: 0.12\n",
      "accumulated_rewards_per_episode: -3.999999999999938\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4340/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4341/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 4342/25000, steps: 76, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.23999999999999944\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 4343/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 4344/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 4345/25000, steps: 74, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.25999999999999956\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4346/25000, steps: 61, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.3899999999999997\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4347/25000, steps: 63, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.36999999999999966\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 4348/25000, steps: 280, e: 0.12\n",
      "accumulated_rewards_per_episode: -1.8000000000000023\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 4349/25000, steps: 228, e: 0.12\n",
      "accumulated_rewards_per_episode: 1.719999999999998\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 4350/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4351/25000, steps: 800, e: 0.12\n",
      "accumulated_rewards_per_episode: -3.9999999999999623\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4352/25000, steps: 800, e: 0.12\n",
      "accumulated_rewards_per_episode: -5.999999999999895\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 4353/25000, steps: 800, e: 0.12\n",
      "accumulated_rewards_per_episode: -3.999999999999962\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 4354/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4355/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4356/25000, steps: 132, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.3200000000000009\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4357/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 4358/25000, steps: 800, e: 0.12\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4359/25000, steps: 491, e: 0.12\n",
      "accumulated_rewards_per_episode: -1.9100000000000037\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4360/25000, steps: 59, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.4099999999999996\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 4361/25000, steps: 92, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.07999999999999925\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4362/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4363/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 4364/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4365/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 4366/25000, steps: 632, e: 0.12\n",
      "accumulated_rewards_per_episode: -3.3199999999999363\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 4367/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4368/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 4369/25000, steps: 119, e: 0.12\n",
      "accumulated_rewards_per_episode: -1.1900000000000008\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4370/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4371/25000, steps: 80, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.1999999999999995\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 4372/25000, steps: 93, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.9300000000000006\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4373/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 4374/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4375/25000, steps: 786, e: 0.12\n",
      "accumulated_rewards_per_episode: -4.859999999999877\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4376/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 4377/25000, steps: 412, e: 0.12\n",
      "accumulated_rewards_per_episode: -3.1199999999999783\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4378/25000, steps: 525, e: 0.12\n",
      "accumulated_rewards_per_episode: -2.2499999999999774\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 4379/25000, steps: 800, e: 0.12\n",
      "accumulated_rewards_per_episode: -5.999999999999896\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 4380/25000, steps: 105, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.050000000000000606\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 4381/25000, steps: 800, e: 0.12\n",
      "accumulated_rewards_per_episode: -4.999999999999918\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4382/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 4383/25000, steps: 60, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.3999999999999997\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 4384/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 4385/25000, steps: 285, e: 0.12\n",
      "accumulated_rewards_per_episode: -1.8499999999999923\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4386/25000, steps: 444, e: 0.12\n",
      "accumulated_rewards_per_episode: -2.4399999999999498\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4387/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4388/25000, steps: 800, e: 0.12\n",
      "accumulated_rewards_per_episode: -2.999999999999895\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4389/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4390/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4391/25000, steps: 800, e: 0.12\n",
      "accumulated_rewards_per_episode: -4.999999999999896\n",
      "START state: (0, 0, 0, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4392/25000, steps: 607, e: 0.12\n",
      "accumulated_rewards_per_episode: -5.069999999999934\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 4393/25000, steps: 765, e: 0.12\n",
      "accumulated_rewards_per_episode: -2.649999999999916\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4394/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4395/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4396/25000, steps: 215, e: 0.12\n",
      "accumulated_rewards_per_episode: -1.1500000000000015\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4397/25000, steps: 230, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.30000000000000154\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4398/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 4399/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 4400/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4401/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4402/25000, steps: 271, e: 0.12\n",
      "accumulated_rewards_per_episode: -1.7099999999999966\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4403/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4404/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 4405/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4406/25000, steps: 800, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.9999999999999116\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 4407/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 4408/25000, steps: 800, e: 0.12\n",
      "accumulated_rewards_per_episode: 5.000000000000084\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 4409/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4410/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4411/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4412/25000, steps: 55, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.4499999999999996\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 4413/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 4414/25000, steps: 362, e: 0.12\n",
      "accumulated_rewards_per_episode: -2.619999999999989\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 4415/25000, steps: 329, e: 0.12\n",
      "accumulated_rewards_per_episode: -2.289999999999996\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 4416/25000, steps: 228, e: 0.12\n",
      "accumulated_rewards_per_episode: -1.2799999999999991\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4417/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 4418/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4419/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4420/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 4421/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4422/25000, steps: 465, e: 0.12\n",
      "accumulated_rewards_per_episode: -2.6499999999999675\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4423/25000, steps: 126, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.26000000000000073\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4424/25000, steps: 69, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.3099999999999996\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 4425/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 4426/25000, steps: 161, e: 0.12\n",
      "accumulated_rewards_per_episode: -1.6100000000000012\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 4427/25000, steps: 85, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.14999999999999936\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 4428/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4429/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4430/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4431/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 4432/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 4433/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 4434/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4435/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4436/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4437/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4438/25000, steps: 800, e: 0.12\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4439/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4440/25000, steps: 359, e: 0.12\n",
      "accumulated_rewards_per_episode: -2.589999999999968\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4441/25000, steps: 59, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5900000000000003\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 4442/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4443/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 4444/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 4445/25000, steps: 115, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.15000000000000063\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4446/25000, steps: 55, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.44999999999999973\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 4447/25000, steps: 800, e: 0.12\n",
      "accumulated_rewards_per_episode: -3.9999999999999174\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 4448/25000, steps: 64, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.3599999999999996\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 4449/25000, steps: 124, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.24000000000000088\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 4450/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 4451/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4452/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4453/25000, steps: 164, e: 0.12\n",
      "accumulated_rewards_per_episode: -1.6400000000000012\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4454/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 4455/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4456/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 4457/25000, steps: 83, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.16999999999999948\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 4458/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 4459/25000, steps: 411, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.8900000000000124\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 4460/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4461/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 4462/25000, steps: 287, e: 0.12\n",
      "accumulated_rewards_per_episode: -1.869999999999985\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4463/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 4464/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 4465/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4466/25000, steps: 687, e: 0.12\n",
      "accumulated_rewards_per_episode: 2.1300000000000567\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4467/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 4468/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 4469/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 4470/25000, steps: 59, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.4099999999999997\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 4471/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4472/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 4473/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 4474/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4475/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4476/25000, steps: 68, e: 0.12\n",
      "accumulated_rewards_per_episode: 1.3199999999999996\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 4477/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 4478/25000, steps: 201, e: 0.12\n",
      "accumulated_rewards_per_episode: -1.0100000000000013\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 4479/25000, steps: 547, e: 0.12\n",
      "accumulated_rewards_per_episode: -3.469999999999949\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4480/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 4481/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 4482/25000, steps: 66, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.33999999999999964\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 4483/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 4484/25000, steps: 800, e: 0.12\n",
      "accumulated_rewards_per_episode: -5.999999999999874\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 4485/25000, steps: 343, e: 0.12\n",
      "accumulated_rewards_per_episode: -2.429999999999993\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 4486/25000, steps: 800, e: 0.12\n",
      "accumulated_rewards_per_episode: -3.9999999999999174\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 4487/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 4488/25000, steps: 320, e: 0.12\n",
      "accumulated_rewards_per_episode: -3.1999999999999758\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4489/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 4490/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 4491/25000, steps: 54, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5400000000000003\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4492/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 4493/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 4494/25000, steps: 320, e: 0.12\n",
      "accumulated_rewards_per_episode: -2.1999999999999975\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4495/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 4496/25000, steps: 79, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.7900000000000005\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 4497/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 4498/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4499/25000, steps: 56, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.4399999999999996\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 4500/25000, steps: 327, e: 0.12\n",
      "accumulated_rewards_per_episode: 2.7300000000000555\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 4501/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4502/25000, steps: 60, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.39999999999999947\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 4503/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 4504/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 4505/25000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4506/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4507/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 4508/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 4509/25000, steps: 60, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.3999999999999997\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 4510/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 4511/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4512/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 4513/25000, steps: 289, e: 0.11\n",
      "accumulated_rewards_per_episode: -2.8899999999999824\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4514/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 4515/25000, steps: 280, e: 0.11\n",
      "accumulated_rewards_per_episode: -2.7999999999999843\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4516/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 4517/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 4518/25000, steps: 474, e: 0.11\n",
      "accumulated_rewards_per_episode: -1.7399999999999978\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4519/25000, steps: 267, e: 0.11\n",
      "accumulated_rewards_per_episode: -1.6699999999999897\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4520/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4521/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 4522/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 4523/25000, steps: 476, e: 0.11\n",
      "accumulated_rewards_per_episode: -4.759999999999943\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4524/25000, steps: 320, e: 0.11\n",
      "accumulated_rewards_per_episode: -2.1999999999999975\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4525/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 4526/25000, steps: 210, e: 0.11\n",
      "accumulated_rewards_per_episode: -2.099999999999999\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4527/25000, steps: 263, e: 0.11\n",
      "accumulated_rewards_per_episode: -1.6300000000000017\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4528/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 4529/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4530/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4531/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 4532/25000, steps: 109, e: 0.11\n",
      "accumulated_rewards_per_episode: -1.0900000000000007\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4533/25000, steps: 275, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.750000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 4534/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 4535/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: -5.999999999999891\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4536/25000, steps: 562, e: 0.11\n",
      "accumulated_rewards_per_episode: -5.619999999999925\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4537/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 4538/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 4539/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 4540/25000, steps: 118, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.18000000000000066\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4541/25000, steps: 431, e: 0.11\n",
      "accumulated_rewards_per_episode: -3.3099999999999525\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 4542/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 4543/25000, steps: 561, e: 0.11\n",
      "accumulated_rewards_per_episode: -3.609999999999964\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4544/25000, steps: 62, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.37999999999999956\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4545/25000, steps: 53, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5300000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4546/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4547/25000, steps: 58, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5800000000000003\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4548/25000, steps: 159, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5900000000000009\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4549/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 4550/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 4551/25000, steps: 53, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5300000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4552/25000, steps: 610, e: 0.11\n",
      "accumulated_rewards_per_episode: -4.099999999999937\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 4553/25000, steps: 68, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.3199999999999994\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4554/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4555/25000, steps: 137, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.37000000000000094\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4556/25000, steps: 76, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.23999999999999955\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 4557/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 4558/25000, steps: 83, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.8300000000000005\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4559/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 4560/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 4561/25000, steps: 358, e: 0.11\n",
      "accumulated_rewards_per_episode: -1.5799999999999947\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4562/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4563/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4564/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 4565/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4566/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4567/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4568/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 4569/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 1.4799999999999998\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 4570/25000, steps: 413, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.1300000000000025\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 4571/25000, steps: 120, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.7999999999999989\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4572/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 4573/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4574/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4575/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4576/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4577/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4578/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4579/25000, steps: 62, e: 0.11\n",
      "accumulated_rewards_per_episode: 1.3799999999999997\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 4580/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 4581/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 4582/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4583/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4584/25000, steps: 54, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.45999999999999974\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4585/25000, steps: 73, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.7300000000000004\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 4586/25000, steps: 263, e: 0.11\n",
      "accumulated_rewards_per_episode: -1.6299999999999941\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 4587/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4588/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 4589/25000, steps: 168, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.6800000000000012\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4590/25000, steps: 268, e: 0.11\n",
      "accumulated_rewards_per_episode: -1.679999999999989\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 4591/25000, steps: 55, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.4499999999999995\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4592/25000, steps: 96, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.039999999999999473\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 4593/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4594/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4595/25000, steps: 199, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.9900000000000015\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 4596/25000, steps: 60, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.3999999999999997\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4597/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4598/25000, steps: 72, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.2799999999999996\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 4599/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4600/25000, steps: 131, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.31000000000000066\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4601/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 4602/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 4603/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 4604/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4605/25000, steps: 59, e: 0.11\n",
      "accumulated_rewards_per_episode: 1.4099999999999997\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 4606/25000, steps: 78, e: 0.11\n",
      "accumulated_rewards_per_episode: 1.2199999999999995\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 4607/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 4608/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 4609/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 4610/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 4611/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 4612/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4613/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4614/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 4615/25000, steps: 338, e: 0.11\n",
      "accumulated_rewards_per_episode: -3.379999999999972\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 4616/25000, steps: 261, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.6100000000000017\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4617/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4618/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 4619/25000, steps: 73, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.2699999999999996\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4620/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 4621/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4622/25000, steps: 255, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.4499999999999982\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 4623/25000, steps: 119, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.19000000000000056\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 4624/25000, steps: 146, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.5399999999999989\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4625/25000, steps: 168, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.6800000000000009\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4626/25000, steps: 172, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.7200000000000013\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 4627/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 4628/25000, steps: 56, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.4399999999999996\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4629/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4630/25000, steps: 84, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.15999999999999948\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 4631/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4632/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 4633/25000, steps: 301, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.010000000000002307\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4634/25000, steps: 63, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.6300000000000003\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4635/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4636/25000, steps: 480, e: 0.11\n",
      "accumulated_rewards_per_episode: -3.799999999999942\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4637/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: 3.0000000000000293\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 4638/25000, steps: 700, e: 0.11\n",
      "accumulated_rewards_per_episode: -5.9999999999999165\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4639/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4640/25000, steps: 328, e: 0.11\n",
      "accumulated_rewards_per_episode: -1.280000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4641/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4642/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: -5.999999999999884\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4643/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: -3.9999999999999405\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 4644/25000, steps: 443, e: 0.11\n",
      "accumulated_rewards_per_episode: -2.4299999999999935\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 4645/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 4646/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4647/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: -4.9999999999999405\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4648/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 4649/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: -5.999999999999918\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 4650/25000, steps: 177, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.7700000000000012\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 4651/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: -1.999999999999913\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4652/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: -4.999999999999917\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 4653/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: -5.999999999999896\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4654/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 4655/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: -2.999999999999874\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 4656/25000, steps: 555, e: 0.11\n",
      "accumulated_rewards_per_episode: -3.5499999999999665\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4657/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4658/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4659/25000, steps: 118, e: 0.11\n",
      "accumulated_rewards_per_episode: -1.1800000000000008\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4660/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4661/25000, steps: 86, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.8600000000000005\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4662/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4663/25000, steps: 63, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.6300000000000003\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4664/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4665/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4666/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 4667/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 4668/25000, steps: 262, e: 0.11\n",
      "accumulated_rewards_per_episode: -1.6200000000000017\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4669/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4670/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4671/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4672/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 4673/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 4674/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 4675/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 4676/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 4677/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4678/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4679/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4680/25000, steps: 156, e: 0.11\n",
      "accumulated_rewards_per_episode: -1.5600000000000012\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 4681/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: -4.9999999999998765\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4682/25000, steps: 99, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.9900000000000007\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 4683/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4684/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4685/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: -3.9999999999999534\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 4686/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: -3.9999999999999183\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 4687/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: -1.9999999999999916\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 4688/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: -3.9999999999999307\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4689/25000, steps: 56, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.4399999999999996\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 4690/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: -4.99999999999994\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 4691/25000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4692/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: -3.9999999999999396\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 4693/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 4694/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: 7.0000000000001075\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4695/25000, steps: 800, e: 0.11\n",
      "accumulated_rewards_per_episode: -3.9999999999999183\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4696/25000, steps: 800, e: 0.1\n",
      "accumulated_rewards_per_episode: 1.000000000000088\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4697/25000, steps: 800, e: 0.1\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4698/25000, steps: 800, e: 0.1\n",
      "accumulated_rewards_per_episode: -2.0000000000000036\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 4699/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4700/25000, steps: 800, e: 0.1\n",
      "accumulated_rewards_per_episode: 8.00000000000005\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 4701/25000, steps: 800, e: 0.1\n",
      "accumulated_rewards_per_episode: 2.0000000000001226\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 4702/25000, steps: 800, e: 0.1\n",
      "accumulated_rewards_per_episode: -4.99999999999994\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 4703/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4704/25000, steps: 800, e: 0.1\n",
      "accumulated_rewards_per_episode: -5.999999999999895\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4705/25000, steps: 193, e: 0.1\n",
      "accumulated_rewards_per_episode: 5.070000000000029\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4706/25000, steps: 76, e: 0.1\n",
      "accumulated_rewards_per_episode: 2.240000000000003\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4707/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4708/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4709/25000, steps: 155, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5500000000000008\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4710/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 4711/25000, steps: 800, e: 0.1\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 4712/25000, steps: 800, e: 0.1\n",
      "accumulated_rewards_per_episode: -4.999999999999905\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 4713/25000, steps: 405, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.9500000000000262\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4714/25000, steps: 146, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.4600000000000008\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4715/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4716/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 4717/25000, steps: 97, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.02999999999999947\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4718/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4719/25000, steps: 800, e: 0.1\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 4720/25000, steps: 462, e: 0.1\n",
      "accumulated_rewards_per_episode: -1.6199999999999968\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4721/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 4722/25000, steps: 69, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.3099999999999996\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4723/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 4724/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4725/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4726/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 4727/25000, steps: 315, e: 0.1\n",
      "accumulated_rewards_per_episode: -2.149999999999999\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 4728/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4729/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 4730/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 4731/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4732/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 4733/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4734/25000, steps: 800, e: 0.1\n",
      "accumulated_rewards_per_episode: -1.9999999999999334\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 4735/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 4736/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 4737/25000, steps: 119, e: 0.1\n",
      "accumulated_rewards_per_episode: -1.1900000000000008\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 4738/25000, steps: 69, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.6900000000000004\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4739/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4740/25000, steps: 420, e: 0.1\n",
      "accumulated_rewards_per_episode: -3.1999999999999766\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4741/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4742/25000, steps: 732, e: 0.1\n",
      "accumulated_rewards_per_episode: 3.680000000000053\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4743/25000, steps: 78, e: 0.1\n",
      "accumulated_rewards_per_episode: 2.2200000000000033\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 4744/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4745/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4746/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 4747/25000, steps: 202, e: 0.1\n",
      "accumulated_rewards_per_episode: -1.0200000000000016\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 4748/25000, steps: 76, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.23999999999999955\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 4749/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 4750/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 4751/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 4752/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 4753/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 4754/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 4755/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4756/25000, steps: 199, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.009999999999998581\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 4757/25000, steps: 398, e: 0.1\n",
      "accumulated_rewards_per_episode: 1.0200000000000284\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 4758/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4759/25000, steps: 58, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.4199999999999996\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 4760/25000, steps: 62, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.37999999999999956\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4761/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 4762/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4763/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 4764/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4765/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4766/25000, steps: 55, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.44999999999999973\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4767/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 4768/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4769/25000, steps: 800, e: 0.1\n",
      "accumulated_rewards_per_episode: 8.000000000000087\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4770/25000, steps: 561, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.6099999999999656\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 4771/25000, steps: 78, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.21999999999999942\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 4772/25000, steps: 99, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.009999999999999358\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4773/25000, steps: 67, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.3299999999999996\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 4774/25000, steps: 509, e: 0.1\n",
      "accumulated_rewards_per_episode: -4.089999999999936\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 4775/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4776/25000, steps: 401, e: 0.1\n",
      "accumulated_rewards_per_episode: -1.010000000000003\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4777/25000, steps: 93, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.06999999999999937\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4778/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4779/25000, steps: 140, e: 0.1\n",
      "accumulated_rewards_per_episode: 1.5999999999999988\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4780/25000, steps: 70, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.2999999999999996\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 4781/25000, steps: 85, e: 0.1\n",
      "accumulated_rewards_per_episode: 1.1499999999999995\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 4782/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 4783/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 4784/25000, steps: 105, e: 0.1\n",
      "accumulated_rewards_per_episode: 1.9500000000000008\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4785/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4786/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 4787/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 4788/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 4789/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4790/25000, steps: 101, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.010000000000000545\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 4791/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 4792/25000, steps: 53, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.46999999999999964\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4793/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4794/25000, steps: 104, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.040000000000000646\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4795/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4796/25000, steps: 196, e: 0.1\n",
      "accumulated_rewards_per_episode: -1.9600000000000015\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4797/25000, steps: 567, e: 0.1\n",
      "accumulated_rewards_per_episode: 1.3299999999999956\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4798/25000, steps: 100, e: 0.1\n",
      "accumulated_rewards_per_episode: -5.30825383648903e-16\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4799/25000, steps: 96, e: 0.1\n",
      "accumulated_rewards_per_episode: 1.0399999999999994\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4800/25000, steps: 211, e: 0.1\n",
      "accumulated_rewards_per_episode: 6.890000000000032\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 4801/25000, steps: 126, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.7399999999999993\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 4802/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4803/25000, steps: 108, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.08000000000000053\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4804/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4805/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 4806/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 4807/25000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4808/25000, steps: 112, e: 0.099\n",
      "accumulated_rewards_per_episode: -1.1200000000000008\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4809/25000, steps: 218, e: 0.099\n",
      "accumulated_rewards_per_episode: -2.1799999999999975\n",
      "START state: (0, 0, 7, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4810/25000, steps: 193, e: 0.099\n",
      "accumulated_rewards_per_episode: -0.9300000000000015\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4811/25000, steps: 52, e: 0.099\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4812/25000, steps: 205, e: 0.099\n",
      "accumulated_rewards_per_episode: -1.0500000000000014\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4813/25000, steps: 57, e: 0.099\n",
      "accumulated_rewards_per_episode: -0.5700000000000003\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 4814/25000, steps: 59, e: 0.099\n",
      "accumulated_rewards_per_episode: 0.4099999999999997\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4815/25000, steps: 52, e: 0.099\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4816/25000, steps: 52, e: 0.099\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4817/25000, steps: 669, e: 0.099\n",
      "accumulated_rewards_per_episode: 7.3100000000000644\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 4818/25000, steps: 107, e: 0.099\n",
      "accumulated_rewards_per_episode: -0.07000000000000053\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 4819/25000, steps: 115, e: 0.099\n",
      "accumulated_rewards_per_episode: -0.15000000000000052\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 4820/25000, steps: 52, e: 0.099\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 4821/25000, steps: 52, e: 0.099\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4822/25000, steps: 52, e: 0.099\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 4823/25000, steps: 99, e: 0.099\n",
      "accumulated_rewards_per_episode: 0.009999999999999456\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4824/25000, steps: 52, e: 0.099\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 4825/25000, steps: 800, e: 0.099\n",
      "accumulated_rewards_per_episode: -3.9999999999999063\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4826/25000, steps: 52, e: 0.099\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 4827/25000, steps: 52, e: 0.099\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 4828/25000, steps: 633, e: 0.099\n",
      "accumulated_rewards_per_episode: -6.3299999999999095\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4829/25000, steps: 52, e: 0.098\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4830/25000, steps: 734, e: 0.098\n",
      "accumulated_rewards_per_episode: -3.339999999999976\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4831/25000, steps: 127, e: 0.098\n",
      "accumulated_rewards_per_episode: -0.2700000000000007\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4832/25000, steps: 52, e: 0.098\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 4833/25000, steps: 52, e: 0.098\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4834/25000, steps: 52, e: 0.098\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 4835/25000, steps: 57, e: 0.098\n",
      "accumulated_rewards_per_episode: 0.4299999999999997\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 4836/25000, steps: 52, e: 0.098\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 4837/25000, steps: 505, e: 0.098\n",
      "accumulated_rewards_per_episode: -3.0499999999999807\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4838/25000, steps: 52, e: 0.098\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 4839/25000, steps: 52, e: 0.098\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 4840/25000, steps: 69, e: 0.098\n",
      "accumulated_rewards_per_episode: -0.6900000000000004\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 4841/25000, steps: 71, e: 0.098\n",
      "accumulated_rewards_per_episode: -0.7100000000000004\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4842/25000, steps: 52, e: 0.098\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 4843/25000, steps: 52, e: 0.098\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4844/25000, steps: 120, e: 0.098\n",
      "accumulated_rewards_per_episode: -1.2000000000000008\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4845/25000, steps: 52, e: 0.098\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 4846/25000, steps: 52, e: 0.098\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4847/25000, steps: 52, e: 0.098\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4848/25000, steps: 788, e: 0.098\n",
      "accumulated_rewards_per_episode: -3.8799999999998764\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4849/25000, steps: 52, e: 0.098\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 4850/25000, steps: 52, e: 0.097\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4851/25000, steps: 52, e: 0.097\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 4852/25000, steps: 800, e: 0.097\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 4853/25000, steps: 639, e: 0.097\n",
      "accumulated_rewards_per_episode: -4.389999999999953\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 4854/25000, steps: 52, e: 0.097\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 4855/25000, steps: 194, e: 0.097\n",
      "accumulated_rewards_per_episode: -0.9400000000000012\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4856/25000, steps: 52, e: 0.097\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 4857/25000, steps: 52, e: 0.097\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 4858/25000, steps: 53, e: 0.097\n",
      "accumulated_rewards_per_episode: 0.46999999999999964\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4859/25000, steps: 52, e: 0.097\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 4860/25000, steps: 52, e: 0.097\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 4861/25000, steps: 93, e: 0.097\n",
      "accumulated_rewards_per_episode: 0.06999999999999937\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 4862/25000, steps: 52, e: 0.097\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4863/25000, steps: 75, e: 0.097\n",
      "accumulated_rewards_per_episode: 0.24999999999999956\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 4864/25000, steps: 109, e: 0.097\n",
      "accumulated_rewards_per_episode: -1.0900000000000007\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 4865/25000, steps: 52, e: 0.097\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 4866/25000, steps: 563, e: 0.097\n",
      "accumulated_rewards_per_episode: -0.6300000000000006\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4867/25000, steps: 94, e: 0.097\n",
      "accumulated_rewards_per_episode: -0.9400000000000006\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4868/25000, steps: 59, e: 0.097\n",
      "accumulated_rewards_per_episode: 0.4099999999999995\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 4869/25000, steps: 52, e: 0.097\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 4870/25000, steps: 52, e: 0.097\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4871/25000, steps: 52, e: 0.097\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 4872/25000, steps: 587, e: 0.096\n",
      "accumulated_rewards_per_episode: -4.869999999999919\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 4873/25000, steps: 52, e: 0.096\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 4874/25000, steps: 54, e: 0.096\n",
      "accumulated_rewards_per_episode: 0.45999999999999974\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 4875/25000, steps: 52, e: 0.096\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 4876/25000, steps: 52, e: 0.096\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4877/25000, steps: 53, e: 0.096\n",
      "accumulated_rewards_per_episode: 0.46999999999999975\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 4878/25000, steps: 52, e: 0.096\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4879/25000, steps: 52, e: 0.096\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4880/25000, steps: 87, e: 0.096\n",
      "accumulated_rewards_per_episode: 0.12999999999999945\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4881/25000, steps: 52, e: 0.096\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4882/25000, steps: 52, e: 0.096\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4883/25000, steps: 52, e: 0.096\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4884/25000, steps: 57, e: 0.096\n",
      "accumulated_rewards_per_episode: 0.4299999999999996\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 4885/25000, steps: 52, e: 0.096\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4886/25000, steps: 53, e: 0.096\n",
      "accumulated_rewards_per_episode: 0.46999999999999975\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4887/25000, steps: 52, e: 0.096\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 4888/25000, steps: 52, e: 0.096\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4889/25000, steps: 52, e: 0.096\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 4890/25000, steps: 99, e: 0.096\n",
      "accumulated_rewards_per_episode: 0.009999999999999247\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4891/25000, steps: 52, e: 0.096\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 4892/25000, steps: 800, e: 0.096\n",
      "accumulated_rewards_per_episode: -5.99999999999991\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 4893/25000, steps: 132, e: 0.095\n",
      "accumulated_rewards_per_episode: -1.320000000000001\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4894/25000, steps: 52, e: 0.095\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 4895/25000, steps: 800, e: 0.095\n",
      "accumulated_rewards_per_episode: -3.9999999999999547\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4896/25000, steps: 52, e: 0.095\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4897/25000, steps: 52, e: 0.095\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4898/25000, steps: 193, e: 0.095\n",
      "accumulated_rewards_per_episode: -0.9300000000000015\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4899/25000, steps: 52, e: 0.095\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4900/25000, steps: 800, e: 0.095\n",
      "accumulated_rewards_per_episode: 4.00000000000012\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4901/25000, steps: 52, e: 0.095\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 4902/25000, steps: 52, e: 0.095\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4903/25000, steps: 800, e: 0.095\n",
      "accumulated_rewards_per_episode: -6.081940506774686e-15\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 4904/25000, steps: 52, e: 0.095\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4905/25000, steps: 65, e: 0.095\n",
      "accumulated_rewards_per_episode: -0.6500000000000004\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 4906/25000, steps: 255, e: 0.095\n",
      "accumulated_rewards_per_episode: -1.5499999999999934\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 4907/25000, steps: 72, e: 0.095\n",
      "accumulated_rewards_per_episode: 0.27999999999999947\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4908/25000, steps: 52, e: 0.095\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 4909/25000, steps: 335, e: 0.095\n",
      "accumulated_rewards_per_episode: -0.35000000000000225\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 4910/25000, steps: 52, e: 0.095\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 4911/25000, steps: 257, e: 0.095\n",
      "accumulated_rewards_per_episode: -1.5699999999999918\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 4912/25000, steps: 52, e: 0.095\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 4913/25000, steps: 52, e: 0.095\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 4914/25000, steps: 52, e: 0.095\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 4915/25000, steps: 69, e: 0.094\n",
      "accumulated_rewards_per_episode: -0.6900000000000004\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4916/25000, steps: 127, e: 0.094\n",
      "accumulated_rewards_per_episode: 0.7299999999999991\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 4917/25000, steps: 379, e: 0.094\n",
      "accumulated_rewards_per_episode: -2.7899999999999636\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4918/25000, steps: 52, e: 0.094\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4919/25000, steps: 148, e: 0.094\n",
      "accumulated_rewards_per_episode: -0.4800000000000011\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4920/25000, steps: 79, e: 0.094\n",
      "accumulated_rewards_per_episode: 0.20999999999999952\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 4921/25000, steps: 52, e: 0.094\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 4922/25000, steps: 52, e: 0.094\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 4923/25000, steps: 52, e: 0.094\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4924/25000, steps: 52, e: 0.094\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4925/25000, steps: 52, e: 0.094\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 4926/25000, steps: 52, e: 0.094\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4927/25000, steps: 52, e: 0.094\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4928/25000, steps: 308, e: 0.094\n",
      "accumulated_rewards_per_episode: -3.0799999999999783\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 4929/25000, steps: 52, e: 0.094\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 4930/25000, steps: 69, e: 0.094\n",
      "accumulated_rewards_per_episode: 0.3099999999999996\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4931/25000, steps: 52, e: 0.094\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 4932/25000, steps: 245, e: 0.094\n",
      "accumulated_rewards_per_episode: -1.4499999999999968\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 4933/25000, steps: 627, e: 0.094\n",
      "accumulated_rewards_per_episode: 8.730000000000109\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4934/25000, steps: 52, e: 0.094\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 4935/25000, steps: 224, e: 0.094\n",
      "accumulated_rewards_per_episode: -1.2399999999999998\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4936/25000, steps: 52, e: 0.094\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4937/25000, steps: 52, e: 0.094\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4938/25000, steps: 52, e: 0.093\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4939/25000, steps: 67, e: 0.093\n",
      "accumulated_rewards_per_episode: 0.32999999999999957\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 4940/25000, steps: 52, e: 0.093\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 4941/25000, steps: 52, e: 0.093\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 4942/25000, steps: 52, e: 0.093\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 4943/25000, steps: 800, e: 0.093\n",
      "accumulated_rewards_per_episode: -4.999999999999874\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4944/25000, steps: 343, e: 0.093\n",
      "accumulated_rewards_per_episode: -0.43000000000000216\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 4945/25000, steps: 79, e: 0.093\n",
      "accumulated_rewards_per_episode: 0.20999999999999952\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4946/25000, steps: 52, e: 0.093\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 4947/25000, steps: 52, e: 0.093\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4948/25000, steps: 52, e: 0.093\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4949/25000, steps: 74, e: 0.093\n",
      "accumulated_rewards_per_episode: 1.2599999999999996\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 4950/25000, steps: 52, e: 0.093\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4951/25000, steps: 52, e: 0.093\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 4952/25000, steps: 52, e: 0.093\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4953/25000, steps: 194, e: 0.093\n",
      "accumulated_rewards_per_episode: -0.9400000000000015\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 4954/25000, steps: 52, e: 0.093\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 4955/25000, steps: 800, e: 0.093\n",
      "accumulated_rewards_per_episode: 9.00000000000005\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4956/25000, steps: 169, e: 0.093\n",
      "accumulated_rewards_per_episode: 1.309999999999999\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4957/25000, steps: 52, e: 0.093\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4958/25000, steps: 800, e: 0.093\n",
      "accumulated_rewards_per_episode: -4.99999999999994\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4959/25000, steps: 66, e: 0.093\n",
      "accumulated_rewards_per_episode: 0.3399999999999995\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 4960/25000, steps: 52, e: 0.092\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4961/25000, steps: 268, e: 0.092\n",
      "accumulated_rewards_per_episode: 0.31999999999999806\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4962/25000, steps: 256, e: 0.092\n",
      "accumulated_rewards_per_episode: 0.43999999999999795\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 4963/25000, steps: 74, e: 0.092\n",
      "accumulated_rewards_per_episode: 0.25999999999999956\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 4964/25000, steps: 59, e: 0.092\n",
      "accumulated_rewards_per_episode: 0.4099999999999996\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4965/25000, steps: 52, e: 0.092\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4966/25000, steps: 52, e: 0.092\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 4967/25000, steps: 52, e: 0.092\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 4968/25000, steps: 116, e: 0.092\n",
      "accumulated_rewards_per_episode: -0.16000000000000075\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4969/25000, steps: 52, e: 0.092\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4970/25000, steps: 52, e: 0.092\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4971/25000, steps: 52, e: 0.092\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 4972/25000, steps: 52, e: 0.092\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4973/25000, steps: 617, e: 0.092\n",
      "accumulated_rewards_per_episode: -6.169999999999913\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 4974/25000, steps: 800, e: 0.092\n",
      "accumulated_rewards_per_episode: -5.999999999999909\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 4975/25000, steps: 52, e: 0.092\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4976/25000, steps: 344, e: 0.092\n",
      "accumulated_rewards_per_episode: -1.4400000000000026\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 4977/25000, steps: 52, e: 0.092\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 4978/25000, steps: 800, e: 0.092\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 4979/25000, steps: 125, e: 0.092\n",
      "accumulated_rewards_per_episode: -0.2500000000000006\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 4980/25000, steps: 52, e: 0.092\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4981/25000, steps: 52, e: 0.092\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4982/25000, steps: 52, e: 0.092\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4983/25000, steps: 800, e: 0.091\n",
      "accumulated_rewards_per_episode: -5.999999999999918\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 4984/25000, steps: 94, e: 0.091\n",
      "accumulated_rewards_per_episode: 0.05999999999999948\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4985/25000, steps: 800, e: 0.091\n",
      "accumulated_rewards_per_episode: -5.999999999999896\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 4986/25000, steps: 52, e: 0.091\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4987/25000, steps: 52, e: 0.091\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4988/25000, steps: 543, e: 0.091\n",
      "accumulated_rewards_per_episode: -4.42999999999993\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4989/25000, steps: 52, e: 0.091\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4990/25000, steps: 52, e: 0.091\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 4991/25000, steps: 62, e: 0.091\n",
      "accumulated_rewards_per_episode: 0.3799999999999996\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 4992/25000, steps: 52, e: 0.091\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4993/25000, steps: 84, e: 0.091\n",
      "accumulated_rewards_per_episode: 0.15999999999999948\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4994/25000, steps: 52, e: 0.091\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4995/25000, steps: 785, e: 0.091\n",
      "accumulated_rewards_per_episode: -6.849999999999877\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4996/25000, steps: 52, e: 0.091\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4997/25000, steps: 52, e: 0.091\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4998/25000, steps: 800, e: 0.091\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 4999/25000, steps: 52, e: 0.091\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5000/25000, steps: 52, e: 0.091\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5001/25000, steps: 52, e: 0.091\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 5002/25000, steps: 52, e: 0.091\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 5003/25000, steps: 800, e: 0.091\n",
      "accumulated_rewards_per_episode: -5.999999999999874\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5004/25000, steps: 152, e: 0.091\n",
      "accumulated_rewards_per_episode: -0.5200000000000008\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5005/25000, steps: 187, e: 0.091\n",
      "accumulated_rewards_per_episode: -0.8700000000000011\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 5006/25000, steps: 52, e: 0.09\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 5007/25000, steps: 71, e: 0.09\n",
      "accumulated_rewards_per_episode: 0.2899999999999995\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5008/25000, steps: 52, e: 0.09\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5009/25000, steps: 52, e: 0.09\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 5010/25000, steps: 52, e: 0.09\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 5011/25000, steps: 773, e: 0.09\n",
      "accumulated_rewards_per_episode: -6.72999999999988\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 5012/25000, steps: 52, e: 0.09\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 5013/25000, steps: 52, e: 0.09\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 5014/25000, steps: 52, e: 0.09\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5015/25000, steps: 52, e: 0.09\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5016/25000, steps: 686, e: 0.09\n",
      "accumulated_rewards_per_episode: -5.859999999999898\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5017/25000, steps: 52, e: 0.09\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 2, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5018/25000, steps: 378, e: 0.09\n",
      "accumulated_rewards_per_episode: -3.7799999999999634\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5019/25000, steps: 52, e: 0.09\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5020/25000, steps: 209, e: 0.09\n",
      "accumulated_rewards_per_episode: -1.0900000000000014\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 5021/25000, steps: 52, e: 0.09\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 5022/25000, steps: 52, e: 0.09\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 5023/25000, steps: 800, e: 0.09\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5024/25000, steps: 800, e: 0.09\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 5025/25000, steps: 800, e: 0.09\n",
      "accumulated_rewards_per_episode: -5.999999999999918\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5026/25000, steps: 73, e: 0.09\n",
      "accumulated_rewards_per_episode: 0.2699999999999996\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 5027/25000, steps: 52, e: 0.09\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 5028/25000, steps: 800, e: 0.09\n",
      "accumulated_rewards_per_episode: -5.999999999999874\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5029/25000, steps: 800, e: 0.089\n",
      "accumulated_rewards_per_episode: -6.999999999999889\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5030/25000, steps: 507, e: 0.089\n",
      "accumulated_rewards_per_episode: -4.069999999999936\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 5031/25000, steps: 651, e: 0.089\n",
      "accumulated_rewards_per_episode: -5.509999999999906\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5032/25000, steps: 108, e: 0.089\n",
      "accumulated_rewards_per_episode: -0.08000000000000053\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 5033/25000, steps: 775, e: 0.089\n",
      "accumulated_rewards_per_episode: -6.749999999999879\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5034/25000, steps: 335, e: 0.089\n",
      "accumulated_rewards_per_episode: -1.3500000000000023\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 5035/25000, steps: 52, e: 0.089\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5036/25000, steps: 127, e: 0.089\n",
      "accumulated_rewards_per_episode: -1.270000000000001\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5037/25000, steps: 330, e: 0.089\n",
      "accumulated_rewards_per_episode: -2.299999999999996\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 5038/25000, steps: 800, e: 0.089\n",
      "accumulated_rewards_per_episode: -1.9999999999999931\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 5039/25000, steps: 52, e: 0.089\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 5040/25000, steps: 211, e: 0.089\n",
      "accumulated_rewards_per_episode: -0.11000000000000158\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5041/25000, steps: 302, e: 0.089\n",
      "accumulated_rewards_per_episode: -1.020000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 5042/25000, steps: 60, e: 0.089\n",
      "accumulated_rewards_per_episode: 0.3999999999999997\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5043/25000, steps: 52, e: 0.089\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 5044/25000, steps: 52, e: 0.089\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5045/25000, steps: 201, e: 0.089\n",
      "accumulated_rewards_per_episode: -1.0100000000000011\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 5046/25000, steps: 52, e: 0.089\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 5047/25000, steps: 800, e: 0.089\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5048/25000, steps: 52, e: 0.089\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 5049/25000, steps: 101, e: 0.089\n",
      "accumulated_rewards_per_episode: -1.0100000000000007\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 5050/25000, steps: 52, e: 0.089\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 5051/25000, steps: 702, e: 0.089\n",
      "accumulated_rewards_per_episode: -5.019999999999902\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 5052/25000, steps: 800, e: 0.088\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 5053/25000, steps: 118, e: 0.088\n",
      "accumulated_rewards_per_episode: -0.18000000000000077\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5054/25000, steps: 792, e: 0.088\n",
      "accumulated_rewards_per_episode: -0.919999999999932\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 5055/25000, steps: 52, e: 0.088\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 5056/25000, steps: 800, e: 0.088\n",
      "accumulated_rewards_per_episode: 0.9999999999999936\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5057/25000, steps: 349, e: 0.088\n",
      "accumulated_rewards_per_episode: 2.5100000000000278\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5058/25000, steps: 52, e: 0.088\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 5059/25000, steps: 800, e: 0.088\n",
      "accumulated_rewards_per_episode: -1.9999999999999278\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 5060/25000, steps: 261, e: 0.088\n",
      "accumulated_rewards_per_episode: -1.610000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 5061/25000, steps: 52, e: 0.088\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 5062/25000, steps: 52, e: 0.088\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 5063/25000, steps: 728, e: 0.088\n",
      "accumulated_rewards_per_episode: -3.279999999999921\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5064/25000, steps: 303, e: 0.088\n",
      "accumulated_rewards_per_episode: -2.0300000000000016\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5065/25000, steps: 758, e: 0.088\n",
      "accumulated_rewards_per_episode: -3.579999999999883\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5066/25000, steps: 52, e: 0.088\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5067/25000, steps: 55, e: 0.088\n",
      "accumulated_rewards_per_episode: 0.4499999999999996\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5068/25000, steps: 118, e: 0.088\n",
      "accumulated_rewards_per_episode: -0.18000000000000066\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 5069/25000, steps: 52, e: 0.088\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 5070/25000, steps: 800, e: 0.088\n",
      "accumulated_rewards_per_episode: -3.9999999999999614\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 5071/25000, steps: 70, e: 0.088\n",
      "accumulated_rewards_per_episode: 0.2999999999999995\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5072/25000, steps: 800, e: 0.088\n",
      "accumulated_rewards_per_episode: -3.9999999999999596\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5073/25000, steps: 800, e: 0.088\n",
      "accumulated_rewards_per_episode: -5.999999999999896\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5074/25000, steps: 800, e: 0.088\n",
      "accumulated_rewards_per_episode: -3.9999999999999623\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5075/25000, steps: 757, e: 0.088\n",
      "accumulated_rewards_per_episode: 17.429999999999964\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 5076/25000, steps: 359, e: 0.087\n",
      "accumulated_rewards_per_episode: -1.5899999999999956\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5077/25000, steps: 131, e: 0.087\n",
      "accumulated_rewards_per_episode: -1.310000000000001\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 5078/25000, steps: 52, e: 0.087\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 5079/25000, steps: 249, e: 0.087\n",
      "accumulated_rewards_per_episode: 0.5099999999999982\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 5080/25000, steps: 52, e: 0.087\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5081/25000, steps: 95, e: 0.087\n",
      "accumulated_rewards_per_episode: 2.0500000000000016\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 5082/25000, steps: 52, e: 0.087\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5083/25000, steps: 800, e: 0.087\n",
      "accumulated_rewards_per_episode: 6.000000000000125\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 5084/25000, steps: 401, e: 0.087\n",
      "accumulated_rewards_per_episode: -2.0100000000000025\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 5085/25000, steps: 123, e: 0.087\n",
      "accumulated_rewards_per_episode: -0.2300000000000007\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 5086/25000, steps: 800, e: 0.087\n",
      "accumulated_rewards_per_episode: 3.0000000000000417\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 5087/25000, steps: 52, e: 0.087\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5088/25000, steps: 800, e: 0.087\n",
      "accumulated_rewards_per_episode: -3.999999999999907\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5089/25000, steps: 448, e: 0.087\n",
      "accumulated_rewards_per_episode: 0.5199999999999974\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 5090/25000, steps: 52, e: 0.087\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 5091/25000, steps: 800, e: 0.087\n",
      "accumulated_rewards_per_episode: -4.999999999999874\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 5092/25000, steps: 800, e: 0.087\n",
      "accumulated_rewards_per_episode: 4.000000000000064\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5093/25000, steps: 800, e: 0.087\n",
      "accumulated_rewards_per_episode: -4.999999999999896\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5094/25000, steps: 52, e: 0.087\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5095/25000, steps: 480, e: 0.087\n",
      "accumulated_rewards_per_episode: -1.7999999999999654\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 5096/25000, steps: 473, e: 0.087\n",
      "accumulated_rewards_per_episode: -3.7299999999999436\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5097/25000, steps: 565, e: 0.087\n",
      "accumulated_rewards_per_episode: -1.6499999999999477\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 5098/25000, steps: 215, e: 0.087\n",
      "accumulated_rewards_per_episode: -0.1500000000000014\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5099/25000, steps: 165, e: 0.087\n",
      "accumulated_rewards_per_episode: -0.6500000000000012\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5100/25000, steps: 52, e: 0.086\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5101/25000, steps: 800, e: 0.086\n",
      "accumulated_rewards_per_episode: 1.000000000000023\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 5102/25000, steps: 117, e: 0.086\n",
      "accumulated_rewards_per_episode: -1.1700000000000008\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5103/25000, steps: 254, e: 0.086\n",
      "accumulated_rewards_per_episode: -2.53999999999999\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5104/25000, steps: 52, e: 0.086\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 5105/25000, steps: 85, e: 0.086\n",
      "accumulated_rewards_per_episode: 0.14999999999999925\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 5106/25000, steps: 52, e: 0.086\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5107/25000, steps: 800, e: 0.086\n",
      "accumulated_rewards_per_episode: -5.999999999999905\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 5108/25000, steps: 52, e: 0.086\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 5109/25000, steps: 510, e: 0.086\n",
      "accumulated_rewards_per_episode: 1.9000000000000532\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 5110/25000, steps: 86, e: 0.086\n",
      "accumulated_rewards_per_episode: 2.1400000000000077\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5111/25000, steps: 52, e: 0.086\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 5112/25000, steps: 124, e: 0.086\n",
      "accumulated_rewards_per_episode: 1.7599999999999991\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 5113/25000, steps: 52, e: 0.086\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 5114/25000, steps: 389, e: 0.086\n",
      "accumulated_rewards_per_episode: 0.11000000000000712\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 5115/25000, steps: 800, e: 0.086\n",
      "accumulated_rewards_per_episode: -2.999999999999929\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 5116/25000, steps: 458, e: 0.086\n",
      "accumulated_rewards_per_episode: -0.5800000000000027\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 5117/25000, steps: 77, e: 0.086\n",
      "accumulated_rewards_per_episode: 0.22999999999999954\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 5118/25000, steps: 52, e: 0.086\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 5119/25000, steps: 52, e: 0.086\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 5120/25000, steps: 52, e: 0.086\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 5121/25000, steps: 52, e: 0.086\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5122/25000, steps: 52, e: 0.086\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5123/25000, steps: 52, e: 0.086\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5124/25000, steps: 52, e: 0.085\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 5125/25000, steps: 52, e: 0.085\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5126/25000, steps: 97, e: 0.085\n",
      "accumulated_rewards_per_episode: 1.0299999999999994\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 5127/25000, steps: 52, e: 0.085\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5128/25000, steps: 62, e: 0.085\n",
      "accumulated_rewards_per_episode: 1.3799999999999994\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 5129/25000, steps: 52, e: 0.085\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 5130/25000, steps: 800, e: 0.085\n",
      "accumulated_rewards_per_episode: -0.9999999999999938\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5131/25000, steps: 800, e: 0.085\n",
      "accumulated_rewards_per_episode: 1.0000000000000977\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 5132/25000, steps: 313, e: 0.085\n",
      "accumulated_rewards_per_episode: 4.870000000000028\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5133/25000, steps: 398, e: 0.085\n",
      "accumulated_rewards_per_episode: 3.0200000000000653\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 5134/25000, steps: 56, e: 0.085\n",
      "accumulated_rewards_per_episode: 0.4399999999999997\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 5135/25000, steps: 69, e: 0.085\n",
      "accumulated_rewards_per_episode: -0.6900000000000004\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 5136/25000, steps: 52, e: 0.085\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5137/25000, steps: 52, e: 0.085\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5138/25000, steps: 52, e: 0.085\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5139/25000, steps: 52, e: 0.085\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5140/25000, steps: 53, e: 0.085\n",
      "accumulated_rewards_per_episode: 0.46999999999999964\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5141/25000, steps: 52, e: 0.085\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 5142/25000, steps: 531, e: 0.085\n",
      "accumulated_rewards_per_episode: -2.309999999999966\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 5143/25000, steps: 85, e: 0.085\n",
      "accumulated_rewards_per_episode: -0.8500000000000005\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5144/25000, steps: 52, e: 0.085\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 5145/25000, steps: 52, e: 0.085\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5146/25000, steps: 52, e: 0.085\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5147/25000, steps: 52, e: 0.085\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 5148/25000, steps: 52, e: 0.084\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5149/25000, steps: 219, e: 0.084\n",
      "accumulated_rewards_per_episode: -1.1900000000000013\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5150/25000, steps: 52, e: 0.084\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5151/25000, steps: 800, e: 0.084\n",
      "accumulated_rewards_per_episode: -0.999999999999991\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 5152/25000, steps: 52, e: 0.084\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 5153/25000, steps: 74, e: 0.084\n",
      "accumulated_rewards_per_episode: 0.25999999999999945\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5154/25000, steps: 800, e: 0.084\n",
      "accumulated_rewards_per_episode: 3.000000000000054\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 5155/25000, steps: 639, e: 0.084\n",
      "accumulated_rewards_per_episode: 0.6100000000000005\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 5156/25000, steps: 52, e: 0.084\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5157/25000, steps: 72, e: 0.084\n",
      "accumulated_rewards_per_episode: -0.7200000000000004\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 5158/25000, steps: 62, e: 0.084\n",
      "accumulated_rewards_per_episode: -0.6200000000000003\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5159/25000, steps: 228, e: 0.084\n",
      "accumulated_rewards_per_episode: -1.2800000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 5160/25000, steps: 77, e: 0.084\n",
      "accumulated_rewards_per_episode: -0.7700000000000005\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 5161/25000, steps: 52, e: 0.084\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5162/25000, steps: 52, e: 0.084\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5163/25000, steps: 52, e: 0.084\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5164/25000, steps: 52, e: 0.084\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 5165/25000, steps: 52, e: 0.084\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 5166/25000, steps: 52, e: 0.084\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5167/25000, steps: 58, e: 0.084\n",
      "accumulated_rewards_per_episode: 0.4199999999999997\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 5168/25000, steps: 55, e: 0.084\n",
      "accumulated_rewards_per_episode: -0.5500000000000003\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5169/25000, steps: 800, e: 0.084\n",
      "accumulated_rewards_per_episode: -4.999999999999896\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 5170/25000, steps: 75, e: 0.084\n",
      "accumulated_rewards_per_episode: 0.24999999999999933\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5171/25000, steps: 80, e: 0.084\n",
      "accumulated_rewards_per_episode: 0.1999999999999995\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 5172/25000, steps: 52, e: 0.084\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 5173/25000, steps: 52, e: 0.083\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5174/25000, steps: 800, e: 0.083\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5175/25000, steps: 52, e: 0.083\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5176/25000, steps: 635, e: 0.083\n",
      "accumulated_rewards_per_episode: 0.6500000000000498\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 5177/25000, steps: 331, e: 0.083\n",
      "accumulated_rewards_per_episode: -0.31000000000000244\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 5178/25000, steps: 52, e: 0.083\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5179/25000, steps: 52, e: 0.083\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5180/25000, steps: 104, e: 0.083\n",
      "accumulated_rewards_per_episode: -0.040000000000000535\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 5181/25000, steps: 55, e: 0.083\n",
      "accumulated_rewards_per_episode: 0.44999999999999973\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 5182/25000, steps: 52, e: 0.083\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 5183/25000, steps: 52, e: 0.083\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 5184/25000, steps: 363, e: 0.083\n",
      "accumulated_rewards_per_episode: -2.629999999999989\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5185/25000, steps: 52, e: 0.083\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5186/25000, steps: 162, e: 0.083\n",
      "accumulated_rewards_per_episode: -0.6200000000000011\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5187/25000, steps: 52, e: 0.083\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 5188/25000, steps: 330, e: 0.083\n",
      "accumulated_rewards_per_episode: 0.699999999999998\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5189/25000, steps: 579, e: 0.083\n",
      "accumulated_rewards_per_episode: -4.789999999999921\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 5190/25000, steps: 262, e: 0.083\n",
      "accumulated_rewards_per_episode: 0.3799999999999981\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 5191/25000, steps: 75, e: 0.083\n",
      "accumulated_rewards_per_episode: 0.24999999999999944\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 5192/25000, steps: 59, e: 0.083\n",
      "accumulated_rewards_per_episode: 0.4099999999999995\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 5193/25000, steps: 52, e: 0.083\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 5194/25000, steps: 52, e: 0.083\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 5195/25000, steps: 52, e: 0.083\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5196/25000, steps: 300, e: 0.083\n",
      "accumulated_rewards_per_episode: -2.99999999999998\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5197/25000, steps: 800, e: 0.083\n",
      "accumulated_rewards_per_episode: -0.9999999999999994\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 5198/25000, steps: 561, e: 0.082\n",
      "accumulated_rewards_per_episode: -4.609999999999947\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 5199/25000, steps: 52, e: 0.082\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 5200/25000, steps: 52, e: 0.082\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 5201/25000, steps: 330, e: 0.082\n",
      "accumulated_rewards_per_episode: -2.299999999999974\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 5202/25000, steps: 52, e: 0.082\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5203/25000, steps: 52, e: 0.082\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 5204/25000, steps: 76, e: 0.082\n",
      "accumulated_rewards_per_episode: 0.23999999999999955\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5205/25000, steps: 52, e: 0.082\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5206/25000, steps: 52, e: 0.082\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5207/25000, steps: 52, e: 0.082\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 5208/25000, steps: 510, e: 0.082\n",
      "accumulated_rewards_per_episode: -4.099999999999958\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5209/25000, steps: 214, e: 0.082\n",
      "accumulated_rewards_per_episode: -1.1400000000000015\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 5210/25000, steps: 52, e: 0.082\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5211/25000, steps: 800, e: 0.082\n",
      "accumulated_rewards_per_episode: -3.9999999999999556\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5212/25000, steps: 79, e: 0.082\n",
      "accumulated_rewards_per_episode: 0.20999999999999952\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 5213/25000, steps: 201, e: 0.082\n",
      "accumulated_rewards_per_episode: -1.0100000000000013\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5214/25000, steps: 234, e: 0.082\n",
      "accumulated_rewards_per_episode: 0.6599999999999981\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5215/25000, steps: 129, e: 0.082\n",
      "accumulated_rewards_per_episode: -0.29000000000000087\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 5216/25000, steps: 52, e: 0.082\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5217/25000, steps: 52, e: 0.082\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5218/25000, steps: 154, e: 0.082\n",
      "accumulated_rewards_per_episode: -1.5400000000000011\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5219/25000, steps: 52, e: 0.082\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 5220/25000, steps: 407, e: 0.082\n",
      "accumulated_rewards_per_episode: -4.069999999999958\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 5221/25000, steps: 235, e: 0.082\n",
      "accumulated_rewards_per_episode: -1.3500000000000016\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 5222/25000, steps: 52, e: 0.082\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5223/25000, steps: 123, e: 0.082\n",
      "accumulated_rewards_per_episode: -0.2300000000000007\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5224/25000, steps: 187, e: 0.081\n",
      "accumulated_rewards_per_episode: -0.8700000000000011\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5225/25000, steps: 52, e: 0.081\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5226/25000, steps: 530, e: 0.081\n",
      "accumulated_rewards_per_episode: -5.299999999999931\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 5227/25000, steps: 52, e: 0.081\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 5228/25000, steps: 152, e: 0.081\n",
      "accumulated_rewards_per_episode: -0.5200000000000009\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5229/25000, steps: 52, e: 0.081\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 5230/25000, steps: 354, e: 0.081\n",
      "accumulated_rewards_per_episode: -1.539999999999985\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5231/25000, steps: 52, e: 0.081\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 5232/25000, steps: 478, e: 0.081\n",
      "accumulated_rewards_per_episode: -1.7800000000000005\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5233/25000, steps: 230, e: 0.081\n",
      "accumulated_rewards_per_episode: -0.30000000000000177\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 5234/25000, steps: 52, e: 0.081\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5235/25000, steps: 88, e: 0.081\n",
      "accumulated_rewards_per_episode: 0.11999999999999923\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 5236/25000, steps: 800, e: 0.081\n",
      "accumulated_rewards_per_episode: -2.9999999999998765\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 5237/25000, steps: 458, e: 0.081\n",
      "accumulated_rewards_per_episode: -3.579999999999947\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5238/25000, steps: 69, e: 0.081\n",
      "accumulated_rewards_per_episode: 0.3099999999999995\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 5239/25000, steps: 52, e: 0.081\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 5240/25000, steps: 52, e: 0.081\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 5241/25000, steps: 83, e: 0.081\n",
      "accumulated_rewards_per_episode: 0.16999999999999948\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 5242/25000, steps: 52, e: 0.081\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5243/25000, steps: 52, e: 0.081\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5244/25000, steps: 52, e: 0.081\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5245/25000, steps: 52, e: 0.081\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 5246/25000, steps: 52, e: 0.081\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 5247/25000, steps: 55, e: 0.081\n",
      "accumulated_rewards_per_episode: 0.44999999999999973\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5248/25000, steps: 52, e: 0.081\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 5249/25000, steps: 52, e: 0.08\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 5250/25000, steps: 331, e: 0.08\n",
      "accumulated_rewards_per_episode: -1.3100000000000025\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 5251/25000, steps: 385, e: 0.08\n",
      "accumulated_rewards_per_episode: -2.849999999999984\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 5252/25000, steps: 60, e: 0.08\n",
      "accumulated_rewards_per_episode: 0.3999999999999997\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5253/25000, steps: 52, e: 0.08\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5254/25000, steps: 214, e: 0.08\n",
      "accumulated_rewards_per_episode: -0.1400000000000014\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5255/25000, steps: 310, e: 0.08\n",
      "accumulated_rewards_per_episode: -2.1\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5256/25000, steps: 52, e: 0.08\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 5257/25000, steps: 63, e: 0.08\n",
      "accumulated_rewards_per_episode: 2.370000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 5258/25000, steps: 181, e: 0.08\n",
      "accumulated_rewards_per_episode: -0.810000000000001\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 5259/25000, steps: 92, e: 0.08\n",
      "accumulated_rewards_per_episode: 0.07999999999999947\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 5260/25000, steps: 707, e: 0.08\n",
      "accumulated_rewards_per_episode: 3.9300000000000193\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5261/25000, steps: 52, e: 0.08\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5262/25000, steps: 52, e: 0.08\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 5263/25000, steps: 800, e: 0.08\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 5264/25000, steps: 321, e: 0.08\n",
      "accumulated_rewards_per_episode: -2.209999999999996\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 5265/25000, steps: 52, e: 0.08\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5266/25000, steps: 52, e: 0.08\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 5267/25000, steps: 800, e: 0.08\n",
      "accumulated_rewards_per_episode: -2.9999999999999063\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5268/25000, steps: 52, e: 0.08\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5269/25000, steps: 52, e: 0.08\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 5270/25000, steps: 555, e: 0.08\n",
      "accumulated_rewards_per_episode: 0.4500000000000046\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 5271/25000, steps: 171, e: 0.08\n",
      "accumulated_rewards_per_episode: -0.7100000000000013\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 5272/25000, steps: 52, e: 0.08\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5273/25000, steps: 52, e: 0.08\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 5274/25000, steps: 52, e: 0.08\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 5275/25000, steps: 53, e: 0.079\n",
      "accumulated_rewards_per_episode: 0.46999999999999953\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5276/25000, steps: 94, e: 0.079\n",
      "accumulated_rewards_per_episode: 2.060000000000003\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 5277/25000, steps: 394, e: 0.079\n",
      "accumulated_rewards_per_episode: -2.939999999999982\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 5278/25000, steps: 670, e: 0.079\n",
      "accumulated_rewards_per_episode: 2.3000000000001117\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5279/25000, steps: 781, e: 0.079\n",
      "accumulated_rewards_per_episode: -6.809999999999878\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 5280/25000, steps: 52, e: 0.079\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5281/25000, steps: 52, e: 0.079\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5282/25000, steps: 130, e: 0.079\n",
      "accumulated_rewards_per_episode: -0.30000000000000077\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 5283/25000, steps: 52, e: 0.079\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 5284/25000, steps: 55, e: 0.079\n",
      "accumulated_rewards_per_episode: 0.4499999999999996\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5285/25000, steps: 115, e: 0.079\n",
      "accumulated_rewards_per_episode: -0.15000000000000052\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5286/25000, steps: 52, e: 0.079\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 5287/25000, steps: 52, e: 0.079\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5288/25000, steps: 52, e: 0.079\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 5289/25000, steps: 101, e: 0.079\n",
      "accumulated_rewards_per_episode: -0.010000000000000545\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5290/25000, steps: 132, e: 0.079\n",
      "accumulated_rewards_per_episode: -0.32000000000000073\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5291/25000, steps: 800, e: 0.079\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5292/25000, steps: 493, e: 0.079\n",
      "accumulated_rewards_per_episode: 4.07000000000005\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5293/25000, steps: 150, e: 0.079\n",
      "accumulated_rewards_per_episode: -0.500000000000001\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5294/25000, steps: 52, e: 0.079\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5295/25000, steps: 90, e: 0.079\n",
      "accumulated_rewards_per_episode: -0.9000000000000006\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5296/25000, steps: 52, e: 0.079\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 5297/25000, steps: 800, e: 0.079\n",
      "accumulated_rewards_per_episode: -2.999999999999874\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 5298/25000, steps: 52, e: 0.079\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 5299/25000, steps: 242, e: 0.079\n",
      "accumulated_rewards_per_episode: -1.4200000000000017\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 5300/25000, steps: 52, e: 0.079\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 5301/25000, steps: 559, e: 0.079\n",
      "accumulated_rewards_per_episode: -4.589999999999947\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5302/25000, steps: 52, e: 0.078\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5303/25000, steps: 57, e: 0.078\n",
      "accumulated_rewards_per_episode: 0.4299999999999997\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5304/25000, steps: 64, e: 0.078\n",
      "accumulated_rewards_per_episode: 0.35999999999999943\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 5305/25000, steps: 52, e: 0.078\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 5306/25000, steps: 348, e: 0.078\n",
      "accumulated_rewards_per_episode: -2.479999999999992\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5307/25000, steps: 52, e: 0.078\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 5308/25000, steps: 286, e: 0.078\n",
      "accumulated_rewards_per_episode: -1.859999999999989\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 5309/25000, steps: 52, e: 0.078\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5310/25000, steps: 214, e: 0.078\n",
      "accumulated_rewards_per_episode: 0.8600000000000017\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5311/25000, steps: 52, e: 0.078\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5312/25000, steps: 282, e: 0.078\n",
      "accumulated_rewards_per_episode: -2.819999999999984\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 5313/25000, steps: 52, e: 0.078\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5314/25000, steps: 52, e: 0.078\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 5315/25000, steps: 476, e: 0.078\n",
      "accumulated_rewards_per_episode: -1.759999999999968\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5316/25000, steps: 144, e: 0.078\n",
      "accumulated_rewards_per_episode: -0.44000000000000106\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5317/25000, steps: 52, e: 0.078\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5318/25000, steps: 73, e: 0.078\n",
      "accumulated_rewards_per_episode: 0.2699999999999996\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5319/25000, steps: 52, e: 0.078\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5320/25000, steps: 52, e: 0.078\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 5321/25000, steps: 52, e: 0.078\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5322/25000, steps: 66, e: 0.078\n",
      "accumulated_rewards_per_episode: 0.3399999999999994\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 5323/25000, steps: 52, e: 0.078\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 5324/25000, steps: 78, e: 0.078\n",
      "accumulated_rewards_per_episode: 0.21999999999999953\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5325/25000, steps: 52, e: 0.078\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5326/25000, steps: 52, e: 0.078\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5327/25000, steps: 52, e: 0.078\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 5328/25000, steps: 52, e: 0.078\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5329/25000, steps: 52, e: 0.077\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 5330/25000, steps: 250, e: 0.077\n",
      "accumulated_rewards_per_episode: -2.4999999999999907\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 5331/25000, steps: 418, e: 0.077\n",
      "accumulated_rewards_per_episode: -3.179999999999977\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5332/25000, steps: 52, e: 0.077\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 5333/25000, steps: 800, e: 0.077\n",
      "accumulated_rewards_per_episode: -2.999999999999906\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5334/25000, steps: 52, e: 0.077\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 5335/25000, steps: 484, e: 0.077\n",
      "accumulated_rewards_per_episode: -2.8399999999999794\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5336/25000, steps: 52, e: 0.077\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5337/25000, steps: 52, e: 0.077\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 5338/25000, steps: 800, e: 0.077\n",
      "accumulated_rewards_per_episode: -1.9999999999998859\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 5339/25000, steps: 105, e: 0.077\n",
      "accumulated_rewards_per_episode: -0.05000000000000054\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5340/25000, steps: 383, e: 0.077\n",
      "accumulated_rewards_per_episode: -0.8300000000000027\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5341/25000, steps: 52, e: 0.077\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5342/25000, steps: 683, e: 0.077\n",
      "accumulated_rewards_per_episode: -3.829999999999899\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5343/25000, steps: 52, e: 0.077\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 5344/25000, steps: 55, e: 0.077\n",
      "accumulated_rewards_per_episode: 0.4499999999999996\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 5345/25000, steps: 52, e: 0.077\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 5346/25000, steps: 293, e: 0.077\n",
      "accumulated_rewards_per_episode: -1.929999999999984\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 5347/25000, steps: 52, e: 0.077\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 5348/25000, steps: 513, e: 0.077\n",
      "accumulated_rewards_per_episode: -4.129999999999957\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5349/25000, steps: 59, e: 0.077\n",
      "accumulated_rewards_per_episode: 0.4099999999999996\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 5350/25000, steps: 629, e: 0.077\n",
      "accumulated_rewards_per_episode: -4.28999999999991\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5351/25000, steps: 566, e: 0.077\n",
      "accumulated_rewards_per_episode: 0.34000000000005315\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 5352/25000, steps: 360, e: 0.077\n",
      "accumulated_rewards_per_episode: -2.599999999999975\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5353/25000, steps: 263, e: 0.077\n",
      "accumulated_rewards_per_episode: 1.3699999999999983\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 5354/25000, steps: 52, e: 0.077\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5355/25000, steps: 800, e: 0.077\n",
      "accumulated_rewards_per_episode: -5.999999999999916\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 5356/25000, steps: 800, e: 0.076\n",
      "accumulated_rewards_per_episode: -3.999999999999874\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 5357/25000, steps: 684, e: 0.076\n",
      "accumulated_rewards_per_episode: 0.16000000000003411\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 5358/25000, steps: 52, e: 0.076\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 5359/25000, steps: 52, e: 0.076\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5360/25000, steps: 405, e: 0.076\n",
      "accumulated_rewards_per_episode: 1.9499999999999968\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 5361/25000, steps: 52, e: 0.076\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5362/25000, steps: 52, e: 0.076\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 5363/25000, steps: 52, e: 0.076\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5364/25000, steps: 52, e: 0.076\n",
      "accumulated_rewards_per_episode: 1.4799999999999998\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5365/25000, steps: 52, e: 0.076\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 5366/25000, steps: 52, e: 0.076\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5367/25000, steps: 52, e: 0.076\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5368/25000, steps: 52, e: 0.076\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5369/25000, steps: 603, e: 0.076\n",
      "accumulated_rewards_per_episode: -6.029999999999916\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 5370/25000, steps: 52, e: 0.076\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 5371/25000, steps: 103, e: 0.076\n",
      "accumulated_rewards_per_episode: -1.0300000000000007\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 5372/25000, steps: 403, e: 0.076\n",
      "accumulated_rewards_per_episode: -3.0299999999999585\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5373/25000, steps: 800, e: 0.076\n",
      "accumulated_rewards_per_episode: -5.999999999999874\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 5374/25000, steps: 617, e: 0.076\n",
      "accumulated_rewards_per_episode: -3.1699999999999493\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5375/25000, steps: 52, e: 0.076\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 5376/25000, steps: 800, e: 0.076\n",
      "accumulated_rewards_per_episode: -3.999999999999874\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5377/25000, steps: 72, e: 0.076\n",
      "accumulated_rewards_per_episode: -0.7200000000000004\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 5378/25000, steps: 52, e: 0.076\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5379/25000, steps: 624, e: 0.076\n",
      "accumulated_rewards_per_episode: 1.7599999999999953\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 5380/25000, steps: 52, e: 0.076\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5381/25000, steps: 52, e: 0.076\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 5382/25000, steps: 755, e: 0.076\n",
      "accumulated_rewards_per_episode: 1.4500000000000306\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 5383/25000, steps: 72, e: 0.075\n",
      "accumulated_rewards_per_episode: 0.2799999999999996\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 5384/25000, steps: 59, e: 0.075\n",
      "accumulated_rewards_per_episode: 0.4099999999999996\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5385/25000, steps: 52, e: 0.075\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 5386/25000, steps: 77, e: 0.075\n",
      "accumulated_rewards_per_episode: 0.22999999999999954\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5387/25000, steps: 52, e: 0.075\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 5388/25000, steps: 52, e: 0.075\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5389/25000, steps: 141, e: 0.075\n",
      "accumulated_rewards_per_episode: 1.589999999999999\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 5390/25000, steps: 52, e: 0.075\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 5391/25000, steps: 52, e: 0.075\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5392/25000, steps: 685, e: 0.075\n",
      "accumulated_rewards_per_episode: -1.8499999999999495\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5393/25000, steps: 53, e: 0.075\n",
      "accumulated_rewards_per_episode: 0.46999999999999964\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5394/25000, steps: 530, e: 0.075\n",
      "accumulated_rewards_per_episode: -4.299999999999931\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5395/25000, steps: 100, e: 0.075\n",
      "accumulated_rewards_per_episode: -1.0000000000000007\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 5396/25000, steps: 52, e: 0.075\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 5397/25000, steps: 52, e: 0.075\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5398/25000, steps: 106, e: 0.075\n",
      "accumulated_rewards_per_episode: -0.06000000000000054\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5399/25000, steps: 742, e: 0.075\n",
      "accumulated_rewards_per_episode: 1.580000000000131\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5400/25000, steps: 490, e: 0.075\n",
      "accumulated_rewards_per_episode: 4.100000000000013\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5401/25000, steps: 52, e: 0.075\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5402/25000, steps: 52, e: 0.075\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5403/25000, steps: 52, e: 0.075\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 5404/25000, steps: 52, e: 0.075\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5405/25000, steps: 52, e: 0.075\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5406/25000, steps: 52, e: 0.075\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 5407/25000, steps: 800, e: 0.075\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 5408/25000, steps: 214, e: 0.075\n",
      "accumulated_rewards_per_episode: 0.8599999999999985\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5409/25000, steps: 58, e: 0.075\n",
      "accumulated_rewards_per_episode: 0.4199999999999997\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 5410/25000, steps: 303, e: 0.075\n",
      "accumulated_rewards_per_episode: -1.030000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 5411/25000, steps: 52, e: 0.074\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5412/25000, steps: 52, e: 0.074\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5413/25000, steps: 52, e: 0.074\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 5414/25000, steps: 453, e: 0.074\n",
      "accumulated_rewards_per_episode: -3.529999999999969\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 5415/25000, steps: 68, e: 0.074\n",
      "accumulated_rewards_per_episode: 0.3199999999999994\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5416/25000, steps: 800, e: 0.074\n",
      "accumulated_rewards_per_episode: -4.999999999999896\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5417/25000, steps: 259, e: 0.074\n",
      "accumulated_rewards_per_episode: -0.5900000000000016\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5418/25000, steps: 52, e: 0.074\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 5419/25000, steps: 613, e: 0.074\n",
      "accumulated_rewards_per_episode: -2.129999999999936\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5420/25000, steps: 800, e: 0.074\n",
      "accumulated_rewards_per_episode: 1.8342966034978758e-14\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5421/25000, steps: 548, e: 0.074\n",
      "accumulated_rewards_per_episode: -4.47999999999995\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5422/25000, steps: 738, e: 0.074\n",
      "accumulated_rewards_per_episode: -3.379999999999887\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 5423/25000, steps: 52, e: 0.074\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 5424/25000, steps: 52, e: 0.074\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 5425/25000, steps: 52, e: 0.074\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5426/25000, steps: 52, e: 0.074\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5427/25000, steps: 800, e: 0.074\n",
      "accumulated_rewards_per_episode: 1.7104373473131318e-15\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 5428/25000, steps: 52, e: 0.074\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 5429/25000, steps: 800, e: 0.074\n",
      "accumulated_rewards_per_episode: -1.000000000000005\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 5430/25000, steps: 759, e: 0.074\n",
      "accumulated_rewards_per_episode: -5.589999999999904\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 5431/25000, steps: 800, e: 0.074\n",
      "accumulated_rewards_per_episode: -4.999999999999918\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5432/25000, steps: 102, e: 0.074\n",
      "accumulated_rewards_per_episode: -1.0200000000000007\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 5433/25000, steps: 52, e: 0.074\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5434/25000, steps: 347, e: 0.074\n",
      "accumulated_rewards_per_episode: 7.530000000000051\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 5435/25000, steps: 52, e: 0.074\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 5436/25000, steps: 52, e: 0.074\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 5437/25000, steps: 569, e: 0.074\n",
      "accumulated_rewards_per_episode: -3.689999999999963\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5438/25000, steps: 52, e: 0.074\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5439/25000, steps: 52, e: 0.073\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 5440/25000, steps: 52, e: 0.073\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5441/25000, steps: 140, e: 0.073\n",
      "accumulated_rewards_per_episode: -0.40000000000000085\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5442/25000, steps: 371, e: 0.073\n",
      "accumulated_rewards_per_episode: -0.7100000000000026\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5443/25000, steps: 61, e: 0.073\n",
      "accumulated_rewards_per_episode: 1.3899999999999997\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 5444/25000, steps: 52, e: 0.073\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5445/25000, steps: 52, e: 0.073\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5446/25000, steps: 52, e: 0.073\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5447/25000, steps: 78, e: 0.073\n",
      "accumulated_rewards_per_episode: 0.21999999999999953\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5448/25000, steps: 52, e: 0.073\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 5449/25000, steps: 174, e: 0.073\n",
      "accumulated_rewards_per_episode: -1.7400000000000013\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5450/25000, steps: 800, e: 0.073\n",
      "accumulated_rewards_per_episode: -4.999999999999917\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 5451/25000, steps: 52, e: 0.073\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 5452/25000, steps: 52, e: 0.073\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5453/25000, steps: 800, e: 0.073\n",
      "accumulated_rewards_per_episode: -3.999999999999962\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5454/25000, steps: 470, e: 0.073\n",
      "accumulated_rewards_per_episode: -3.699999999999966\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5455/25000, steps: 431, e: 0.073\n",
      "accumulated_rewards_per_episode: 4.690000000000071\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5456/25000, steps: 52, e: 0.073\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5457/25000, steps: 52, e: 0.073\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 5458/25000, steps: 52, e: 0.073\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5459/25000, steps: 195, e: 0.073\n",
      "accumulated_rewards_per_episode: -0.9500000000000015\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5460/25000, steps: 52, e: 0.073\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 5461/25000, steps: 800, e: 0.073\n",
      "accumulated_rewards_per_episode: 1.778785452266618e-14\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 5462/25000, steps: 116, e: 0.073\n",
      "accumulated_rewards_per_episode: -0.1600000000000006\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5463/25000, steps: 71, e: 0.073\n",
      "accumulated_rewards_per_episode: 0.2899999999999996\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5464/25000, steps: 52, e: 0.073\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5465/25000, steps: 52, e: 0.073\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5466/25000, steps: 52, e: 0.073\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 5467/25000, steps: 52, e: 0.073\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5468/25000, steps: 118, e: 0.072\n",
      "accumulated_rewards_per_episode: 0.8199999999999994\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 5469/25000, steps: 764, e: 0.072\n",
      "accumulated_rewards_per_episode: -4.6399999999999455\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5470/25000, steps: 159, e: 0.072\n",
      "accumulated_rewards_per_episode: -0.5900000000000011\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5471/25000, steps: 52, e: 0.072\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 5472/25000, steps: 580, e: 0.072\n",
      "accumulated_rewards_per_episode: 1.2000000000000264\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5473/25000, steps: 76, e: 0.072\n",
      "accumulated_rewards_per_episode: 0.23999999999999944\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5474/25000, steps: 269, e: 0.072\n",
      "accumulated_rewards_per_episode: 1.3099999999999983\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 5475/25000, steps: 800, e: 0.072\n",
      "accumulated_rewards_per_episode: 2.00000000000006\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 5476/25000, steps: 481, e: 0.072\n",
      "accumulated_rewards_per_episode: 1.1900000000000552\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 5477/25000, steps: 355, e: 0.072\n",
      "accumulated_rewards_per_episode: 7.450000000000044\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5478/25000, steps: 195, e: 0.072\n",
      "accumulated_rewards_per_episode: 0.04999999999999859\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 5479/25000, steps: 52, e: 0.072\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 5480/25000, steps: 800, e: 0.072\n",
      "accumulated_rewards_per_episode: 2.000000000000026\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 5481/25000, steps: 52, e: 0.072\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 5482/25000, steps: 52, e: 0.072\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5483/25000, steps: 392, e: 0.072\n",
      "accumulated_rewards_per_episode: -2.919999999999961\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 5484/25000, steps: 52, e: 0.072\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 5485/25000, steps: 52, e: 0.072\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5486/25000, steps: 658, e: 0.072\n",
      "accumulated_rewards_per_episode: 0.4199999999999968\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5487/25000, steps: 52, e: 0.072\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 5488/25000, steps: 52, e: 0.072\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5489/25000, steps: 342, e: 0.072\n",
      "accumulated_rewards_per_episode: -2.4199999999999715\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 5490/25000, steps: 52, e: 0.072\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5491/25000, steps: 372, e: 0.072\n",
      "accumulated_rewards_per_episode: -2.719999999999965\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 5492/25000, steps: 52, e: 0.072\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 5493/25000, steps: 52, e: 0.072\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 5494/25000, steps: 55, e: 0.072\n",
      "accumulated_rewards_per_episode: 0.44999999999999973\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5495/25000, steps: 52, e: 0.072\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5496/25000, steps: 52, e: 0.071\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 5497/25000, steps: 52, e: 0.071\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5498/25000, steps: 52, e: 0.071\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5499/25000, steps: 73, e: 0.071\n",
      "accumulated_rewards_per_episode: -0.7300000000000004\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5500/25000, steps: 52, e: 0.071\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5501/25000, steps: 52, e: 0.071\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 5502/25000, steps: 426, e: 0.071\n",
      "accumulated_rewards_per_episode: -1.2599999999999907\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 5503/25000, steps: 52, e: 0.071\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5504/25000, steps: 52, e: 0.071\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5505/25000, steps: 52, e: 0.071\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 5506/25000, steps: 52, e: 0.071\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5507/25000, steps: 186, e: 0.071\n",
      "accumulated_rewards_per_episode: -1.8600000000000014\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 5508/25000, steps: 53, e: 0.071\n",
      "accumulated_rewards_per_episode: 0.46999999999999964\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5509/25000, steps: 486, e: 0.071\n",
      "accumulated_rewards_per_episode: -1.8599999999999628\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5510/25000, steps: 800, e: 0.071\n",
      "accumulated_rewards_per_episode: -3.9999999999999236\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 5511/25000, steps: 800, e: 0.071\n",
      "accumulated_rewards_per_episode: 1.999999999999994\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 5512/25000, steps: 230, e: 0.071\n",
      "accumulated_rewards_per_episode: 2.700000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5513/25000, steps: 52, e: 0.071\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5514/25000, steps: 52, e: 0.071\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 5515/25000, steps: 52, e: 0.071\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 5516/25000, steps: 52, e: 0.071\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 5517/25000, steps: 52, e: 0.071\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 5518/25000, steps: 109, e: 0.071\n",
      "accumulated_rewards_per_episode: -0.09000000000000054\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5519/25000, steps: 461, e: 0.071\n",
      "accumulated_rewards_per_episode: 0.38999999999999657\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 5520/25000, steps: 232, e: 0.071\n",
      "accumulated_rewards_per_episode: -1.3200000000000018\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5521/25000, steps: 69, e: 0.071\n",
      "accumulated_rewards_per_episode: 0.3099999999999995\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5522/25000, steps: 67, e: 0.071\n",
      "accumulated_rewards_per_episode: 0.3299999999999994\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 5523/25000, steps: 52, e: 0.071\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5524/25000, steps: 52, e: 0.071\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 5525/25000, steps: 52, e: 0.071\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5526/25000, steps: 52, e: 0.07\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 5527/25000, steps: 402, e: 0.07\n",
      "accumulated_rewards_per_episode: -1.0199999999999656\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 5528/25000, steps: 52, e: 0.07\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5529/25000, steps: 52, e: 0.07\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5530/25000, steps: 52, e: 0.07\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5531/25000, steps: 52, e: 0.07\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 5532/25000, steps: 52, e: 0.07\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5533/25000, steps: 52, e: 0.07\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 5534/25000, steps: 123, e: 0.07\n",
      "accumulated_rewards_per_episode: -1.2300000000000009\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 5535/25000, steps: 52, e: 0.07\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 5536/25000, steps: 52, e: 0.07\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 5537/25000, steps: 774, e: 0.07\n",
      "accumulated_rewards_per_episode: -4.739999999999946\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5538/25000, steps: 52, e: 0.07\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 5539/25000, steps: 52, e: 0.07\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 5540/25000, steps: 67, e: 0.07\n",
      "accumulated_rewards_per_episode: 0.3299999999999996\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5541/25000, steps: 64, e: 0.07\n",
      "accumulated_rewards_per_episode: 0.35999999999999954\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 5542/25000, steps: 52, e: 0.07\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5543/25000, steps: 108, e: 0.07\n",
      "accumulated_rewards_per_episode: -1.0800000000000007\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 5544/25000, steps: 52, e: 0.07\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 5545/25000, steps: 52, e: 0.07\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5546/25000, steps: 616, e: 0.07\n",
      "accumulated_rewards_per_episode: 0.8399999999999952\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 5547/25000, steps: 91, e: 0.07\n",
      "accumulated_rewards_per_episode: 0.08999999999999947\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 5548/25000, steps: 162, e: 0.07\n",
      "accumulated_rewards_per_episode: -0.6200000000000012\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5549/25000, steps: 52, e: 0.07\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 5550/25000, steps: 52, e: 0.07\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5551/25000, steps: 73, e: 0.07\n",
      "accumulated_rewards_per_episode: 0.2699999999999996\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 5552/25000, steps: 89, e: 0.07\n",
      "accumulated_rewards_per_episode: -0.8900000000000006\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5553/25000, steps: 382, e: 0.07\n",
      "accumulated_rewards_per_episode: -2.819999999999963\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 5554/25000, steps: 665, e: 0.07\n",
      "accumulated_rewards_per_episode: 4.350000000000058\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 5555/25000, steps: 155, e: 0.07\n",
      "accumulated_rewards_per_episode: -0.550000000000001\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 5556/25000, steps: 324, e: 0.069\n",
      "accumulated_rewards_per_episode: -0.24000000000000218\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5557/25000, steps: 275, e: 0.069\n",
      "accumulated_rewards_per_episode: -1.7499999999999878\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5558/25000, steps: 52, e: 0.069\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 5559/25000, steps: 99, e: 0.069\n",
      "accumulated_rewards_per_episode: 0.00999999999999947\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 5560/25000, steps: 575, e: 0.069\n",
      "accumulated_rewards_per_episode: 1.2500000000000118\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5561/25000, steps: 52, e: 0.069\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 5562/25000, steps: 52, e: 0.069\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 5563/25000, steps: 52, e: 0.069\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 5564/25000, steps: 52, e: 0.069\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5565/25000, steps: 199, e: 0.069\n",
      "accumulated_rewards_per_episode: -0.9900000000000015\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5566/25000, steps: 52, e: 0.069\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5567/25000, steps: 52, e: 0.069\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5568/25000, steps: 52, e: 0.069\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 5569/25000, steps: 85, e: 0.069\n",
      "accumulated_rewards_per_episode: -0.8500000000000005\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 5570/25000, steps: 52, e: 0.069\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5571/25000, steps: 69, e: 0.069\n",
      "accumulated_rewards_per_episode: -0.6900000000000004\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5572/25000, steps: 788, e: 0.069\n",
      "accumulated_rewards_per_episode: -6.879999999999876\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 5573/25000, steps: 52, e: 0.069\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5574/25000, steps: 800, e: 0.069\n",
      "accumulated_rewards_per_episode: -4.999999999999874\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5575/25000, steps: 288, e: 0.069\n",
      "accumulated_rewards_per_episode: -0.8800000000000022\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 5576/25000, steps: 84, e: 0.069\n",
      "accumulated_rewards_per_episode: 0.15999999999999936\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5577/25000, steps: 52, e: 0.069\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5578/25000, steps: 52, e: 0.069\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 5579/25000, steps: 52, e: 0.069\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5580/25000, steps: 714, e: 0.069\n",
      "accumulated_rewards_per_episode: -3.139999999999901\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5581/25000, steps: 52, e: 0.069\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5582/25000, steps: 82, e: 0.069\n",
      "accumulated_rewards_per_episode: -0.8200000000000005\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5583/25000, steps: 52, e: 0.069\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5584/25000, steps: 74, e: 0.069\n",
      "accumulated_rewards_per_episode: -0.7400000000000004\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 5585/25000, steps: 52, e: 0.069\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5586/25000, steps: 52, e: 0.068\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 5587/25000, steps: 589, e: 0.068\n",
      "accumulated_rewards_per_episode: -5.889999999999919\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 5588/25000, steps: 52, e: 0.068\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 5589/25000, steps: 650, e: 0.068\n",
      "accumulated_rewards_per_episode: -5.499999999999921\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 5590/25000, steps: 52, e: 0.068\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5591/25000, steps: 52, e: 0.068\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5592/25000, steps: 800, e: 0.068\n",
      "accumulated_rewards_per_episode: -5.999999999999917\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 5593/25000, steps: 800, e: 0.068\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5594/25000, steps: 356, e: 0.068\n",
      "accumulated_rewards_per_episode: -3.559999999999968\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5595/25000, steps: 204, e: 0.068\n",
      "accumulated_rewards_per_episode: -2.0400000000000005\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5596/25000, steps: 52, e: 0.068\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 5597/25000, steps: 52, e: 0.068\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5598/25000, steps: 310, e: 0.068\n",
      "accumulated_rewards_per_episode: -1.1000000000000025\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 5599/25000, steps: 52, e: 0.068\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5600/25000, steps: 587, e: 0.068\n",
      "accumulated_rewards_per_episode: -5.869999999999919\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 5601/25000, steps: 52, e: 0.068\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5602/25000, steps: 52, e: 0.068\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5603/25000, steps: 502, e: 0.068\n",
      "accumulated_rewards_per_episode: -2.0199999999999845\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 5604/25000, steps: 800, e: 0.068\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5605/25000, steps: 800, e: 0.068\n",
      "accumulated_rewards_per_episode: -6.999999999999896\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5606/25000, steps: 421, e: 0.068\n",
      "accumulated_rewards_per_episode: -4.209999999999955\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5607/25000, steps: 329, e: 0.068\n",
      "accumulated_rewards_per_episode: -3.289999999999974\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 5608/25000, steps: 800, e: 0.068\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 5609/25000, steps: 800, e: 0.068\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5610/25000, steps: 52, e: 0.068\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5611/25000, steps: 58, e: 0.068\n",
      "accumulated_rewards_per_episode: 0.4199999999999996\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 5612/25000, steps: 393, e: 0.068\n",
      "accumulated_rewards_per_episode: -2.9299999999999606\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5613/25000, steps: 52, e: 0.068\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 5614/25000, steps: 52, e: 0.068\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5615/25000, steps: 800, e: 0.068\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 5616/25000, steps: 52, e: 0.067\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5617/25000, steps: 52, e: 0.067\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5618/25000, steps: 403, e: 0.067\n",
      "accumulated_rewards_per_episode: -4.0299999999999585\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 5619/25000, steps: 186, e: 0.067\n",
      "accumulated_rewards_per_episode: -0.8600000000000014\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5620/25000, steps: 52, e: 0.067\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 5621/25000, steps: 800, e: 0.067\n",
      "accumulated_rewards_per_episode: -5.999999999999917\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5622/25000, steps: 52, e: 0.067\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5623/25000, steps: 800, e: 0.067\n",
      "accumulated_rewards_per_episode: -5.999999999999917\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5624/25000, steps: 800, e: 0.067\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 5625/25000, steps: 52, e: 0.067\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 5626/25000, steps: 800, e: 0.067\n",
      "accumulated_rewards_per_episode: -5.999999999999874\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 5627/25000, steps: 360, e: 0.067\n",
      "accumulated_rewards_per_episode: -1.6000000000000023\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 5628/25000, steps: 52, e: 0.067\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 5629/25000, steps: 800, e: 0.067\n",
      "accumulated_rewards_per_episode: -5.999999999999874\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 5630/25000, steps: 800, e: 0.067\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 5631/25000, steps: 398, e: 0.067\n",
      "accumulated_rewards_per_episode: 0.019999999999996826\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 5632/25000, steps: 397, e: 0.067\n",
      "accumulated_rewards_per_episode: -1.970000000000003\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 5633/25000, steps: 52, e: 0.067\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5634/25000, steps: 130, e: 0.067\n",
      "accumulated_rewards_per_episode: -0.30000000000000077\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5635/25000, steps: 52, e: 0.067\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5636/25000, steps: 800, e: 0.067\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5637/25000, steps: 52, e: 0.067\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5638/25000, steps: 52, e: 0.067\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5639/25000, steps: 52, e: 0.067\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5640/25000, steps: 800, e: 0.067\n",
      "accumulated_rewards_per_episode: -4.999999999999917\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5641/25000, steps: 800, e: 0.067\n",
      "accumulated_rewards_per_episode: -3.9999999999999614\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 5642/25000, steps: 52, e: 0.067\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5643/25000, steps: 800, e: 0.067\n",
      "accumulated_rewards_per_episode: -5.999999999999918\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5644/25000, steps: 800, e: 0.067\n",
      "accumulated_rewards_per_episode: -1.9999999999999742\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 5645/25000, steps: 800, e: 0.067\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5646/25000, steps: 800, e: 0.067\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 5647/25000, steps: 800, e: 0.066\n",
      "accumulated_rewards_per_episode: -5.999999999999918\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 5648/25000, steps: 52, e: 0.066\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 5649/25000, steps: 52, e: 0.066\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5650/25000, steps: 52, e: 0.066\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 5651/25000, steps: 800, e: 0.066\n",
      "accumulated_rewards_per_episode: -2.0000000000000036\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5652/25000, steps: 800, e: 0.066\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 5653/25000, steps: 800, e: 0.066\n",
      "accumulated_rewards_per_episode: -2.000000000000006\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5654/25000, steps: 221, e: 0.066\n",
      "accumulated_rewards_per_episode: -2.209999999999997\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5655/25000, steps: 800, e: 0.066\n",
      "accumulated_rewards_per_episode: -4.999999999999917\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 5656/25000, steps: 800, e: 0.066\n",
      "accumulated_rewards_per_episode: -3.9999999999999343\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5657/25000, steps: 764, e: 0.066\n",
      "accumulated_rewards_per_episode: -3.639999999999924\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5658/25000, steps: 96, e: 0.066\n",
      "accumulated_rewards_per_episode: 1.0399999999999994\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5659/25000, steps: 800, e: 0.066\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5660/25000, steps: 800, e: 0.066\n",
      "accumulated_rewards_per_episode: -5.999999999999874\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5661/25000, steps: 52, e: 0.066\n",
      "accumulated_rewards_per_episode: 1.4799999999999998\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 5662/25000, steps: 800, e: 0.066\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5663/25000, steps: 800, e: 0.066\n",
      "accumulated_rewards_per_episode: -1.999999999999941\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 5664/25000, steps: 52, e: 0.066\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5665/25000, steps: 800, e: 0.066\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 5666/25000, steps: 52, e: 0.066\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5667/25000, steps: 166, e: 0.066\n",
      "accumulated_rewards_per_episode: -1.6600000000000013\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5668/25000, steps: 800, e: 0.066\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5669/25000, steps: 52, e: 0.066\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5670/25000, steps: 52, e: 0.066\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 5671/25000, steps: 800, e: 0.066\n",
      "accumulated_rewards_per_episode: -4.999999999999874\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5672/25000, steps: 560, e: 0.066\n",
      "accumulated_rewards_per_episode: 4.4000000000000385\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5673/25000, steps: 350, e: 0.066\n",
      "accumulated_rewards_per_episode: -1.5000000000000024\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5674/25000, steps: 800, e: 0.066\n",
      "accumulated_rewards_per_episode: -2.999999999999969\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5675/25000, steps: 700, e: 0.066\n",
      "accumulated_rewards_per_episode: -4.99999999999994\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 5676/25000, steps: 154, e: 0.066\n",
      "accumulated_rewards_per_episode: -1.5400000000000011\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 5677/25000, steps: 52, e: 0.066\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 5678/25000, steps: 800, e: 0.066\n",
      "accumulated_rewards_per_episode: -5.999999999999874\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5679/25000, steps: 800, e: 0.065\n",
      "accumulated_rewards_per_episode: -2.9999999999999183\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5680/25000, steps: 800, e: 0.065\n",
      "accumulated_rewards_per_episode: 2.0000000000000537\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5681/25000, steps: 800, e: 0.065\n",
      "accumulated_rewards_per_episode: -3.999999999999949\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 5682/25000, steps: 52, e: 0.065\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5683/25000, steps: 800, e: 0.065\n",
      "accumulated_rewards_per_episode: 4.000000000000091\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5684/25000, steps: 800, e: 0.065\n",
      "accumulated_rewards_per_episode: -4.99999999999994\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5685/25000, steps: 52, e: 0.065\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 5686/25000, steps: 800, e: 0.065\n",
      "accumulated_rewards_per_episode: -5.999999999999917\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 5687/25000, steps: 52, e: 0.065\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 5688/25000, steps: 800, e: 0.065\n",
      "accumulated_rewards_per_episode: -5.999999999999917\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 5689/25000, steps: 800, e: 0.065\n",
      "accumulated_rewards_per_episode: -5.999999999999874\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 5690/25000, steps: 800, e: 0.065\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 5691/25000, steps: 800, e: 0.065\n",
      "accumulated_rewards_per_episode: -2.999999999999895\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 5692/25000, steps: 800, e: 0.065\n",
      "accumulated_rewards_per_episode: -3.9999999999999623\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5693/25000, steps: 66, e: 0.065\n",
      "accumulated_rewards_per_episode: 0.3399999999999994\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5694/25000, steps: 52, e: 0.065\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 5695/25000, steps: 52, e: 0.065\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5696/25000, steps: 800, e: 0.065\n",
      "accumulated_rewards_per_episode: -0.999999999999927\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5697/25000, steps: 800, e: 0.065\n",
      "accumulated_rewards_per_episode: -5.999999999999917\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 5698/25000, steps: 800, e: 0.065\n",
      "accumulated_rewards_per_episode: -3.9999999999999454\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 5699/25000, steps: 80, e: 0.065\n",
      "accumulated_rewards_per_episode: -0.8000000000000005\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 5700/25000, steps: 184, e: 0.065\n",
      "accumulated_rewards_per_episode: -1.8400000000000014\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 5701/25000, steps: 800, e: 0.065\n",
      "accumulated_rewards_per_episode: 4.000000000000085\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5702/25000, steps: 800, e: 0.065\n",
      "accumulated_rewards_per_episode: -5.999999999999917\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 5703/25000, steps: 558, e: 0.065\n",
      "accumulated_rewards_per_episode: -2.579999999999991\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 5704/25000, steps: 52, e: 0.065\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5705/25000, steps: 52, e: 0.065\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 5706/25000, steps: 67, e: 0.065\n",
      "accumulated_rewards_per_episode: 0.3299999999999995\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5707/25000, steps: 52, e: 0.065\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5708/25000, steps: 52, e: 0.065\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5709/25000, steps: 52, e: 0.065\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5710/25000, steps: 175, e: 0.065\n",
      "accumulated_rewards_per_episode: -1.7500000000000013\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5711/25000, steps: 52, e: 0.064\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5712/25000, steps: 800, e: 0.064\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 5713/25000, steps: 265, e: 0.064\n",
      "accumulated_rewards_per_episode: -1.650000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5714/25000, steps: 800, e: 0.064\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 5715/25000, steps: 800, e: 0.064\n",
      "accumulated_rewards_per_episode: 4.000000000000075\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 5716/25000, steps: 52, e: 0.064\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5717/25000, steps: 447, e: 0.064\n",
      "accumulated_rewards_per_episode: -2.469999999999993\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 5718/25000, steps: 105, e: 0.064\n",
      "accumulated_rewards_per_episode: 0.9499999999999991\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5719/25000, steps: 294, e: 0.064\n",
      "accumulated_rewards_per_episode: 0.05999999999999781\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5720/25000, steps: 52, e: 0.064\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 5721/25000, steps: 52, e: 0.064\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 5722/25000, steps: 52, e: 0.064\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5723/25000, steps: 52, e: 0.064\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 5724/25000, steps: 52, e: 0.064\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5725/25000, steps: 800, e: 0.064\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 5726/25000, steps: 203, e: 0.064\n",
      "accumulated_rewards_per_episode: -1.0300000000000014\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5727/25000, steps: 800, e: 0.064\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5728/25000, steps: 406, e: 0.064\n",
      "accumulated_rewards_per_episode: -4.059999999999958\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5729/25000, steps: 52, e: 0.064\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 5730/25000, steps: 91, e: 0.064\n",
      "accumulated_rewards_per_episode: -0.9100000000000006\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5731/25000, steps: 68, e: 0.064\n",
      "accumulated_rewards_per_episode: 0.3199999999999995\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5732/25000, steps: 52, e: 0.064\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 5733/25000, steps: 800, e: 0.064\n",
      "accumulated_rewards_per_episode: -4.99999999999992\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5734/25000, steps: 299, e: 0.064\n",
      "accumulated_rewards_per_episode: -0.9900000000000022\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 5735/25000, steps: 630, e: 0.064\n",
      "accumulated_rewards_per_episode: -2.299999999999976\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5736/25000, steps: 52, e: 0.064\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 5737/25000, steps: 800, e: 0.064\n",
      "accumulated_rewards_per_episode: -2.9999999999999796\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5738/25000, steps: 52, e: 0.064\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 5739/25000, steps: 52, e: 0.064\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5740/25000, steps: 463, e: 0.064\n",
      "accumulated_rewards_per_episode: -4.629999999999946\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 5741/25000, steps: 330, e: 0.064\n",
      "accumulated_rewards_per_episode: -1.2999999999999976\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 5742/25000, steps: 707, e: 0.064\n",
      "accumulated_rewards_per_episode: -3.069999999999949\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5743/25000, steps: 232, e: 0.064\n",
      "accumulated_rewards_per_episode: -0.3200000000000018\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5744/25000, steps: 213, e: 0.063\n",
      "accumulated_rewards_per_episode: -0.13000000000000128\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 5745/25000, steps: 56, e: 0.063\n",
      "accumulated_rewards_per_episode: 0.4399999999999997\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 5746/25000, steps: 104, e: 0.063\n",
      "accumulated_rewards_per_episode: -0.040000000000000535\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5747/25000, steps: 800, e: 0.063\n",
      "accumulated_rewards_per_episode: -5.999999999999917\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5748/25000, steps: 740, e: 0.063\n",
      "accumulated_rewards_per_episode: -2.399999999999997\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 5749/25000, steps: 52, e: 0.063\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5750/25000, steps: 594, e: 0.063\n",
      "accumulated_rewards_per_episode: -1.9399999999999187\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 5751/25000, steps: 736, e: 0.063\n",
      "accumulated_rewards_per_episode: -5.35999999999991\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 5752/25000, steps: 800, e: 0.063\n",
      "accumulated_rewards_per_episode: -1.9999999999999107\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 5753/25000, steps: 52, e: 0.063\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 5754/25000, steps: 800, e: 0.063\n",
      "accumulated_rewards_per_episode: 2.689862221849637e-14\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5755/25000, steps: 800, e: 0.063\n",
      "accumulated_rewards_per_episode: -0.999999999999989\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 5756/25000, steps: 117, e: 0.063\n",
      "accumulated_rewards_per_episode: 0.8299999999999994\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5757/25000, steps: 52, e: 0.063\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 5758/25000, steps: 800, e: 0.063\n",
      "accumulated_rewards_per_episode: -4.999999999999874\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 5759/25000, steps: 800, e: 0.063\n",
      "accumulated_rewards_per_episode: -1.9999999999999898\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5760/25000, steps: 800, e: 0.063\n",
      "accumulated_rewards_per_episode: -5.999999999999917\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 5761/25000, steps: 316, e: 0.063\n",
      "accumulated_rewards_per_episode: -2.1599999999999984\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 5762/25000, steps: 800, e: 0.063\n",
      "accumulated_rewards_per_episode: 4.000000000000115\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 5763/25000, steps: 52, e: 0.063\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 5764/25000, steps: 52, e: 0.063\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5765/25000, steps: 800, e: 0.063\n",
      "accumulated_rewards_per_episode: -5.221517662690189e-15\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5766/25000, steps: 800, e: 0.063\n",
      "accumulated_rewards_per_episode: -5.97091820431217e-15\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 5767/25000, steps: 102, e: 0.063\n",
      "accumulated_rewards_per_episode: -1.0200000000000007\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5768/25000, steps: 92, e: 0.063\n",
      "accumulated_rewards_per_episode: -0.9200000000000006\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 5769/25000, steps: 55, e: 0.063\n",
      "accumulated_rewards_per_episode: -0.5500000000000003\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 5770/25000, steps: 800, e: 0.063\n",
      "accumulated_rewards_per_episode: 0.9999999999999942\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 5771/25000, steps: 157, e: 0.063\n",
      "accumulated_rewards_per_episode: -0.5700000000000011\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 5772/25000, steps: 93, e: 0.063\n",
      "accumulated_rewards_per_episode: -0.9300000000000006\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5773/25000, steps: 52, e: 0.063\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5774/25000, steps: 89, e: 0.063\n",
      "accumulated_rewards_per_episode: 0.10999999999999946\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 5775/25000, steps: 800, e: 0.063\n",
      "accumulated_rewards_per_episode: -5.999999999999874\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5776/25000, steps: 52, e: 0.063\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5777/25000, steps: 759, e: 0.062\n",
      "accumulated_rewards_per_episode: -6.589999999999883\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 5778/25000, steps: 800, e: 0.062\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 5779/25000, steps: 800, e: 0.062\n",
      "accumulated_rewards_per_episode: -2.999999999999874\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 5780/25000, steps: 301, e: 0.062\n",
      "accumulated_rewards_per_episode: 3.990000000000017\n",
      "START state: (0, 0, 9, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5781/25000, steps: 622, e: 0.062\n",
      "accumulated_rewards_per_episode: -2.2199999999999775\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5782/25000, steps: 59, e: 0.062\n",
      "accumulated_rewards_per_episode: 0.4099999999999996\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5783/25000, steps: 52, e: 0.062\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 5784/25000, steps: 52, e: 0.062\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 5785/25000, steps: 94, e: 0.062\n",
      "accumulated_rewards_per_episode: 1.0599999999999994\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5786/25000, steps: 52, e: 0.062\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 5787/25000, steps: 478, e: 0.062\n",
      "accumulated_rewards_per_episode: -0.7799999999999903\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 5788/25000, steps: 412, e: 0.062\n",
      "accumulated_rewards_per_episode: 5.880000000000052\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 5789/25000, steps: 127, e: 0.062\n",
      "accumulated_rewards_per_episode: 2.730000000000003\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 5790/25000, steps: 57, e: 0.062\n",
      "accumulated_rewards_per_episode: -0.5700000000000003\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5791/25000, steps: 52, e: 0.062\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5792/25000, steps: 333, e: 0.062\n",
      "accumulated_rewards_per_episode: -2.3299999999999734\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 5793/25000, steps: 109, e: 0.062\n",
      "accumulated_rewards_per_episode: 1.9099999999999997\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5794/25000, steps: 52, e: 0.062\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 5795/25000, steps: 52, e: 0.062\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5796/25000, steps: 67, e: 0.062\n",
      "accumulated_rewards_per_episode: 0.3299999999999996\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 5797/25000, steps: 52, e: 0.062\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 5798/25000, steps: 52, e: 0.062\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 5799/25000, steps: 800, e: 0.062\n",
      "accumulated_rewards_per_episode: -0.9999999999999998\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5800/25000, steps: 800, e: 0.062\n",
      "accumulated_rewards_per_episode: 5.000000000000069\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 5801/25000, steps: 700, e: 0.062\n",
      "accumulated_rewards_per_episode: 9.0000000000001\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 5802/25000, steps: 153, e: 0.062\n",
      "accumulated_rewards_per_episode: 2.470000000000005\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 5803/25000, steps: 800, e: 0.062\n",
      "accumulated_rewards_per_episode: 2.0000000000001226\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5804/25000, steps: 800, e: 0.062\n",
      "accumulated_rewards_per_episode: -2.3071822230491534e-15\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 5805/25000, steps: 52, e: 0.062\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5806/25000, steps: 97, e: 0.062\n",
      "accumulated_rewards_per_episode: 1.0299999999999994\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 5807/25000, steps: 190, e: 0.062\n",
      "accumulated_rewards_per_episode: -1.9000000000000015\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5808/25000, steps: 509, e: 0.062\n",
      "accumulated_rewards_per_episode: 0.9100000000000295\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 5809/25000, steps: 52, e: 0.062\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 5810/25000, steps: 52, e: 0.061\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 5811/25000, steps: 158, e: 0.061\n",
      "accumulated_rewards_per_episode: -1.5800000000000012\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5812/25000, steps: 52, e: 0.061\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 5813/25000, steps: 411, e: 0.061\n",
      "accumulated_rewards_per_episode: -4.109999999999957\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 5814/25000, steps: 52, e: 0.061\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5815/25000, steps: 52, e: 0.061\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5816/25000, steps: 209, e: 0.061\n",
      "accumulated_rewards_per_episode: -2.0899999999999994\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 5817/25000, steps: 800, e: 0.061\n",
      "accumulated_rewards_per_episode: 3.000000000000033\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5818/25000, steps: 52, e: 0.061\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 5819/25000, steps: 52, e: 0.061\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 5820/25000, steps: 800, e: 0.061\n",
      "accumulated_rewards_per_episode: -4.999999999999917\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 5821/25000, steps: 118, e: 0.061\n",
      "accumulated_rewards_per_episode: -1.1800000000000008\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 5822/25000, steps: 800, e: 0.061\n",
      "accumulated_rewards_per_episode: -5.859895901849654e-15\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 5823/25000, steps: 210, e: 0.061\n",
      "accumulated_rewards_per_episode: -1.1000000000000014\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 5824/25000, steps: 52, e: 0.061\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5825/25000, steps: 114, e: 0.061\n",
      "accumulated_rewards_per_episode: -0.14000000000000062\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5826/25000, steps: 112, e: 0.061\n",
      "accumulated_rewards_per_episode: -1.1200000000000008\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 5827/25000, steps: 52, e: 0.061\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5828/25000, steps: 70, e: 0.061\n",
      "accumulated_rewards_per_episode: 0.2999999999999995\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 5829/25000, steps: 91, e: 0.061\n",
      "accumulated_rewards_per_episode: 0.08999999999999936\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5830/25000, steps: 77, e: 0.061\n",
      "accumulated_rewards_per_episode: 0.22999999999999932\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5831/25000, steps: 69, e: 0.061\n",
      "accumulated_rewards_per_episode: 0.3099999999999995\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5832/25000, steps: 52, e: 0.061\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5833/25000, steps: 800, e: 0.061\n",
      "accumulated_rewards_per_episode: -4.999999999999934\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5834/25000, steps: 672, e: 0.061\n",
      "accumulated_rewards_per_episode: 1.2800000000000713\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5835/25000, steps: 52, e: 0.061\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 5836/25000, steps: 52, e: 0.061\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 5837/25000, steps: 800, e: 0.061\n",
      "accumulated_rewards_per_episode: 3.0000000000000506\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 5838/25000, steps: 52, e: 0.061\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 5839/25000, steps: 57, e: 0.061\n",
      "accumulated_rewards_per_episode: 0.4299999999999997\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5840/25000, steps: 52, e: 0.061\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 5841/25000, steps: 52, e: 0.061\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 5842/25000, steps: 111, e: 0.061\n",
      "accumulated_rewards_per_episode: -0.11000000000000067\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 5843/25000, steps: 52, e: 0.061\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 5844/25000, steps: 52, e: 0.06\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 5845/25000, steps: 86, e: 0.06\n",
      "accumulated_rewards_per_episode: 2.1400000000000055\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 5846/25000, steps: 224, e: 0.06\n",
      "accumulated_rewards_per_episode: -1.2400000000000015\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 5847/25000, steps: 52, e: 0.06\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 5848/25000, steps: 52, e: 0.06\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5849/25000, steps: 52, e: 0.06\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5850/25000, steps: 154, e: 0.06\n",
      "accumulated_rewards_per_episode: 0.4599999999999991\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 5851/25000, steps: 52, e: 0.06\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 5852/25000, steps: 52, e: 0.06\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5853/25000, steps: 136, e: 0.06\n",
      "accumulated_rewards_per_episode: -0.3600000000000008\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 5854/25000, steps: 223, e: 0.06\n",
      "accumulated_rewards_per_episode: -2.2299999999999964\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 5855/25000, steps: 52, e: 0.06\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5856/25000, steps: 52, e: 0.06\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 5857/25000, steps: 532, e: 0.06\n",
      "accumulated_rewards_per_episode: -4.319999999999931\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5858/25000, steps: 658, e: 0.06\n",
      "accumulated_rewards_per_episode: -1.5799999999999983\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5859/25000, steps: 258, e: 0.06\n",
      "accumulated_rewards_per_episode: -1.5800000000000016\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 5860/25000, steps: 123, e: 0.06\n",
      "accumulated_rewards_per_episode: -0.2300000000000006\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 5861/25000, steps: 730, e: 0.06\n",
      "accumulated_rewards_per_episode: -3.299999999999889\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 5862/25000, steps: 245, e: 0.06\n",
      "accumulated_rewards_per_episode: 0.5499999999999983\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 5863/25000, steps: 52, e: 0.06\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5864/25000, steps: 52, e: 0.06\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 5865/25000, steps: 52, e: 0.06\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 5866/25000, steps: 295, e: 0.06\n",
      "accumulated_rewards_per_episode: -1.950000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 5867/25000, steps: 800, e: 0.06\n",
      "accumulated_rewards_per_episode: -1.0000000000000056\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 5868/25000, steps: 791, e: 0.06\n",
      "accumulated_rewards_per_episode: 0.09000000000000091\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 5869/25000, steps: 274, e: 0.06\n",
      "accumulated_rewards_per_episode: 4.260000000000024\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5870/25000, steps: 800, e: 0.06\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 5871/25000, steps: 139, e: 0.06\n",
      "accumulated_rewards_per_episode: 2.6100000000000008\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 5872/25000, steps: 52, e: 0.06\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5873/25000, steps: 800, e: 0.06\n",
      "accumulated_rewards_per_episode: -4.999999999999874\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 5874/25000, steps: 490, e: 0.06\n",
      "accumulated_rewards_per_episode: -2.899999999999962\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 5875/25000, steps: 69, e: 0.06\n",
      "accumulated_rewards_per_episode: 0.3099999999999995\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 5876/25000, steps: 73, e: 0.06\n",
      "accumulated_rewards_per_episode: -0.7300000000000004\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5877/25000, steps: 52, e: 0.06\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 5878/25000, steps: 565, e: 0.06\n",
      "accumulated_rewards_per_episode: -2.649999999999924\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5879/25000, steps: 52, e: 0.059\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5880/25000, steps: 52, e: 0.059\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5881/25000, steps: 52, e: 0.059\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5882/25000, steps: 52, e: 0.059\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 5883/25000, steps: 522, e: 0.059\n",
      "accumulated_rewards_per_episode: -0.21999999999999992\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5884/25000, steps: 52, e: 0.059\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 5885/25000, steps: 88, e: 0.059\n",
      "accumulated_rewards_per_episode: 0.11999999999999934\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 5886/25000, steps: 84, e: 0.059\n",
      "accumulated_rewards_per_episode: 0.15999999999999936\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 5887/25000, steps: 384, e: 0.059\n",
      "accumulated_rewards_per_episode: -3.839999999999962\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 5888/25000, steps: 52, e: 0.059\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5889/25000, steps: 800, e: 0.059\n",
      "accumulated_rewards_per_episode: 3.0000000000000977\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5890/25000, steps: 52, e: 0.059\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 5891/25000, steps: 65, e: 0.059\n",
      "accumulated_rewards_per_episode: 1.3499999999999996\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5892/25000, steps: 52, e: 0.059\n",
      "accumulated_rewards_per_episode: 1.4799999999999998\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5893/25000, steps: 52, e: 0.059\n",
      "accumulated_rewards_per_episode: 1.4799999999999998\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5894/25000, steps: 800, e: 0.059\n",
      "accumulated_rewards_per_episode: 3.000000000000087\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 5895/25000, steps: 348, e: 0.059\n",
      "accumulated_rewards_per_episode: 0.5199999999999976\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5896/25000, steps: 178, e: 0.059\n",
      "accumulated_rewards_per_episode: 1.2199999999999986\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 5897/25000, steps: 52, e: 0.059\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 5898/25000, steps: 153, e: 0.059\n",
      "accumulated_rewards_per_episode: -0.5300000000000008\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 5899/25000, steps: 64, e: 0.059\n",
      "accumulated_rewards_per_episode: 0.35999999999999954\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5900/25000, steps: 65, e: 0.059\n",
      "accumulated_rewards_per_episode: 0.34999999999999964\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 5901/25000, steps: 222, e: 0.059\n",
      "accumulated_rewards_per_episode: 1.7800000000000105\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 5902/25000, steps: 181, e: 0.059\n",
      "accumulated_rewards_per_episode: 0.1899999999999984\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 5903/25000, steps: 61, e: 0.059\n",
      "accumulated_rewards_per_episode: 0.3899999999999997\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5904/25000, steps: 213, e: 0.059\n",
      "accumulated_rewards_per_episode: -2.1299999999999986\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 5905/25000, steps: 52, e: 0.059\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5906/25000, steps: 147, e: 0.059\n",
      "accumulated_rewards_per_episode: -0.4700000000000011\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 5907/25000, steps: 52, e: 0.059\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5908/25000, steps: 237, e: 0.059\n",
      "accumulated_rewards_per_episode: -1.3699999999999983\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 5909/25000, steps: 437, e: 0.059\n",
      "accumulated_rewards_per_episode: 2.63000000000003\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5910/25000, steps: 223, e: 0.059\n",
      "accumulated_rewards_per_episode: -1.2300000000000015\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5911/25000, steps: 122, e: 0.059\n",
      "accumulated_rewards_per_episode: -1.2200000000000009\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5912/25000, steps: 52, e: 0.059\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5913/25000, steps: 521, e: 0.059\n",
      "accumulated_rewards_per_episode: 3.7900000000000507\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 5914/25000, steps: 52, e: 0.059\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5915/25000, steps: 52, e: 0.058\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 5916/25000, steps: 52, e: 0.058\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5917/25000, steps: 429, e: 0.058\n",
      "accumulated_rewards_per_episode: -2.289999999999953\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5918/25000, steps: 52, e: 0.058\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 0, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5919/25000, steps: 52, e: 0.058\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 5920/25000, steps: 627, e: 0.058\n",
      "accumulated_rewards_per_episode: -2.26999999999996\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5921/25000, steps: 456, e: 0.058\n",
      "accumulated_rewards_per_episode: -0.5600000000000033\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5922/25000, steps: 159, e: 0.058\n",
      "accumulated_rewards_per_episode: 2.4100000000000064\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 5923/25000, steps: 103, e: 0.058\n",
      "accumulated_rewards_per_episode: -1.0300000000000007\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 5924/25000, steps: 52, e: 0.058\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5925/25000, steps: 134, e: 0.058\n",
      "accumulated_rewards_per_episode: -0.3400000000000008\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5926/25000, steps: 309, e: 0.058\n",
      "accumulated_rewards_per_episode: -2.0899999999999848\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 5927/25000, steps: 468, e: 0.058\n",
      "accumulated_rewards_per_episode: -2.679999999999959\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 5928/25000, steps: 52, e: 0.058\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 5929/25000, steps: 52, e: 0.058\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5930/25000, steps: 52, e: 0.058\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5931/25000, steps: 368, e: 0.058\n",
      "accumulated_rewards_per_episode: -2.679999999999966\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5932/25000, steps: 52, e: 0.058\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 5933/25000, steps: 66, e: 0.058\n",
      "accumulated_rewards_per_episode: -0.6600000000000004\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5934/25000, steps: 800, e: 0.058\n",
      "accumulated_rewards_per_episode: 1.0000000000000249\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 5935/25000, steps: 340, e: 0.058\n",
      "accumulated_rewards_per_episode: -2.399999999999972\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 5936/25000, steps: 71, e: 0.058\n",
      "accumulated_rewards_per_episode: 0.2899999999999996\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5937/25000, steps: 52, e: 0.058\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 5938/25000, steps: 403, e: 0.058\n",
      "accumulated_rewards_per_episode: -0.030000000000003156\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5939/25000, steps: 52, e: 0.058\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 5940/25000, steps: 52, e: 0.058\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 5941/25000, steps: 151, e: 0.058\n",
      "accumulated_rewards_per_episode: 1.4899999999999989\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5942/25000, steps: 582, e: 0.058\n",
      "accumulated_rewards_per_episode: -1.8199999999999867\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 5943/25000, steps: 688, e: 0.058\n",
      "accumulated_rewards_per_episode: -2.87999999999992\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5944/25000, steps: 650, e: 0.058\n",
      "accumulated_rewards_per_episode: 3.500000000000024\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5945/25000, steps: 457, e: 0.058\n",
      "accumulated_rewards_per_episode: 1.4300000000000066\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 5946/25000, steps: 52, e: 0.058\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 5947/25000, steps: 52, e: 0.058\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 5948/25000, steps: 633, e: 0.058\n",
      "accumulated_rewards_per_episode: -4.329999999999931\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5949/25000, steps: 455, e: 0.058\n",
      "accumulated_rewards_per_episode: -2.54999999999997\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 5950/25000, steps: 52, e: 0.057\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 5951/25000, steps: 52, e: 0.057\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 5952/25000, steps: 85, e: 0.057\n",
      "accumulated_rewards_per_episode: -0.8500000000000005\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5953/25000, steps: 52, e: 0.057\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5954/25000, steps: 513, e: 0.057\n",
      "accumulated_rewards_per_episode: 0.8699999999999966\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5955/25000, steps: 255, e: 0.057\n",
      "accumulated_rewards_per_episode: -1.5500000000000016\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5956/25000, steps: 226, e: 0.057\n",
      "accumulated_rewards_per_episode: 0.739999999999998\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5957/25000, steps: 52, e: 0.057\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5958/25000, steps: 88, e: 0.057\n",
      "accumulated_rewards_per_episode: 0.11999999999999934\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5959/25000, steps: 55, e: 0.057\n",
      "accumulated_rewards_per_episode: -0.5500000000000003\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 5960/25000, steps: 606, e: 0.057\n",
      "accumulated_rewards_per_episode: -2.059999999999964\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5961/25000, steps: 152, e: 0.057\n",
      "accumulated_rewards_per_episode: 0.47999999999999887\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 5962/25000, steps: 105, e: 0.057\n",
      "accumulated_rewards_per_episode: -0.05000000000000054\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5963/25000, steps: 243, e: 0.057\n",
      "accumulated_rewards_per_episode: 0.5699999999999983\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5964/25000, steps: 52, e: 0.057\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 5965/25000, steps: 52, e: 0.057\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5966/25000, steps: 52, e: 0.057\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 5967/25000, steps: 52, e: 0.057\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5968/25000, steps: 135, e: 0.057\n",
      "accumulated_rewards_per_episode: -0.3500000000000007\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 5969/25000, steps: 68, e: 0.057\n",
      "accumulated_rewards_per_episode: 0.3199999999999994\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5970/25000, steps: 52, e: 0.057\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 5971/25000, steps: 52, e: 0.057\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5972/25000, steps: 800, e: 0.057\n",
      "accumulated_rewards_per_episode: 3.0000000000001057\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5973/25000, steps: 800, e: 0.057\n",
      "accumulated_rewards_per_episode: -1.0000000000000058\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5974/25000, steps: 52, e: 0.057\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5975/25000, steps: 52, e: 0.057\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5976/25000, steps: 800, e: 0.057\n",
      "accumulated_rewards_per_episode: -2.000000000000004\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 5977/25000, steps: 52, e: 0.057\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5978/25000, steps: 75, e: 0.057\n",
      "accumulated_rewards_per_episode: 0.24999999999999933\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5979/25000, steps: 52, e: 0.057\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5980/25000, steps: 52, e: 0.057\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5981/25000, steps: 71, e: 0.057\n",
      "accumulated_rewards_per_episode: 1.2899999999999994\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 5982/25000, steps: 122, e: 0.057\n",
      "accumulated_rewards_per_episode: -1.2200000000000009\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 5983/25000, steps: 800, e: 0.057\n",
      "accumulated_rewards_per_episode: -4.9999999999999405\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 5984/25000, steps: 800, e: 0.057\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 5985/25000, steps: 294, e: 0.057\n",
      "accumulated_rewards_per_episode: -1.940000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 5986/25000, steps: 800, e: 0.057\n",
      "accumulated_rewards_per_episode: -4.999999999999918\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 5987/25000, steps: 800, e: 0.056\n",
      "accumulated_rewards_per_episode: -1.999999999999956\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 5988/25000, steps: 52, e: 0.056\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5989/25000, steps: 506, e: 0.056\n",
      "accumulated_rewards_per_episode: 8.940000000000088\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 5990/25000, steps: 800, e: 0.056\n",
      "accumulated_rewards_per_episode: 4.000000000000082\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 5991/25000, steps: 52, e: 0.056\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5992/25000, steps: 365, e: 0.056\n",
      "accumulated_rewards_per_episode: -2.6499999999999884\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5993/25000, steps: 52, e: 0.056\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5994/25000, steps: 92, e: 0.056\n",
      "accumulated_rewards_per_episode: 0.07999999999999947\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 5995/25000, steps: 104, e: 0.056\n",
      "accumulated_rewards_per_episode: -1.0400000000000007\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5996/25000, steps: 52, e: 0.056\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5997/25000, steps: 75, e: 0.056\n",
      "accumulated_rewards_per_episode: 0.24999999999999944\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 5998/25000, steps: 626, e: 0.056\n",
      "accumulated_rewards_per_episode: -5.259999999999933\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 5999/25000, steps: 554, e: 0.056\n",
      "accumulated_rewards_per_episode: -1.5399999999999692\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 6000/25000, steps: 545, e: 0.056\n",
      "accumulated_rewards_per_episode: -3.4499999999999496\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 6001/25000, steps: 286, e: 0.056\n",
      "accumulated_rewards_per_episode: -0.8600000000000021\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 6002/25000, steps: 59, e: 0.056\n",
      "accumulated_rewards_per_episode: -0.5900000000000003\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 6003/25000, steps: 52, e: 0.056\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6004/25000, steps: 52, e: 0.056\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 6005/25000, steps: 52, e: 0.056\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 6006/25000, steps: 223, e: 0.056\n",
      "accumulated_rewards_per_episode: -2.2299999999999964\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 6007/25000, steps: 77, e: 0.056\n",
      "accumulated_rewards_per_episode: 0.22999999999999954\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 6008/25000, steps: 52, e: 0.056\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6009/25000, steps: 52, e: 0.056\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6010/25000, steps: 800, e: 0.056\n",
      "accumulated_rewards_per_episode: 3.0000000000000657\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 6011/25000, steps: 126, e: 0.056\n",
      "accumulated_rewards_per_episode: 1.739999999999999\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 6012/25000, steps: 389, e: 0.056\n",
      "accumulated_rewards_per_episode: -2.8899999999999832\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 6013/25000, steps: 784, e: 0.056\n",
      "accumulated_rewards_per_episode: 1.160000000000049\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6014/25000, steps: 60, e: 0.056\n",
      "accumulated_rewards_per_episode: -0.6000000000000003\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 6015/25000, steps: 52, e: 0.056\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 6016/25000, steps: 52, e: 0.056\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 6017/25000, steps: 52, e: 0.056\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6018/25000, steps: 52, e: 0.056\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 6019/25000, steps: 657, e: 0.056\n",
      "accumulated_rewards_per_episode: -0.5700000000000034\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 6020/25000, steps: 52, e: 0.056\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 6021/25000, steps: 161, e: 0.056\n",
      "accumulated_rewards_per_episode: -1.6100000000000012\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6022/25000, steps: 52, e: 0.056\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6023/25000, steps: 52, e: 0.056\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 6024/25000, steps: 52, e: 0.055\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 6025/25000, steps: 72, e: 0.055\n",
      "accumulated_rewards_per_episode: 0.27999999999999936\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6026/25000, steps: 52, e: 0.055\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 6027/25000, steps: 532, e: 0.055\n",
      "accumulated_rewards_per_episode: -0.3200000000000038\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 6028/25000, steps: 52, e: 0.055\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6029/25000, steps: 89, e: 0.055\n",
      "accumulated_rewards_per_episode: 0.10999999999999924\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6030/25000, steps: 52, e: 0.055\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 6031/25000, steps: 800, e: 0.055\n",
      "accumulated_rewards_per_episode: 1.000000000000071\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6032/25000, steps: 52, e: 0.055\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6033/25000, steps: 69, e: 0.055\n",
      "accumulated_rewards_per_episode: -0.6900000000000004\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6034/25000, steps: 800, e: 0.055\n",
      "accumulated_rewards_per_episode: -3.9999999999999245\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6035/25000, steps: 671, e: 0.055\n",
      "accumulated_rewards_per_episode: 0.2899999999999945\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6036/25000, steps: 757, e: 0.055\n",
      "accumulated_rewards_per_episode: -0.5700000000000055\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6037/25000, steps: 52, e: 0.055\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 6038/25000, steps: 52, e: 0.055\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6039/25000, steps: 800, e: 0.055\n",
      "accumulated_rewards_per_episode: -4.99999999999994\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6040/25000, steps: 72, e: 0.055\n",
      "accumulated_rewards_per_episode: 0.2799999999999996\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6041/25000, steps: 52, e: 0.055\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 6042/25000, steps: 800, e: 0.055\n",
      "accumulated_rewards_per_episode: -6.999999999999895\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 6043/25000, steps: 144, e: 0.055\n",
      "accumulated_rewards_per_episode: -1.440000000000001\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 6044/25000, steps: 52, e: 0.055\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 6045/25000, steps: 255, e: 0.055\n",
      "accumulated_rewards_per_episode: -2.5499999999999896\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 6046/25000, steps: 52, e: 0.055\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 6047/25000, steps: 800, e: 0.055\n",
      "accumulated_rewards_per_episode: -4.999999999999917\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 6048/25000, steps: 394, e: 0.055\n",
      "accumulated_rewards_per_episode: -1.9400000000000026\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 6049/25000, steps: 162, e: 0.055\n",
      "accumulated_rewards_per_episode: -1.6200000000000012\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6050/25000, steps: 52, e: 0.055\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 6051/25000, steps: 303, e: 0.055\n",
      "accumulated_rewards_per_episode: -3.0299999999999794\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 6052/25000, steps: 800, e: 0.055\n",
      "accumulated_rewards_per_episode: -2.9999999999999263\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 6053/25000, steps: 158, e: 0.055\n",
      "accumulated_rewards_per_episode: -0.580000000000001\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6054/25000, steps: 768, e: 0.055\n",
      "accumulated_rewards_per_episode: -4.679999999999897\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6055/25000, steps: 800, e: 0.055\n",
      "accumulated_rewards_per_episode: -4.999999999999874\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6056/25000, steps: 55, e: 0.055\n",
      "accumulated_rewards_per_episode: -0.5500000000000003\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6057/25000, steps: 430, e: 0.055\n",
      "accumulated_rewards_per_episode: -1.2999999999999894\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6058/25000, steps: 52, e: 0.055\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6059/25000, steps: 52, e: 0.055\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6060/25000, steps: 52, e: 0.055\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 6061/25000, steps: 52, e: 0.055\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6062/25000, steps: 79, e: 0.054\n",
      "accumulated_rewards_per_episode: 0.20999999999999952\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6063/25000, steps: 53, e: 0.054\n",
      "accumulated_rewards_per_episode: -0.5300000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 6064/25000, steps: 449, e: 0.054\n",
      "accumulated_rewards_per_episode: 5.510000000000049\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6065/25000, steps: 412, e: 0.054\n",
      "accumulated_rewards_per_episode: 3.8800000000000194\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 6066/25000, steps: 52, e: 0.054\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 6067/25000, steps: 336, e: 0.054\n",
      "accumulated_rewards_per_episode: -1.360000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6068/25000, steps: 54, e: 0.054\n",
      "accumulated_rewards_per_episode: 0.45999999999999963\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 6069/25000, steps: 52, e: 0.054\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 6070/25000, steps: 52, e: 0.054\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 6071/25000, steps: 104, e: 0.054\n",
      "accumulated_rewards_per_episode: -1.0400000000000007\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6072/25000, steps: 52, e: 0.054\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6073/25000, steps: 52, e: 0.054\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 6074/25000, steps: 52, e: 0.054\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 6075/25000, steps: 204, e: 0.054\n",
      "accumulated_rewards_per_episode: 3.960000000000009\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 6076/25000, steps: 533, e: 0.054\n",
      "accumulated_rewards_per_episode: -0.32999999999999846\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 6077/25000, steps: 52, e: 0.054\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 6078/25000, steps: 52, e: 0.054\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6079/25000, steps: 158, e: 0.054\n",
      "accumulated_rewards_per_episode: -1.5800000000000012\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 6080/25000, steps: 800, e: 0.054\n",
      "accumulated_rewards_per_episode: -4.999999999999874\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 6081/25000, steps: 52, e: 0.054\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6082/25000, steps: 800, e: 0.054\n",
      "accumulated_rewards_per_episode: -7.999999999999874\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6083/25000, steps: 52, e: 0.054\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 6084/25000, steps: 52, e: 0.054\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 6085/25000, steps: 88, e: 0.054\n",
      "accumulated_rewards_per_episode: -0.8800000000000006\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6086/25000, steps: 52, e: 0.054\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 6087/25000, steps: 52, e: 0.054\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6088/25000, steps: 52, e: 0.054\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 6089/25000, steps: 800, e: 0.054\n",
      "accumulated_rewards_per_episode: -3.9999999999999187\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6090/25000, steps: 678, e: 0.054\n",
      "accumulated_rewards_per_episode: 1.2199999999999944\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6091/25000, steps: 800, e: 0.054\n",
      "accumulated_rewards_per_episode: -0.9999999999999107\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 6092/25000, steps: 545, e: 0.054\n",
      "accumulated_rewards_per_episode: -1.449999999999976\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6093/25000, steps: 52, e: 0.054\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6094/25000, steps: 800, e: 0.054\n",
      "accumulated_rewards_per_episode: -1.999999999999997\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 6095/25000, steps: 52, e: 0.054\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 6096/25000, steps: 52, e: 0.054\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6097/25000, steps: 800, e: 0.054\n",
      "accumulated_rewards_per_episode: 3.843800278069409e-14\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 6098/25000, steps: 358, e: 0.054\n",
      "accumulated_rewards_per_episode: 7.4200000000000434\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6099/25000, steps: 800, e: 0.054\n",
      "accumulated_rewards_per_episode: 3.000000000000014\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6100/25000, steps: 800, e: 0.054\n",
      "accumulated_rewards_per_episode: -0.999999999999941\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 6101/25000, steps: 79, e: 0.053\n",
      "accumulated_rewards_per_episode: 0.20999999999999952\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6102/25000, steps: 52, e: 0.053\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 6103/25000, steps: 110, e: 0.053\n",
      "accumulated_rewards_per_episode: -0.10000000000000063\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6104/25000, steps: 800, e: 0.053\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6105/25000, steps: 800, e: 0.053\n",
      "accumulated_rewards_per_episode: -2.0000000000000036\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 6106/25000, steps: 52, e: 0.053\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 6107/25000, steps: 52, e: 0.053\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 6108/25000, steps: 477, e: 0.053\n",
      "accumulated_rewards_per_episode: -3.7699999999999427\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6109/25000, steps: 795, e: 0.053\n",
      "accumulated_rewards_per_episode: 3.0500000000000114\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 6110/25000, steps: 200, e: 0.053\n",
      "accumulated_rewards_per_episode: 0.9999999999999982\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 6111/25000, steps: 52, e: 0.053\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 6112/25000, steps: 52, e: 0.053\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 6113/25000, steps: 52, e: 0.053\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6114/25000, steps: 52, e: 0.053\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 6115/25000, steps: 106, e: 0.053\n",
      "accumulated_rewards_per_episode: -1.0600000000000007\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 6116/25000, steps: 800, e: 0.053\n",
      "accumulated_rewards_per_episode: -0.9999999999999427\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6117/25000, steps: 395, e: 0.053\n",
      "accumulated_rewards_per_episode: 0.049999999999997255\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 6118/25000, steps: 800, e: 0.053\n",
      "accumulated_rewards_per_episode: -2.9999999999999587\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 6119/25000, steps: 101, e: 0.053\n",
      "accumulated_rewards_per_episode: -0.010000000000000753\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6120/25000, steps: 64, e: 0.053\n",
      "accumulated_rewards_per_episode: 0.35999999999999954\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 6121/25000, steps: 707, e: 0.053\n",
      "accumulated_rewards_per_episode: -6.069999999999894\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 6122/25000, steps: 52, e: 0.053\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 6123/25000, steps: 52, e: 0.053\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 6124/25000, steps: 800, e: 0.053\n",
      "accumulated_rewards_per_episode: -5.999999999999901\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 6125/25000, steps: 131, e: 0.053\n",
      "accumulated_rewards_per_episode: 3.6900000000000093\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6126/25000, steps: 800, e: 0.053\n",
      "accumulated_rewards_per_episode: 7.474229568593671e-14\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 6127/25000, steps: 82, e: 0.053\n",
      "accumulated_rewards_per_episode: -0.8200000000000005\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 6128/25000, steps: 52, e: 0.053\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6129/25000, steps: 52, e: 0.053\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6130/25000, steps: 800, e: 0.053\n",
      "accumulated_rewards_per_episode: 2.355754480376504e-15\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 6131/25000, steps: 103, e: 0.053\n",
      "accumulated_rewards_per_episode: -0.030000000000000644\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 6132/25000, steps: 52, e: 0.053\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6133/25000, steps: 108, e: 0.053\n",
      "accumulated_rewards_per_episode: -0.08000000000000053\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 6134/25000, steps: 52, e: 0.053\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 6135/25000, steps: 52, e: 0.053\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6136/25000, steps: 668, e: 0.053\n",
      "accumulated_rewards_per_episode: -6.679999999999902\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 6137/25000, steps: 800, e: 0.053\n",
      "accumulated_rewards_per_episode: 2.0396878630535298e-14\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6138/25000, steps: 356, e: 0.053\n",
      "accumulated_rewards_per_episode: 3.440000000000011\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 6139/25000, steps: 674, e: 0.053\n",
      "accumulated_rewards_per_episode: -2.7399999999999007\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6140/25000, steps: 348, e: 0.052\n",
      "accumulated_rewards_per_episode: 2.5200000000000156\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 6141/25000, steps: 800, e: 0.052\n",
      "accumulated_rewards_per_episode: 2.0000000000000373\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 6142/25000, steps: 800, e: 0.052\n",
      "accumulated_rewards_per_episode: -1.9999999999999765\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 6143/25000, steps: 656, e: 0.052\n",
      "accumulated_rewards_per_episode: 2.4400000000000537\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 6144/25000, steps: 52, e: 0.052\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 6145/25000, steps: 299, e: 0.052\n",
      "accumulated_rewards_per_episode: -1.989999999999984\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 6146/25000, steps: 54, e: 0.052\n",
      "accumulated_rewards_per_episode: 0.45999999999999974\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 6147/25000, steps: 800, e: 0.052\n",
      "accumulated_rewards_per_episode: 5.000000000000084\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6148/25000, steps: 800, e: 0.052\n",
      "accumulated_rewards_per_episode: -3.9999999999999623\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6149/25000, steps: 176, e: 0.052\n",
      "accumulated_rewards_per_episode: 0.23999999999999844\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6150/25000, steps: 560, e: 0.052\n",
      "accumulated_rewards_per_episode: 5.4000000000000865\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 6151/25000, steps: 76, e: 0.052\n",
      "accumulated_rewards_per_episode: 0.23999999999999932\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 6152/25000, steps: 52, e: 0.052\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 6153/25000, steps: 333, e: 0.052\n",
      "accumulated_rewards_per_episode: -0.33000000000000235\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 6154/25000, steps: 52, e: 0.052\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 6155/25000, steps: 307, e: 0.052\n",
      "accumulated_rewards_per_episode: -3.0699999999999785\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6156/25000, steps: 800, e: 0.052\n",
      "accumulated_rewards_per_episode: -5.999999999999896\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6157/25000, steps: 52, e: 0.052\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 6158/25000, steps: 52, e: 0.052\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6159/25000, steps: 800, e: 0.052\n",
      "accumulated_rewards_per_episode: -3.9999999999999396\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6160/25000, steps: 138, e: 0.052\n",
      "accumulated_rewards_per_episode: -0.3800000000000009\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 6161/25000, steps: 701, e: 0.052\n",
      "accumulated_rewards_per_episode: -0.009999999999969667\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6162/25000, steps: 190, e: 0.052\n",
      "accumulated_rewards_per_episode: -0.9000000000000012\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 6163/25000, steps: 70, e: 0.052\n",
      "accumulated_rewards_per_episode: -0.7000000000000004\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 6164/25000, steps: 723, e: 0.052\n",
      "accumulated_rewards_per_episode: 2.77000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 6165/25000, steps: 800, e: 0.052\n",
      "accumulated_rewards_per_episode: 3.0000000000001563\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 6166/25000, steps: 661, e: 0.052\n",
      "accumulated_rewards_per_episode: -0.6100000000000043\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 6167/25000, steps: 125, e: 0.052\n",
      "accumulated_rewards_per_episode: -1.2500000000000009\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6168/25000, steps: 52, e: 0.052\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 6169/25000, steps: 129, e: 0.052\n",
      "accumulated_rewards_per_episode: -1.290000000000001\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6170/25000, steps: 52, e: 0.052\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 6171/25000, steps: 52, e: 0.052\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6172/25000, steps: 52, e: 0.052\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 6173/25000, steps: 52, e: 0.052\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 6174/25000, steps: 76, e: 0.052\n",
      "accumulated_rewards_per_episode: 0.23999999999999932\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6175/25000, steps: 52, e: 0.052\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6176/25000, steps: 111, e: 0.052\n",
      "accumulated_rewards_per_episode: 0.8899999999999992\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 6177/25000, steps: 187, e: 0.052\n",
      "accumulated_rewards_per_episode: -0.8700000000000011\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 6178/25000, steps: 52, e: 0.052\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6179/25000, steps: 225, e: 0.052\n",
      "accumulated_rewards_per_episode: -2.249999999999996\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 6180/25000, steps: 52, e: 0.051\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 6181/25000, steps: 111, e: 0.051\n",
      "accumulated_rewards_per_episode: -0.11000000000000051\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 6182/25000, steps: 52, e: 0.051\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6183/25000, steps: 52, e: 0.051\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 6184/25000, steps: 52, e: 0.051\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 6185/25000, steps: 52, e: 0.051\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 6186/25000, steps: 800, e: 0.051\n",
      "accumulated_rewards_per_episode: 1.0000000000000426\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 6187/25000, steps: 86, e: 0.051\n",
      "accumulated_rewards_per_episode: -0.8600000000000005\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 6188/25000, steps: 52, e: 0.051\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 6189/25000, steps: 260, e: 0.051\n",
      "accumulated_rewards_per_episode: -0.6000000000000018\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6190/25000, steps: 52, e: 0.051\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 6191/25000, steps: 301, e: 0.051\n",
      "accumulated_rewards_per_episode: -2.0100000000000016\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 6192/25000, steps: 93, e: 0.051\n",
      "accumulated_rewards_per_episode: 0.06999999999999948\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 6193/25000, steps: 800, e: 0.051\n",
      "accumulated_rewards_per_episode: 7.0000000000000675\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 6194/25000, steps: 177, e: 0.051\n",
      "accumulated_rewards_per_episode: 0.22999999999999865\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 6195/25000, steps: 718, e: 0.051\n",
      "accumulated_rewards_per_episode: -1.17999999999999\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6196/25000, steps: 310, e: 0.051\n",
      "accumulated_rewards_per_episode: 1.9000000000000168\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6197/25000, steps: 58, e: 0.051\n",
      "accumulated_rewards_per_episode: 0.4199999999999995\n",
      "START state: (0, 0, 5, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6198/25000, steps: 106, e: 0.051\n",
      "accumulated_rewards_per_episode: -0.06000000000000054\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 6199/25000, steps: 182, e: 0.051\n",
      "accumulated_rewards_per_episode: 1.1799999999999988\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 6200/25000, steps: 52, e: 0.051\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 6201/25000, steps: 419, e: 0.051\n",
      "accumulated_rewards_per_episode: 0.8099999999999975\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6202/25000, steps: 287, e: 0.051\n",
      "accumulated_rewards_per_episode: -0.8700000000000019\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 6203/25000, steps: 737, e: 0.051\n",
      "accumulated_rewards_per_episode: 2.6300000000000026\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6204/25000, steps: 52, e: 0.051\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 6205/25000, steps: 52, e: 0.051\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 6206/25000, steps: 447, e: 0.051\n",
      "accumulated_rewards_per_episode: -0.4699999999999964\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 6207/25000, steps: 665, e: 0.051\n",
      "accumulated_rewards_per_episode: 1.350000000000026\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 6208/25000, steps: 52, e: 0.051\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6209/25000, steps: 191, e: 0.051\n",
      "accumulated_rewards_per_episode: -0.9100000000000015\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 6210/25000, steps: 605, e: 0.051\n",
      "accumulated_rewards_per_episode: 12.950000000000088\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6211/25000, steps: 52, e: 0.051\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 6212/25000, steps: 54, e: 0.051\n",
      "accumulated_rewards_per_episode: -0.5400000000000003\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 6213/25000, steps: 60, e: 0.051\n",
      "accumulated_rewards_per_episode: 0.3999999999999997\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 6214/25000, steps: 97, e: 0.051\n",
      "accumulated_rewards_per_episode: 1.0299999999999994\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 6215/25000, steps: 52, e: 0.051\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 6216/25000, steps: 715, e: 0.051\n",
      "accumulated_rewards_per_episode: 2.8500000000000583\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 6217/25000, steps: 800, e: 0.051\n",
      "accumulated_rewards_per_episode: 1.0000000000000626\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6218/25000, steps: 476, e: 0.051\n",
      "accumulated_rewards_per_episode: -4.759999999999943\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 6219/25000, steps: 52, e: 0.051\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6220/25000, steps: 800, e: 0.051\n",
      "accumulated_rewards_per_episode: 15.000000000000126\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 6221/25000, steps: 170, e: 0.05\n",
      "accumulated_rewards_per_episode: 1.3000000000000091\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6222/25000, steps: 77, e: 0.05\n",
      "accumulated_rewards_per_episode: 2.2300000000000058\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 6223/25000, steps: 52, e: 0.05\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 6224/25000, steps: 521, e: 0.05\n",
      "accumulated_rewards_per_episode: -0.21000000000000368\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 6225/25000, steps: 103, e: 0.05\n",
      "accumulated_rewards_per_episode: -0.030000000000000644\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 6226/25000, steps: 678, e: 0.05\n",
      "accumulated_rewards_per_episode: -1.779999999999987\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6227/25000, steps: 52, e: 0.05\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6228/25000, steps: 86, e: 0.05\n",
      "accumulated_rewards_per_episode: 0.13999999999999946\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 6229/25000, steps: 105, e: 0.05\n",
      "accumulated_rewards_per_episode: -1.0500000000000007\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6230/25000, steps: 52, e: 0.05\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 6231/25000, steps: 112, e: 0.05\n",
      "accumulated_rewards_per_episode: 0.8799999999999992\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 6232/25000, steps: 52, e: 0.05\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 6233/25000, steps: 52, e: 0.05\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6234/25000, steps: 372, e: 0.05\n",
      "accumulated_rewards_per_episode: -1.7199999999999882\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 6235/25000, steps: 515, e: 0.05\n",
      "accumulated_rewards_per_episode: -1.1499999999999408\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6236/25000, steps: 52, e: 0.05\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6237/25000, steps: 92, e: 0.05\n",
      "accumulated_rewards_per_episode: 3.080000000000008\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 6238/25000, steps: 510, e: 0.05\n",
      "accumulated_rewards_per_episode: 1.8999999999999984\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 6239/25000, steps: 800, e: 0.05\n",
      "accumulated_rewards_per_episode: -4.999999999999927\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 6240/25000, steps: 110, e: 0.05\n",
      "accumulated_rewards_per_episode: -0.10000000000000064\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 6241/25000, steps: 321, e: 0.05\n",
      "accumulated_rewards_per_episode: -0.21000000000000235\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 6242/25000, steps: 52, e: 0.05\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 6243/25000, steps: 52, e: 0.05\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6244/25000, steps: 562, e: 0.05\n",
      "accumulated_rewards_per_episode: 0.3799999999999958\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6245/25000, steps: 141, e: 0.05\n",
      "accumulated_rewards_per_episode: 0.589999999999999\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 6246/25000, steps: 52, e: 0.05\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 6247/25000, steps: 426, e: 0.05\n",
      "accumulated_rewards_per_episode: -3.2599999999999754\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6248/25000, steps: 75, e: 0.05\n",
      "accumulated_rewards_per_episode: 0.24999999999999944\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 6249/25000, steps: 461, e: 0.05\n",
      "accumulated_rewards_per_episode: -3.6099999999999675\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 6250/25000, steps: 423, e: 0.05\n",
      "accumulated_rewards_per_episode: -1.2300000000000026\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 6251/25000, steps: 67, e: 0.05\n",
      "accumulated_rewards_per_episode: 0.3299999999999996\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 6252/25000, steps: 56, e: 0.05\n",
      "accumulated_rewards_per_episode: 0.4399999999999997\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6253/25000, steps: 52, e: 0.05\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6254/25000, steps: 52, e: 0.05\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 6255/25000, steps: 52, e: 0.05\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 6256/25000, steps: 153, e: 0.05\n",
      "accumulated_rewards_per_episode: -0.5300000000000011\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 6257/25000, steps: 122, e: 0.05\n",
      "accumulated_rewards_per_episode: -0.22000000000000083\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6258/25000, steps: 52, e: 0.05\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 6259/25000, steps: 73, e: 0.05\n",
      "accumulated_rewards_per_episode: 0.2699999999999996\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 6260/25000, steps: 616, e: 0.05\n",
      "accumulated_rewards_per_episode: -1.159999999999942\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 6261/25000, steps: 314, e: 0.05\n",
      "accumulated_rewards_per_episode: -3.139999999999977\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6262/25000, steps: 52, e: 0.05\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 6263/25000, steps: 52, e: 0.049\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6264/25000, steps: 800, e: 0.049\n",
      "accumulated_rewards_per_episode: -0.9999999999999948\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6265/25000, steps: 110, e: 0.049\n",
      "accumulated_rewards_per_episode: -0.10000000000000074\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6266/25000, steps: 235, e: 0.049\n",
      "accumulated_rewards_per_episode: -1.3500000000000014\n",
      "START state: (0, 0, 8, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6267/25000, steps: 800, e: 0.049\n",
      "accumulated_rewards_per_episode: 9.000000000000083\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6268/25000, steps: 243, e: 0.049\n",
      "accumulated_rewards_per_episode: -1.4299999999999995\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 6269/25000, steps: 52, e: 0.049\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 6270/25000, steps: 52, e: 0.049\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 6271/25000, steps: 52, e: 0.049\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6272/25000, steps: 90, e: 0.049\n",
      "accumulated_rewards_per_episode: -0.9000000000000006\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 6273/25000, steps: 299, e: 0.049\n",
      "accumulated_rewards_per_episode: 0.009999999999997804\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6274/25000, steps: 449, e: 0.049\n",
      "accumulated_rewards_per_episode: -0.4899999999999747\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6275/25000, steps: 52, e: 0.049\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6276/25000, steps: 52, e: 0.049\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 6277/25000, steps: 800, e: 0.049\n",
      "accumulated_rewards_per_episode: 2.000000000000072\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 6278/25000, steps: 688, e: 0.049\n",
      "accumulated_rewards_per_episode: -2.8799999999999635\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6279/25000, steps: 506, e: 0.049\n",
      "accumulated_rewards_per_episode: 0.9399999999999964\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6280/25000, steps: 52, e: 0.049\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 6281/25000, steps: 105, e: 0.049\n",
      "accumulated_rewards_per_episode: -1.0500000000000007\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 6282/25000, steps: 52, e: 0.049\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 6283/25000, steps: 52, e: 0.049\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6284/25000, steps: 142, e: 0.049\n",
      "accumulated_rewards_per_episode: 1.5799999999999987\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6285/25000, steps: 328, e: 0.049\n",
      "accumulated_rewards_per_episode: -1.2799999999999978\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 6286/25000, steps: 800, e: 0.049\n",
      "accumulated_rewards_per_episode: 7.000000000000096\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6287/25000, steps: 64, e: 0.049\n",
      "accumulated_rewards_per_episode: -0.6400000000000003\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6288/25000, steps: 484, e: 0.049\n",
      "accumulated_rewards_per_episode: -3.839999999999963\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6289/25000, steps: 800, e: 0.049\n",
      "accumulated_rewards_per_episode: 8.000000000000158\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 6290/25000, steps: 52, e: 0.049\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6291/25000, steps: 52, e: 0.049\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 6292/25000, steps: 52, e: 0.049\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6293/25000, steps: 800, e: 0.049\n",
      "accumulated_rewards_per_episode: 1.000000000000032\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 6294/25000, steps: 52, e: 0.049\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 6295/25000, steps: 52, e: 0.049\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 6296/25000, steps: 85, e: 0.049\n",
      "accumulated_rewards_per_episode: 0.14999999999999947\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 6297/25000, steps: 124, e: 0.049\n",
      "accumulated_rewards_per_episode: -1.2400000000000009\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 6298/25000, steps: 800, e: 0.049\n",
      "accumulated_rewards_per_episode: -6.999999999999874\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 6299/25000, steps: 527, e: 0.049\n",
      "accumulated_rewards_per_episode: -2.2699999999999756\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 6300/25000, steps: 800, e: 0.049\n",
      "accumulated_rewards_per_episode: -1.9999999999999711\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 6301/25000, steps: 800, e: 0.049\n",
      "accumulated_rewards_per_episode: -1.9999999999999418\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6302/25000, steps: 800, e: 0.049\n",
      "accumulated_rewards_per_episode: 4.232378336688214e-14\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 6303/25000, steps: 63, e: 0.049\n",
      "accumulated_rewards_per_episode: 0.36999999999999944\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6304/25000, steps: 251, e: 0.049\n",
      "accumulated_rewards_per_episode: 0.489999999999998\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 6305/25000, steps: 263, e: 0.048\n",
      "accumulated_rewards_per_episode: 0.3699999999999979\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 6306/25000, steps: 321, e: 0.048\n",
      "accumulated_rewards_per_episode: 0.7899999999999978\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6307/25000, steps: 800, e: 0.048\n",
      "accumulated_rewards_per_episode: -1.0000000000000062\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6308/25000, steps: 121, e: 0.048\n",
      "accumulated_rewards_per_episode: 1.7899999999999991\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 6309/25000, steps: 52, e: 0.048\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 6310/25000, steps: 52, e: 0.048\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 6311/25000, steps: 52, e: 0.048\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6312/25000, steps: 566, e: 0.048\n",
      "accumulated_rewards_per_episode: 3.3400000000000443\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 6313/25000, steps: 605, e: 0.048\n",
      "accumulated_rewards_per_episode: 2.950000000000016\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 6314/25000, steps: 52, e: 0.048\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 6315/25000, steps: 250, e: 0.048\n",
      "accumulated_rewards_per_episode: 0.499999999999998\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6316/25000, steps: 52, e: 0.048\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 6317/25000, steps: 52, e: 0.048\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 6318/25000, steps: 52, e: 0.048\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6319/25000, steps: 52, e: 0.048\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6320/25000, steps: 317, e: 0.048\n",
      "accumulated_rewards_per_episode: -3.1699999999999764\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6321/25000, steps: 52, e: 0.048\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 6322/25000, steps: 800, e: 0.048\n",
      "accumulated_rewards_per_episode: -3.999999999999874\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 6323/25000, steps: 198, e: 0.048\n",
      "accumulated_rewards_per_episode: -0.9800000000000014\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 6324/25000, steps: 165, e: 0.048\n",
      "accumulated_rewards_per_episode: -0.6500000000000012\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 6325/25000, steps: 52, e: 0.048\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6326/25000, steps: 148, e: 0.048\n",
      "accumulated_rewards_per_episode: -0.48000000000000104\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6327/25000, steps: 52, e: 0.048\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 6328/25000, steps: 401, e: 0.048\n",
      "accumulated_rewards_per_episode: 0.9899999999999969\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6329/25000, steps: 241, e: 0.048\n",
      "accumulated_rewards_per_episode: 0.5899999999999981\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 6330/25000, steps: 177, e: 0.048\n",
      "accumulated_rewards_per_episode: -0.7700000000000012\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 6331/25000, steps: 69, e: 0.048\n",
      "accumulated_rewards_per_episode: 0.3099999999999996\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6332/25000, steps: 56, e: 0.048\n",
      "accumulated_rewards_per_episode: 0.4399999999999995\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 6333/25000, steps: 532, e: 0.048\n",
      "accumulated_rewards_per_episode: -3.3199999999999505\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 6334/25000, steps: 433, e: 0.048\n",
      "accumulated_rewards_per_episode: -3.3299999999999734\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 6335/25000, steps: 800, e: 0.048\n",
      "accumulated_rewards_per_episode: -2.999999999999922\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 6336/25000, steps: 52, e: 0.048\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6337/25000, steps: 52, e: 0.048\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6338/25000, steps: 515, e: 0.048\n",
      "accumulated_rewards_per_episode: -5.149999999999935\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 6339/25000, steps: 52, e: 0.048\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 6340/25000, steps: 52, e: 0.048\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6341/25000, steps: 52, e: 0.048\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6342/25000, steps: 294, e: 0.048\n",
      "accumulated_rewards_per_episode: -0.9400000000000015\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6343/25000, steps: 307, e: 0.048\n",
      "accumulated_rewards_per_episode: -1.070000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6344/25000, steps: 800, e: 0.048\n",
      "accumulated_rewards_per_episode: 5.000000000000066\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 6345/25000, steps: 446, e: 0.048\n",
      "accumulated_rewards_per_episode: -1.4600000000000017\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6346/25000, steps: 99, e: 0.048\n",
      "accumulated_rewards_per_episode: 0.009999999999999442\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 6347/25000, steps: 52, e: 0.048\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6348/25000, steps: 507, e: 0.047\n",
      "accumulated_rewards_per_episode: 3.9300000000000326\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6349/25000, steps: 106, e: 0.047\n",
      "accumulated_rewards_per_episode: -0.06000000000000054\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6350/25000, steps: 52, e: 0.047\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6351/25000, steps: 800, e: 0.047\n",
      "accumulated_rewards_per_episode: -4.5276282722994665e-15\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 6352/25000, steps: 446, e: 0.047\n",
      "accumulated_rewards_per_episode: -0.4600000000000011\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6353/25000, steps: 56, e: 0.047\n",
      "accumulated_rewards_per_episode: 1.4399999999999997\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 6354/25000, steps: 52, e: 0.047\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6355/25000, steps: 149, e: 0.047\n",
      "accumulated_rewards_per_episode: -1.490000000000001\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6356/25000, steps: 125, e: 0.047\n",
      "accumulated_rewards_per_episode: 0.7499999999999993\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6357/25000, steps: 83, e: 0.047\n",
      "accumulated_rewards_per_episode: 1.1699999999999993\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 6358/25000, steps: 641, e: 0.047\n",
      "accumulated_rewards_per_episode: 2.590000000000005\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 6359/25000, steps: 52, e: 0.047\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6360/25000, steps: 52, e: 0.047\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 6361/25000, steps: 563, e: 0.047\n",
      "accumulated_rewards_per_episode: -3.629999999999968\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6362/25000, steps: 52, e: 0.047\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 6363/25000, steps: 89, e: 0.047\n",
      "accumulated_rewards_per_episode: 1.1099999999999994\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 6364/25000, steps: 450, e: 0.047\n",
      "accumulated_rewards_per_episode: -1.5\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6365/25000, steps: 52, e: 0.047\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 6366/25000, steps: 52, e: 0.047\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 6367/25000, steps: 52, e: 0.047\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6368/25000, steps: 52, e: 0.047\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 6369/25000, steps: 52, e: 0.047\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 6370/25000, steps: 52, e: 0.047\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 6371/25000, steps: 52, e: 0.047\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6372/25000, steps: 649, e: 0.047\n",
      "accumulated_rewards_per_episode: -3.489999999999906\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 6373/25000, steps: 195, e: 0.047\n",
      "accumulated_rewards_per_episode: 0.049999999999998365\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6374/25000, steps: 482, e: 0.047\n",
      "accumulated_rewards_per_episode: -0.8200000000000035\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 6375/25000, steps: 204, e: 0.047\n",
      "accumulated_rewards_per_episode: -0.04000000000000131\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 6376/25000, steps: 743, e: 0.047\n",
      "accumulated_rewards_per_episode: -1.4300000000000028\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6377/25000, steps: 413, e: 0.047\n",
      "accumulated_rewards_per_episode: -2.13\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6378/25000, steps: 800, e: 0.047\n",
      "accumulated_rewards_per_episode: -6.303985111699717e-15\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 6379/25000, steps: 188, e: 0.047\n",
      "accumulated_rewards_per_episode: -1.8800000000000014\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 6380/25000, steps: 800, e: 0.047\n",
      "accumulated_rewards_per_episode: -5.999999999999896\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 6381/25000, steps: 211, e: 0.047\n",
      "accumulated_rewards_per_episode: -0.11000000000000162\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 6382/25000, steps: 52, e: 0.047\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6383/25000, steps: 128, e: 0.047\n",
      "accumulated_rewards_per_episode: -1.280000000000001\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 6384/25000, steps: 52, e: 0.047\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 6385/25000, steps: 71, e: 0.047\n",
      "accumulated_rewards_per_episode: 0.2899999999999996\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 6386/25000, steps: 52, e: 0.047\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 6387/25000, steps: 52, e: 0.047\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 6388/25000, steps: 449, e: 0.047\n",
      "accumulated_rewards_per_episode: 0.5099999999999976\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6389/25000, steps: 800, e: 0.047\n",
      "accumulated_rewards_per_episode: -0.9999999999999287\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 6390/25000, steps: 317, e: 0.047\n",
      "accumulated_rewards_per_episode: 0.8299999999999981\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 6391/25000, steps: 800, e: 0.047\n",
      "accumulated_rewards_per_episode: 6.275188701998502e-14\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 6392/25000, steps: 215, e: 0.047\n",
      "accumulated_rewards_per_episode: 2.8500000000000294\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6393/25000, steps: 52, e: 0.046\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 6394/25000, steps: 52, e: 0.046\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6395/25000, steps: 52, e: 0.046\n",
      "accumulated_rewards_per_episode: 0.47999999999999976\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 6396/25000, steps: 52, e: 0.046\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 6397/25000, steps: 52, e: 0.046\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 6398/25000, steps: 73, e: 0.046\n",
      "accumulated_rewards_per_episode: 0.2699999999999996\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6399/25000, steps: 52, e: 0.046\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 6400/25000, steps: 689, e: 0.046\n",
      "accumulated_rewards_per_episode: 7.110000000000103\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 6401/25000, steps: 800, e: 0.046\n",
      "accumulated_rewards_per_episode: -2.999999999999962\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 6402/25000, steps: 52, e: 0.046\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6403/25000, steps: 207, e: 0.046\n",
      "accumulated_rewards_per_episode: 2.9300000000000157\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 6404/25000, steps: 52, e: 0.046\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6405/25000, steps: 800, e: 0.046\n",
      "accumulated_rewards_per_episode: 12.00000000000009\n",
      "START state: (0, 0, 5, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6406/25000, steps: 800, e: 0.046\n",
      "accumulated_rewards_per_episode: 6.00000000000009\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 6407/25000, steps: 315, e: 0.046\n",
      "accumulated_rewards_per_episode: -0.1500000000000023\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6408/25000, steps: 52, e: 0.046\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 6409/25000, steps: 52, e: 0.046\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 6410/25000, steps: 800, e: 0.046\n",
      "accumulated_rewards_per_episode: -4.999999999999874\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 6411/25000, steps: 781, e: 0.046\n",
      "accumulated_rewards_per_episode: 2.1900000000000404\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 6412/25000, steps: 800, e: 0.046\n",
      "accumulated_rewards_per_episode: 0.999999999999996\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6413/25000, steps: 78, e: 0.046\n",
      "accumulated_rewards_per_episode: 0.21999999999999953\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6414/25000, steps: 777, e: 0.046\n",
      "accumulated_rewards_per_episode: -1.7699999999999956\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6415/25000, steps: 534, e: 0.046\n",
      "accumulated_rewards_per_episode: 0.6600000000000079\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 6416/25000, steps: 217, e: 0.046\n",
      "accumulated_rewards_per_episode: -0.17000000000000143\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 6417/25000, steps: 92, e: 0.046\n",
      "accumulated_rewards_per_episode: 1.0799999999999994\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6418/25000, steps: 800, e: 0.046\n",
      "accumulated_rewards_per_episode: -0.9999999999999741\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 6419/25000, steps: 144, e: 0.046\n",
      "accumulated_rewards_per_episode: 1.5599999999999992\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 6420/25000, steps: 52, e: 0.046\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 6421/25000, steps: 629, e: 0.046\n",
      "accumulated_rewards_per_episode: 0.710000000000035\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 6422/25000, steps: 155, e: 0.046\n",
      "accumulated_rewards_per_episode: 1.4499999999999988\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6423/25000, steps: 52, e: 0.046\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 6424/25000, steps: 114, e: 0.046\n",
      "accumulated_rewards_per_episode: -0.1400000000000005\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6425/25000, steps: 58, e: 0.046\n",
      "accumulated_rewards_per_episode: -0.5800000000000003\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 6426/25000, steps: 800, e: 0.046\n",
      "accumulated_rewards_per_episode: 5.000000000000077\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6427/25000, steps: 52, e: 0.046\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6428/25000, steps: 256, e: 0.046\n",
      "accumulated_rewards_per_episode: 0.4399999999999984\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6429/25000, steps: 52, e: 0.046\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 6430/25000, steps: 537, e: 0.046\n",
      "accumulated_rewards_per_episode: -0.3700000000000034\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 6431/25000, steps: 52, e: 0.046\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6432/25000, steps: 52, e: 0.046\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6433/25000, steps: 52, e: 0.046\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6434/25000, steps: 52, e: 0.046\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6435/25000, steps: 52, e: 0.046\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 6436/25000, steps: 52, e: 0.046\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6437/25000, steps: 213, e: 0.046\n",
      "accumulated_rewards_per_episode: -1.1300000000000017\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 6438/25000, steps: 800, e: 0.045\n",
      "accumulated_rewards_per_episode: 7.0000000000001545\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6439/25000, steps: 52, e: 0.045\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 6440/25000, steps: 800, e: 0.045\n",
      "accumulated_rewards_per_episode: 3.000000000000015\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6441/25000, steps: 327, e: 0.045\n",
      "accumulated_rewards_per_episode: -0.2700000000000024\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 6442/25000, steps: 800, e: 0.045\n",
      "accumulated_rewards_per_episode: 5.5202370452533955e-14\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 6443/25000, steps: 68, e: 0.045\n",
      "accumulated_rewards_per_episode: 0.3199999999999996\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 6444/25000, steps: 128, e: 0.045\n",
      "accumulated_rewards_per_episode: 1.7200000000000073\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 6445/25000, steps: 82, e: 0.045\n",
      "accumulated_rewards_per_episode: 0.17999999999999938\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 6446/25000, steps: 128, e: 0.045\n",
      "accumulated_rewards_per_episode: -0.28000000000000075\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6447/25000, steps: 96, e: 0.045\n",
      "accumulated_rewards_per_episode: 0.039999999999999473\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 6448/25000, steps: 52, e: 0.045\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 6449/25000, steps: 800, e: 0.045\n",
      "accumulated_rewards_per_episode: 8.000000000000137\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6450/25000, steps: 105, e: 0.045\n",
      "accumulated_rewards_per_episode: -0.05000000000000054\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6451/25000, steps: 52, e: 0.045\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 6452/25000, steps: 52, e: 0.045\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 6453/25000, steps: 58, e: 0.045\n",
      "accumulated_rewards_per_episode: 0.4199999999999996\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6454/25000, steps: 54, e: 0.045\n",
      "accumulated_rewards_per_episode: 0.45999999999999974\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 6455/25000, steps: 52, e: 0.045\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 6456/25000, steps: 392, e: 0.045\n",
      "accumulated_rewards_per_episode: -3.9199999999999604\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 6457/25000, steps: 800, e: 0.045\n",
      "accumulated_rewards_per_episode: 1.000000000000071\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 6458/25000, steps: 52, e: 0.045\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 6459/25000, steps: 575, e: 0.045\n",
      "accumulated_rewards_per_episode: 5.250000000000104\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 6460/25000, steps: 268, e: 0.045\n",
      "accumulated_rewards_per_episode: -1.6800000000000022\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 6461/25000, steps: 105, e: 0.045\n",
      "accumulated_rewards_per_episode: -0.05000000000000054\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6462/25000, steps: 52, e: 0.045\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 6463/25000, steps: 539, e: 0.045\n",
      "accumulated_rewards_per_episode: 0.6099999999999957\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 6464/25000, steps: 635, e: 0.045\n",
      "accumulated_rewards_per_episode: -3.3499999999999295\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 6465/25000, steps: 155, e: 0.045\n",
      "accumulated_rewards_per_episode: -0.5500000000000008\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6466/25000, steps: 800, e: 0.045\n",
      "accumulated_rewards_per_episode: -0.9999999999999629\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6467/25000, steps: 183, e: 0.045\n",
      "accumulated_rewards_per_episode: -0.8300000000000014\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6468/25000, steps: 800, e: 0.045\n",
      "accumulated_rewards_per_episode: 2.000000000000065\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 6469/25000, steps: 709, e: 0.045\n",
      "accumulated_rewards_per_episode: 1.9100000000000537\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 6470/25000, steps: 52, e: 0.045\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 6471/25000, steps: 257, e: 0.045\n",
      "accumulated_rewards_per_episode: 1.4300000000000048\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 6472/25000, steps: 800, e: 0.045\n",
      "accumulated_rewards_per_episode: 7.000000000000024\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6473/25000, steps: 52, e: 0.045\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6474/25000, steps: 187, e: 0.045\n",
      "accumulated_rewards_per_episode: 4.130000000000023\n",
      "START state: (0, 0, 9, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6475/25000, steps: 355, e: 0.045\n",
      "accumulated_rewards_per_episode: -1.5500000000000023\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 6476/25000, steps: 52, e: 0.045\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6477/25000, steps: 52, e: 0.045\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 6478/25000, steps: 52, e: 0.045\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 6479/25000, steps: 52, e: 0.045\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6480/25000, steps: 551, e: 0.045\n",
      "accumulated_rewards_per_episode: -2.509999999999993\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 6481/25000, steps: 198, e: 0.045\n",
      "accumulated_rewards_per_episode: 0.019999999999998803\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 6482/25000, steps: 115, e: 0.045\n",
      "accumulated_rewards_per_episode: -1.1500000000000008\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6483/25000, steps: 52, e: 0.045\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 6484/25000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 6485/25000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 6486/25000, steps: 410, e: 0.044\n",
      "accumulated_rewards_per_episode: 2.900000000000019\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 6487/25000, steps: 166, e: 0.044\n",
      "accumulated_rewards_per_episode: 2.340000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 6488/25000, steps: 112, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.12000000000000068\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6489/25000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 6490/25000, steps: 153, e: 0.044\n",
      "accumulated_rewards_per_episode: -1.5300000000000011\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 6491/25000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6492/25000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 6493/25000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6494/25000, steps: 160, e: 0.044\n",
      "accumulated_rewards_per_episode: 1.3999999999999986\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 6495/25000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6496/25000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 6497/25000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6498/25000, steps: 101, e: 0.044\n",
      "accumulated_rewards_per_episode: -1.0100000000000007\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 6499/25000, steps: 61, e: 0.044\n",
      "accumulated_rewards_per_episode: 0.38999999999999946\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 6500/25000, steps: 647, e: 0.044\n",
      "accumulated_rewards_per_episode: -2.4699999999999065\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 6501/25000, steps: 322, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.22000000000000203\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 6502/25000, steps: 257, e: 0.044\n",
      "accumulated_rewards_per_episode: 1.4299999999999982\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6503/25000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 6504/25000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6505/25000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6506/25000, steps: 61, e: 0.044\n",
      "accumulated_rewards_per_episode: 1.3899999999999997\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 6507/25000, steps: 515, e: 0.044\n",
      "accumulated_rewards_per_episode: -1.149999999999979\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 6508/25000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 6509/25000, steps: 540, e: 0.044\n",
      "accumulated_rewards_per_episode: -3.3999999999999293\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 6510/25000, steps: 207, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.07000000000000137\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6511/25000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 6512/25000, steps: 163, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.6300000000000009\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6513/25000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 6514/25000, steps: 636, e: 0.044\n",
      "accumulated_rewards_per_episode: 0.6400000000000043\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 6515/25000, steps: 108, e: 0.044\n",
      "accumulated_rewards_per_episode: -1.0800000000000007\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 6516/25000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6517/25000, steps: 91, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.9100000000000006\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6518/25000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 6519/25000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 6520/25000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6521/25000, steps: 800, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.9999999999999952\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 6522/25000, steps: 457, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.5700000000000035\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 6523/25000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 6524/25000, steps: 800, e: 0.044\n",
      "accumulated_rewards_per_episode: -3.999999999999896\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6525/25000, steps: 145, e: 0.044\n",
      "accumulated_rewards_per_episode: 1.549999999999999\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 6526/25000, steps: 317, e: 0.044\n",
      "accumulated_rewards_per_episode: 0.8299999999999974\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 6527/25000, steps: 528, e: 0.044\n",
      "accumulated_rewards_per_episode: -1.2799999999999685\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 6528/25000, steps: 123, e: 0.044\n",
      "accumulated_rewards_per_episode: -1.2300000000000009\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 6529/25000, steps: 800, e: 0.044\n",
      "accumulated_rewards_per_episode: 2.0000000000000915\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6530/25000, steps: 485, e: 0.044\n",
      "accumulated_rewards_per_episode: 2.150000000000076\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 6531/25000, steps: 800, e: 0.044\n",
      "accumulated_rewards_per_episode: -2.999999999999896\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 6532/25000, steps: 405, e: 0.043\n",
      "accumulated_rewards_per_episode: -3.04999999999998\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6533/25000, steps: 52, e: 0.043\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 6534/25000, steps: 370, e: 0.043\n",
      "accumulated_rewards_per_episode: 8.300000000000063\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6535/25000, steps: 52, e: 0.043\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 6536/25000, steps: 417, e: 0.043\n",
      "accumulated_rewards_per_episode: 0.8300000000000198\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 6537/25000, steps: 800, e: 0.043\n",
      "accumulated_rewards_per_episode: -1.9999999999999984\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 6538/25000, steps: 800, e: 0.043\n",
      "accumulated_rewards_per_episode: -2.999999999999962\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6539/25000, steps: 52, e: 0.043\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6540/25000, steps: 52, e: 0.043\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 6541/25000, steps: 52, e: 0.043\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 6542/25000, steps: 800, e: 0.043\n",
      "accumulated_rewards_per_episode: -1.9999999999999987\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 6543/25000, steps: 52, e: 0.043\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6544/25000, steps: 657, e: 0.043\n",
      "accumulated_rewards_per_episode: 0.4300000000000115\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 6545/25000, steps: 52, e: 0.043\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6546/25000, steps: 52, e: 0.043\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 6547/25000, steps: 52, e: 0.043\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6548/25000, steps: 656, e: 0.043\n",
      "accumulated_rewards_per_episode: -4.559999999999905\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 6549/25000, steps: 470, e: 0.043\n",
      "accumulated_rewards_per_episode: -1.699999999999988\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 6550/25000, steps: 87, e: 0.043\n",
      "accumulated_rewards_per_episode: 0.12999999999999934\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6551/25000, steps: 209, e: 0.043\n",
      "accumulated_rewards_per_episode: -2.0899999999999994\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 6552/25000, steps: 52, e: 0.043\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6553/25000, steps: 800, e: 0.043\n",
      "accumulated_rewards_per_episode: -5.999999999999874\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6554/25000, steps: 800, e: 0.043\n",
      "accumulated_rewards_per_episode: 2.3186313979906004e-14\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6555/25000, steps: 418, e: 0.043\n",
      "accumulated_rewards_per_episode: -0.18000000000000344\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6556/25000, steps: 462, e: 0.043\n",
      "accumulated_rewards_per_episode: 0.37999999999999634\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 6557/25000, steps: 680, e: 0.043\n",
      "accumulated_rewards_per_episode: -2.799999999999965\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 6558/25000, steps: 289, e: 0.043\n",
      "accumulated_rewards_per_episode: -2.8899999999999824\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6559/25000, steps: 325, e: 0.043\n",
      "accumulated_rewards_per_episode: -0.25000000000000194\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6560/25000, steps: 52, e: 0.043\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 6561/25000, steps: 157, e: 0.043\n",
      "accumulated_rewards_per_episode: -1.5700000000000012\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6562/25000, steps: 144, e: 0.043\n",
      "accumulated_rewards_per_episode: 0.5599999999999989\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6563/25000, steps: 52, e: 0.043\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6564/25000, steps: 148, e: 0.043\n",
      "accumulated_rewards_per_episode: 0.5199999999999989\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 6565/25000, steps: 52, e: 0.043\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 6566/25000, steps: 514, e: 0.043\n",
      "accumulated_rewards_per_episode: 8.860000000000058\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6567/25000, steps: 293, e: 0.043\n",
      "accumulated_rewards_per_episode: 4.0700000000000305\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 6568/25000, steps: 548, e: 0.043\n",
      "accumulated_rewards_per_episode: 4.520000000000012\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6569/25000, steps: 118, e: 0.043\n",
      "accumulated_rewards_per_episode: 1.819999999999999\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6570/25000, steps: 89, e: 0.043\n",
      "accumulated_rewards_per_episode: 1.1099999999999994\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6571/25000, steps: 74, e: 0.043\n",
      "accumulated_rewards_per_episode: -0.7400000000000004\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6572/25000, steps: 54, e: 0.043\n",
      "accumulated_rewards_per_episode: 1.4599999999999995\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 6573/25000, steps: 52, e: 0.043\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 6574/25000, steps: 52, e: 0.043\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 6575/25000, steps: 52, e: 0.043\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 6576/25000, steps: 460, e: 0.043\n",
      "accumulated_rewards_per_episode: -1.5999999999999945\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6577/25000, steps: 154, e: 0.043\n",
      "accumulated_rewards_per_episode: 0.4599999999999991\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6578/25000, steps: 131, e: 0.043\n",
      "accumulated_rewards_per_episode: 4.6900000000000155\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 6579/25000, steps: 65, e: 0.043\n",
      "accumulated_rewards_per_episode: 1.3499999999999996\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6580/25000, steps: 142, e: 0.042\n",
      "accumulated_rewards_per_episode: 4.580000000000014\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6581/25000, steps: 800, e: 0.042\n",
      "accumulated_rewards_per_episode: 3.033337470093045e-14\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6582/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 6583/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6584/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6585/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 4)\n",
      "episode: 6586/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 6587/25000, steps: 800, e: 0.042\n",
      "accumulated_rewards_per_episode: 14.000000000000117\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 6588/25000, steps: 118, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.18000000000000066\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 6589/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 6590/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6591/25000, steps: 91, e: 0.042\n",
      "accumulated_rewards_per_episode: 0.08999999999999925\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 6592/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 6593/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 6594/25000, steps: 133, e: 0.042\n",
      "accumulated_rewards_per_episode: 0.6699999999999993\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 6595/25000, steps: 308, e: 0.042\n",
      "accumulated_rewards_per_episode: -1.0800000000000018\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 6596/25000, steps: 152, e: 0.042\n",
      "accumulated_rewards_per_episode: 0.4799999999999991\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 6597/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 6598/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 6599/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6600/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 6601/25000, steps: 800, e: 0.042\n",
      "accumulated_rewards_per_episode: 14.000000000000124\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6602/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 6603/25000, steps: 578, e: 0.042\n",
      "accumulated_rewards_per_episode: 6.220000000000109\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6604/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 6605/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6606/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 6607/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: 1.4799999999999998\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 6608/25000, steps: 91, e: 0.042\n",
      "accumulated_rewards_per_episode: 1.0899999999999992\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6609/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 6610/25000, steps: 364, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.6400000000000023\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 6611/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6612/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 6613/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6614/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6615/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6616/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6617/25000, steps: 114, e: 0.042\n",
      "accumulated_rewards_per_episode: 0.8599999999999993\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 6618/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 6619/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6620/25000, steps: 693, e: 0.042\n",
      "accumulated_rewards_per_episode: 5.070000000000107\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 6621/25000, steps: 800, e: 0.042\n",
      "accumulated_rewards_per_episode: 5.000000000000025\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6622/25000, steps: 107, e: 0.042\n",
      "accumulated_rewards_per_episode: 1.929999999999999\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6623/25000, steps: 99, e: 0.042\n",
      "accumulated_rewards_per_episode: 1.0099999999999993\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6624/25000, steps: 458, e: 0.042\n",
      "accumulated_rewards_per_episode: 4.4200000000000275\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 6625/25000, steps: 113, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.1300000000000005\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 6626/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 6627/25000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 6628/25000, steps: 82, e: 0.042\n",
      "accumulated_rewards_per_episode: 1.1799999999999993\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6629/25000, steps: 223, e: 0.042\n",
      "accumulated_rewards_per_episode: -0.23000000000000173\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6630/25000, steps: 800, e: 0.041\n",
      "accumulated_rewards_per_episode: -0.9999999999999636\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 6631/25000, steps: 590, e: 0.041\n",
      "accumulated_rewards_per_episode: 2.100000000000005\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 6632/25000, steps: 323, e: 0.041\n",
      "accumulated_rewards_per_episode: 0.7699999999999978\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 6633/25000, steps: 52, e: 0.041\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 6634/25000, steps: 160, e: 0.041\n",
      "accumulated_rewards_per_episode: 0.3999999999999988\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6635/25000, steps: 52, e: 0.041\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 6636/25000, steps: 800, e: 0.041\n",
      "accumulated_rewards_per_episode: 9.000000000000112\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 6637/25000, steps: 111, e: 0.041\n",
      "accumulated_rewards_per_episode: 4.890000000000013\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 6638/25000, steps: 67, e: 0.041\n",
      "accumulated_rewards_per_episode: 1.3299999999999996\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6639/25000, steps: 65, e: 0.041\n",
      "accumulated_rewards_per_episode: 1.3499999999999994\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 6640/25000, steps: 52, e: 0.041\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 6641/25000, steps: 52, e: 0.041\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 6642/25000, steps: 223, e: 0.041\n",
      "accumulated_rewards_per_episode: -0.23000000000000165\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6643/25000, steps: 52, e: 0.041\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 6644/25000, steps: 147, e: 0.041\n",
      "accumulated_rewards_per_episode: 0.5299999999999989\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 6645/25000, steps: 52, e: 0.041\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 6646/25000, steps: 689, e: 0.041\n",
      "accumulated_rewards_per_episode: 13.110000000000097\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 6647/25000, steps: 68, e: 0.041\n",
      "accumulated_rewards_per_episode: 1.3199999999999996\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6648/25000, steps: 52, e: 0.041\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 6649/25000, steps: 52, e: 0.041\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 9)\n",
      "episode: 6650/25000, steps: 92, e: 0.041\n",
      "accumulated_rewards_per_episode: 3.0800000000000045\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6651/25000, steps: 52, e: 0.041\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 6652/25000, steps: 397, e: 0.041\n",
      "accumulated_rewards_per_episode: 4.0300000000000225\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 6653/25000, steps: 52, e: 0.041\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 6654/25000, steps: 85, e: 0.041\n",
      "accumulated_rewards_per_episode: 1.1499999999999992\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6655/25000, steps: 65, e: 0.041\n",
      "accumulated_rewards_per_episode: 1.3499999999999996\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 6656/25000, steps: 61, e: 0.041\n",
      "accumulated_rewards_per_episode: 0.38999999999999957\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 6657/25000, steps: 52, e: 0.041\n",
      "accumulated_rewards_per_episode: 1.4799999999999998\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6658/25000, steps: 288, e: 0.041\n",
      "accumulated_rewards_per_episode: 6.120000000000032\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 6659/25000, steps: 53, e: 0.041\n",
      "accumulated_rewards_per_episode: -0.5300000000000002\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 6660/25000, steps: 321, e: 0.041\n",
      "accumulated_rewards_per_episode: -1.2099999999999989\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 6661/25000, steps: 78, e: 0.041\n",
      "accumulated_rewards_per_episode: 3.220000000000006\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6662/25000, steps: 52, e: 0.041\n",
      "accumulated_rewards_per_episode: 1.4799999999999998\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 6663/25000, steps: 52, e: 0.041\n",
      "accumulated_rewards_per_episode: 1.4799999999999998\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 6664/25000, steps: 52, e: 0.041\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 6665/25000, steps: 92, e: 0.041\n",
      "accumulated_rewards_per_episode: 0.07999999999999947\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 6666/25000, steps: 251, e: 0.041\n",
      "accumulated_rewards_per_episode: 5.490000000000043\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 6667/25000, steps: 52, e: 0.041\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 6668/25000, steps: 52, e: 0.041\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 6669/25000, steps: 52, e: 0.041\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6670/25000, steps: 83, e: 0.041\n",
      "accumulated_rewards_per_episode: 1.1699999999999993\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 6671/25000, steps: 52, e: 0.041\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 6672/25000, steps: 52, e: 0.041\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 6673/25000, steps: 747, e: 0.041\n",
      "accumulated_rewards_per_episode: -4.469999999999885\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 6674/25000, steps: 52, e: 0.041\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6675/25000, steps: 52, e: 0.041\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 6676/25000, steps: 252, e: 0.041\n",
      "accumulated_rewards_per_episode: 0.479999999999998\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 6677/25000, steps: 349, e: 0.041\n",
      "accumulated_rewards_per_episode: 0.5099999999999978\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6678/25000, steps: 409, e: 0.041\n",
      "accumulated_rewards_per_episode: 7.910000000000034\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6679/25000, steps: 706, e: 0.041\n",
      "accumulated_rewards_per_episode: -0.05999999999991017\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 6680/25000, steps: 119, e: 0.041\n",
      "accumulated_rewards_per_episode: 0.8099999999999989\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6681/25000, steps: 441, e: 0.04\n",
      "accumulated_rewards_per_episode: -0.40999999999999126\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 6682/25000, steps: 55, e: 0.04\n",
      "accumulated_rewards_per_episode: 1.4499999999999995\n",
      "START state: (0, 0, 8, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6683/25000, steps: 478, e: 0.04\n",
      "accumulated_rewards_per_episode: -3.7799999999999425\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 6684/25000, steps: 125, e: 0.04\n",
      "accumulated_rewards_per_episode: -0.2500000000000009\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 6685/25000, steps: 52, e: 0.04\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 6686/25000, steps: 176, e: 0.04\n",
      "accumulated_rewards_per_episode: 4.24000000000001\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 6687/25000, steps: 57, e: 0.04\n",
      "accumulated_rewards_per_episode: 1.4299999999999995\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6688/25000, steps: 170, e: 0.04\n",
      "accumulated_rewards_per_episode: 0.2999999999999985\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6689/25000, steps: 800, e: 0.04\n",
      "accumulated_rewards_per_episode: 23.99999999999969\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6690/25000, steps: 215, e: 0.04\n",
      "accumulated_rewards_per_episode: 3.8500000000000254\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6691/25000, steps: 52, e: 0.04\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 6692/25000, steps: 435, e: 0.04\n",
      "accumulated_rewards_per_episode: -2.349999999999995\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 6693/25000, steps: 52, e: 0.04\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 6694/25000, steps: 52, e: 0.04\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6695/25000, steps: 52, e: 0.04\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6696/25000, steps: 163, e: 0.04\n",
      "accumulated_rewards_per_episode: 0.3699999999999988\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 6697/25000, steps: 442, e: 0.04\n",
      "accumulated_rewards_per_episode: 13.580000000000073\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 6698/25000, steps: 52, e: 0.04\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 6699/25000, steps: 52, e: 0.04\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 6700/25000, steps: 800, e: 0.04\n",
      "accumulated_rewards_per_episode: 6.00000000000011\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6701/25000, steps: 283, e: 0.04\n",
      "accumulated_rewards_per_episode: 1.169999999999998\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6702/25000, steps: 52, e: 0.04\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 6703/25000, steps: 52, e: 0.04\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6704/25000, steps: 52, e: 0.04\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 6705/25000, steps: 210, e: 0.04\n",
      "accumulated_rewards_per_episode: -0.1000000000000016\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 6706/25000, steps: 52, e: 0.04\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6707/25000, steps: 52, e: 0.04\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6708/25000, steps: 52, e: 0.04\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 6709/25000, steps: 800, e: 0.04\n",
      "accumulated_rewards_per_episode: 10.00000000000003\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6710/25000, steps: 157, e: 0.04\n",
      "accumulated_rewards_per_episode: 4.430000000000022\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6711/25000, steps: 52, e: 0.04\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 6712/25000, steps: 522, e: 0.04\n",
      "accumulated_rewards_per_episode: 6.7800000000000304\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 6713/25000, steps: 83, e: 0.04\n",
      "accumulated_rewards_per_episode: 2.1700000000000017\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6714/25000, steps: 52, e: 0.04\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6715/25000, steps: 52, e: 0.04\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 6716/25000, steps: 52, e: 0.04\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 6717/25000, steps: 56, e: 0.04\n",
      "accumulated_rewards_per_episode: 1.4399999999999995\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 6718/25000, steps: 52, e: 0.04\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6719/25000, steps: 52, e: 0.04\n",
      "accumulated_rewards_per_episode: 1.4799999999999998\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 6720/25000, steps: 174, e: 0.04\n",
      "accumulated_rewards_per_episode: 0.259999999999999\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 6721/25000, steps: 52, e: 0.04\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6722/25000, steps: 144, e: 0.04\n",
      "accumulated_rewards_per_episode: 2.560000000000006\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6723/25000, steps: 52, e: 0.04\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6724/25000, steps: 364, e: 0.04\n",
      "accumulated_rewards_per_episode: 10.36000000000004\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 6725/25000, steps: 107, e: 0.04\n",
      "accumulated_rewards_per_episode: 2.9300000000000033\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6726/25000, steps: 425, e: 0.04\n",
      "accumulated_rewards_per_episode: -2.249999999999976\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 6727/25000, steps: 197, e: 0.04\n",
      "accumulated_rewards_per_episode: -0.9700000000000015\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 6728/25000, steps: 60, e: 0.04\n",
      "accumulated_rewards_per_episode: 2.4000000000000012\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6729/25000, steps: 186, e: 0.04\n",
      "accumulated_rewards_per_episode: 0.13999999999999835\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6730/25000, steps: 52, e: 0.04\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 6731/25000, steps: 345, e: 0.04\n",
      "accumulated_rewards_per_episode: 2.549999999999998\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6732/25000, steps: 75, e: 0.04\n",
      "accumulated_rewards_per_episode: 1.2499999999999996\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 6733/25000, steps: 55, e: 0.039\n",
      "accumulated_rewards_per_episode: 1.4499999999999997\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 6734/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 6735/25000, steps: 475, e: 0.039\n",
      "accumulated_rewards_per_episode: -2.7499999999999654\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 6736/25000, steps: 617, e: 0.039\n",
      "accumulated_rewards_per_episode: 9.830000000000116\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 6737/25000, steps: 715, e: 0.039\n",
      "accumulated_rewards_per_episode: 2.8500000000000068\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6738/25000, steps: 148, e: 0.039\n",
      "accumulated_rewards_per_episode: 0.5199999999999991\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 6739/25000, steps: 84, e: 0.039\n",
      "accumulated_rewards_per_episode: 1.1599999999999993\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6740/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 5)\n",
      "episode: 6741/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: 1.4799999999999998\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6742/25000, steps: 403, e: 0.039\n",
      "accumulated_rewards_per_episode: -4.0299999999999585\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 6743/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6744/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 6745/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 6746/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 6747/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6748/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 6749/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6750/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6751/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6752/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6753/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6754/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: 1.4799999999999998\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6755/25000, steps: 139, e: 0.039\n",
      "accumulated_rewards_per_episode: 0.6099999999999988\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 6756/25000, steps: 111, e: 0.039\n",
      "accumulated_rewards_per_episode: -1.1100000000000008\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 6757/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6758/25000, steps: 503, e: 0.039\n",
      "accumulated_rewards_per_episode: 4.970000000000066\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6759/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: 1.4799999999999998\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 6760/25000, steps: 287, e: 0.039\n",
      "accumulated_rewards_per_episode: 0.1300000000000019\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 6761/25000, steps: 368, e: 0.039\n",
      "accumulated_rewards_per_episode: 1.3200000000000176\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 6762/25000, steps: 655, e: 0.039\n",
      "accumulated_rewards_per_episode: -2.549999999999905\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 6763/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6764/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 6765/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 6766/25000, steps: 222, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.22000000000000153\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6767/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6768/25000, steps: 147, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.4700000000000008\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 6769/25000, steps: 683, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.8300000000000022\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 6770/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 6771/25000, steps: 204, e: 0.039\n",
      "accumulated_rewards_per_episode: -2.0400000000000005\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6772/25000, steps: 94, e: 0.039\n",
      "accumulated_rewards_per_episode: 0.05999999999999948\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6773/25000, steps: 269, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.6899999999999991\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6774/25000, steps: 104, e: 0.039\n",
      "accumulated_rewards_per_episode: 0.9599999999999994\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6775/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 6776/25000, steps: 558, e: 0.039\n",
      "accumulated_rewards_per_episode: 0.4200000000000599\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 6777/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 6778/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 5)\n",
      "episode: 6779/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6780/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: 0.47999999999999954\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 6781/25000, steps: 52, e: 0.039\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6782/25000, steps: 156, e: 0.039\n",
      "accumulated_rewards_per_episode: 0.43999999999999906\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6783/25000, steps: 633, e: 0.039\n",
      "accumulated_rewards_per_episode: -3.329999999999953\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6784/25000, steps: 635, e: 0.039\n",
      "accumulated_rewards_per_episode: -1.350000000000002\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 6785/25000, steps: 133, e: 0.039\n",
      "accumulated_rewards_per_episode: 0.6699999999999992\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6786/25000, steps: 89, e: 0.038\n",
      "accumulated_rewards_per_episode: 1.1099999999999992\n",
      "START state: (0, 0, 8, 8)\n",
      "episode: 6787/25000, steps: 146, e: 0.038\n",
      "accumulated_rewards_per_episode: -0.4600000000000008\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 6788/25000, steps: 197, e: 0.038\n",
      "accumulated_rewards_per_episode: 1.0299999999999983\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 6789/25000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 6790/25000, steps: 444, e: 0.038\n",
      "accumulated_rewards_per_episode: -2.4399999999999498\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6791/25000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: 1.4799999999999995\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 6792/25000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6793/25000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6794/25000, steps: 66, e: 0.038\n",
      "accumulated_rewards_per_episode: 1.3399999999999996\n",
      "START state: (0, 0, 8, 9)\n",
      "episode: 6795/25000, steps: 152, e: 0.038\n",
      "accumulated_rewards_per_episode: -1.5200000000000011\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 6796/25000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 6797/25000, steps: 271, e: 0.038\n",
      "accumulated_rewards_per_episode: 3.2900000000000196\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 6798/25000, steps: 348, e: 0.038\n",
      "accumulated_rewards_per_episode: 5.520000000000051\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6799/25000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 6800/25000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: 0.47999999999999965\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 6801/25000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6802/25000, steps: 101, e: 0.038\n",
      "accumulated_rewards_per_episode: -1.0100000000000007\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 6803/25000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6804/25000, steps: 78, e: 0.038\n",
      "accumulated_rewards_per_episode: 0.21999999999999942\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 6805/25000, steps: 654, e: 0.038\n",
      "accumulated_rewards_per_episode: 3.46\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 6806/25000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 6807/25000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 4)\n",
      "episode: 6808/25000, steps: 179, e: 0.038\n",
      "accumulated_rewards_per_episode: -0.7900000000000014\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 6809/25000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 9, 8)\n",
      "episode: 6810/25000, steps: 63, e: 0.038\n",
      "accumulated_rewards_per_episode: 0.36999999999999966\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 6811/25000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: 1.4799999999999998\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 6812/25000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 6813/25000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 6814/25000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 6815/25000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 6816/25000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 6817/25000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: -0.5200000000000002\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6818/25000, steps: 181, e: 0.038\n",
      "accumulated_rewards_per_episode: 0.18999999999999861\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6819/25000, steps: 755, e: 0.038\n",
      "accumulated_rewards_per_episode: -3.549999999999941\n",
      "START state: (0, 0, 3, 0)\n"
     ]
    }
   ],
   "source": [
    " t2, epsilon, average_accumulated_rewards = csrl.train_DRQN(EPISODES=25000, num_steps=800, batch_size=32, weights_update=50, state_sequence_size=5, label_sequence_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b66d29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "fig, ax1 = plt.subplots() \n",
    "\n",
    "ax1.set_xlabel('Episode(s)', fontsize=12) \n",
    "ax1.set_ylabel('Exploration Rate (Epsilon)', fontsize=12, color = 'red') \n",
    "ax1.plot(t2, epsilon, color = 'red') \n",
    "ax1.tick_params(axis ='y', labelcolor = 'red') \n",
    "# Adding Twin Axes\n",
    "ax2 = ax1.twinx() \n",
    "ax2.set_ylabel('Accumulated Rewards / 10 Episodes', fontsize=12, color = 'blue') \n",
    "ax2.plot(t2, average_accumulated_rewards, color = 'blue') \n",
    "ax2.tick_params(axis ='y', labelcolor = 'blue') \n",
    "# Show plot\n",
    "#plt.title('The Accumulated Rewards of 10 by 10 Grid-world Simulation (label uncertainty Pl = 1.0)', fontsize=14)\n",
    "plt.grid()\n",
    "plt.rcParams['figure.figsize'] = [8, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76701be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(t2).tofile('t2_protest_seq5_Q3_25000.csv', sep = ',')\n",
    "np.array(average_accumulated_rewards).tofile('rewards_protest_seq5_Q3_25000.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63a780a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "RGB = 256\n",
    "# blue\n",
    "royalblue = (65/RGB,105/RGB,225/RGB)\n",
    "dodgerblue = (30/RGB,144/RGB,255/RGB)\n",
    "# green\n",
    "green = (0/RGB,128/RGB,0/RGB)\n",
    "# purple\n",
    "darkorchid = (104/RGB,34/RGB,139/RGB)\n",
    "# red & pink\n",
    "coral = (205/RGB,91/RGB,69/RGB)\n",
    "red = (139/RGB,0/RGB,0/RGB)\n",
    "pink = (252/RGB,20/RGB,201/RGB)\n",
    "orange = (255/RGB,128/RGB,0/RGB)\n",
    "\n",
    "def cumsum_sma(array, period):\n",
    "    ret = np.cumsum(array, dtype=float)\n",
    "    ret[period:] = ret[period:] - ret[:-period]\n",
    "    return ret[period - 1:] / period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03fa8fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import make_interp_spline\n",
    "t2_smooth = t2.copy()\n",
    "average_smooth = average_accumulated_rewards.copy()\n",
    "X_Y_Spline = make_interp_spline(t2_smooth, average_smooth)\n",
    "X_ = np.linspace(t2_smooth.min(), t2_smooth.max(), 50)\n",
    "Y_ = X_Y_Spline(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd76f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = cumsum_sma(t2,100)\n",
    "averaged = cumsum_sma(average_accumulated_rewards,100)\n",
    "average_smooth = X_Y_Spline(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b36ac52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 7))\n",
    "ax1.set_xlabel('Episode(s)', fontsize=18) \n",
    "ax1.set_ylabel('Averaged Accumulated Rewards', fontsize=18) \n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.lineplot(x = t2, y = average_accumulated_rewards, color=royalblue, alpha=0.4)\n",
    "sns.lineplot(x = t, y = averaged, color=royalblue, linewidth=3)\n",
    "plt.grid(color = 'white', linestyle = '--', linewidth = 1)\n",
    "plt.show()\n",
    "#sns.move_legend(ax, loc='upper left', frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffe0f664",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START state: (0, 0, 3, 9)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 8)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 8)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 9)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 8)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 3)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 2)\n",
      "action: 3\n",
      "reward0.99\n",
      "state: (0, 0, 8, 1)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 6)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 7)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 3, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 2, 8)\n",
      "action: 5\n",
      "reward0.99\n",
      "state: (0, 1, 1, 9)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 8)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 8)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 8)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 3)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 2)\n",
      "action: 3\n",
      "reward0.99\n",
      "state: (0, 0, 8, 1)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 6)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 7)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 3, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 2, 8)\n",
      "action: 5\n",
      "reward0.99\n",
      "state: (0, 1, 1, 9)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 8)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 3)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 2)\n",
      "action: 3\n",
      "reward0.99\n",
      "state: (0, 0, 8, 1)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 1)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 1)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 2)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 6)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 7)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 3, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 2, 8)\n",
      "action: 5\n",
      "reward0.99\n",
      "state: (0, 1, 1, 9)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 0, 0, 9)\n",
      "episode: 0/10, steps: 70, e: 1.0\n",
      "START state: (0, 0, 3, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 8)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 9)\n",
      "action: 4\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 8)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 8)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 3)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 2)\n",
      "action: 3\n",
      "reward0.99\n",
      "state: (0, 0, 8, 1)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 6)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 7)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 3, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 2, 8)\n",
      "action: 5\n",
      "reward0.99\n",
      "state: (0, 1, 1, 9)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 8)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 5)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 4)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 3)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 2)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 2)\n",
      "action: 6\n",
      "reward0.99\n",
      "state: (0, 0, 8, 1)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 0)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 1)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 6)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 7)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 8)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 3, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 2, 8)\n",
      "action: 0\n",
      "reward0.99\n",
      "state: (0, 1, 1, 8)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 8)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 3)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 2)\n",
      "action: 3\n",
      "reward0.99\n",
      "state: (0, 0, 9, 1)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 2)\n",
      "episode: 1/10, steps: 70, e: 1.0\n",
      "START state: (0, 0, 3, 9)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 8)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 8)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 8)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 3)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 2)\n",
      "action: 3\n",
      "reward0.99\n",
      "state: (0, 0, 8, 1)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 2)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 6)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 7)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 3, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 2, 8)\n",
      "action: 5\n",
      "reward0.99\n",
      "state: (0, 1, 1, 9)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 8)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 5)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 3)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 2)\n",
      "action: 3\n",
      "reward0.99\n",
      "state: (0, 0, 8, 1)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 6)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 7)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 3, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 2, 8)\n",
      "action: 5\n",
      "reward0.99\n",
      "state: (0, 1, 1, 9)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 8)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 3)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 2)\n",
      "action: 3\n",
      "reward0.99\n",
      "state: (0, 0, 8, 1)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 6)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 7)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 3, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 2, 8)\n",
      "episode: 2/10, steps: 70, e: 1.0\n",
      "START state: (0, 0, 3, 9)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 4\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 8)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 8)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 8)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 3)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 2)\n",
      "action: 3\n",
      "reward0.99\n",
      "state: (0, 0, 8, 1)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 5)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 6)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 7)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 3, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 2, 8)\n",
      "action: 5\n",
      "reward0.99\n",
      "state: (0, 1, 1, 9)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 8)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 3)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 2)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 1)\n",
      "action: 1\n",
      "reward0.99\n",
      "state: (0, 0, 8, 1)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 0)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 1)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 2)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 6)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 6)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 6)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 6)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 6)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 7)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 3, 8)\n",
      "episode: 3/10, steps: 70, e: 1.0\n",
      "START state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 9)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 8)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 4)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 3)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 2)\n",
      "action: 3\n",
      "reward0.99\n",
      "state: (0, 0, 8, 1)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 0)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 1)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 6)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 7)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 3, 9)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 2, 9)\n",
      "action: 0\n",
      "reward0.99\n",
      "state: (0, 1, 1, 9)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 8)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 4)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 3)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 2)\n",
      "action: 3\n",
      "reward0.99\n",
      "state: (0, 0, 8, 1)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 3)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 4)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 6)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 7)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 3, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 2, 8)\n",
      "action: 5\n",
      "reward0.99\n",
      "state: (0, 1, 1, 9)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 9)\n",
      "episode: 4/10, steps: 70, e: 1.0\n",
      "START state: (0, 0, 3, 9)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 8)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 9)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 8)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 4)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 3)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 2)\n",
      "action: 3\n",
      "reward0.99\n",
      "state: (0, 0, 8, 1)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 6)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 7)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 3, 8)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 2, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 2, 9)\n",
      "action: 0\n",
      "reward0.99\n",
      "state: (0, 1, 1, 9)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 8)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 8)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 8)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 3)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 2)\n",
      "action: 3\n",
      "reward0.99\n",
      "state: (0, 0, 8, 1)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 6)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 6)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 7)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 3, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 2, 8)\n",
      "action: 5\n",
      "reward0.99\n",
      "state: (0, 1, 1, 9)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 8)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 8)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 3)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 9, 2)\n",
      "action: 3\n",
      "reward0.99\n",
      "state: (0, 0, 9, 1)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 1)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 5)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 5)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 6)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 7)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 3, 8)\n",
      "episode: 5/10, steps: 70, e: 1.0\n",
      "START state: (0, 0, 3, 9)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 8)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 8)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 8)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 8)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 3)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 9, 2)\n",
      "action: 3\n",
      "reward0.99\n",
      "state: (0, 0, 8, 1)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 1)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 5)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 5)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 6)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 7)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 3, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 2, 8)\n",
      "action: 5\n",
      "reward0.99\n",
      "state: (0, 1, 1, 9)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 8)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 3)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 2)\n",
      "action: 3\n",
      "reward0.99\n",
      "state: (0, 0, 8, 1)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 6)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 6)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 7)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 8)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 9)\n",
      "episode: 6/10, steps: 70, e: 1.0\n",
      "START state: (0, 0, 3, 9)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 8)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 1, 7)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 0, 0, 8)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 1, 8)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 8)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 8)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 3)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 9, 2)\n",
      "action: 3\n",
      "reward0.99\n",
      "state: (0, 0, 9, 1)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 1)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 6)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 7)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 8)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 9)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 9)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 3, 9)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 2, 9)\n",
      "action: 0\n",
      "reward0.99\n",
      "state: (0, 1, 1, 9)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 8)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 3)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 9, 2)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 2)\n",
      "action: 3\n",
      "reward0.99\n",
      "state: (0, 0, 8, 1)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 6)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 7)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 8)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 3, 9)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 2, 9)\n",
      "action: 0\n",
      "reward0.99\n",
      "state: (0, 1, 1, 9)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 8)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 4)\n",
      "episode: 7/10, steps: 70, e: 1.0\n",
      "START state: (0, 0, 3, 9)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 8)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 3)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 2)\n",
      "action: 3\n",
      "reward0.99\n",
      "state: (0, 0, 8, 1)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 0)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 1)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 6)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 7)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 8)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 9)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 9)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 3, 9)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 2, 9)\n",
      "action: 0\n",
      "reward0.99\n",
      "state: (0, 1, 1, 9)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 8)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 4)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 3)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 2)\n",
      "action: 3\n",
      "reward0.99\n",
      "state: (0, 0, 8, 1)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 5)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 6)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 7)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 3, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 2, 8)\n",
      "action: 5\n",
      "reward0.99\n",
      "state: (0, 1, 1, 9)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 8)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "episode: 8/10, steps: 70, e: 1.0\n",
      "START state: (0, 0, 3, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 8)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 9)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 0, 1, 8)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 8)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 8)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 8)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 3)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 2)\n",
      "action: 3\n",
      "reward0.99\n",
      "state: (0, 0, 8, 1)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 2)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 1)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 6)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 7)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 3, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 2, 8)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 1, 7)\n",
      "action: 5\n",
      "reward0.99\n",
      "state: (0, 1, 0, 8)\n",
      "action: 2\n",
      "reward-0.01\n",
      "state: (0, 0, 0, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 1, 8)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 8)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 3)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 9, 2)\n",
      "action: 3\n",
      "reward0.99\n",
      "state: (0, 0, 9, 1)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 6, 4)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 5)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 6)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 5, 7)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 4, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 3, 8)\n",
      "action: 0\n",
      "reward-0.01\n",
      "state: (0, 1, 2, 8)\n",
      "action: 5\n",
      "reward0.99\n",
      "state: (0, 1, 1, 9)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 2, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward-0.01\n",
      "state: (0, 0, 4, 9)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 8)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 7)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 5, 6)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 6, 5)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 7, 4)\n",
      "action: 6\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 3)\n",
      "action: 3\n",
      "reward-0.01\n",
      "state: (0, 0, 8, 2)\n",
      "action: 3\n",
      "reward0.99\n",
      "state: (0, 0, 8, 1)\n",
      "action: 7\n",
      "reward-0.01\n",
      "state: (0, 1, 9, 2)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 8, 3)\n",
      "action: 5\n",
      "reward-0.01\n",
      "state: (0, 1, 7, 4)\n",
      "episode: 9/10, steps: 70, e: 1.0\n"
     ]
    }
   ],
   "source": [
    "Path = csrl.verify_DRQN(EPISODES=10, num_steps=70, state_sequence_size=5, label_sequence_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0aa7192c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path_1[39, 38, 48, 59, 58, 57, 56, 65, 74, 83, 82, 81]\n",
      "Path_2[92, 83, 74, 65, 56, 57, 48, 38, 28, 19]\n",
      "Path_3[19, 29, 28, 38, 48, 57, 56, 65, 74, 83, 82, 81]\n"
     ]
    }
   ],
   "source": [
    "############################# Plot the Path ##############################\n",
    "import pylab as pl\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "path_idx = 0\n",
    "\n",
    "size_x = csrl.shape[2]\n",
    "size_y = csrl.shape[3]\n",
    "\n",
    "Path_1=[]\n",
    "Path_2=[]\n",
    "Path_3=[]\n",
    "ii=0\n",
    "for i in range(0,len(Path[path_idx])):\n",
    "    if Path[path_idx][i]>=size_x*size_y:\n",
    "        break\n",
    "    Path_1.append(int(Path[path_idx][i]))\n",
    "ii=i\n",
    "    \n",
    "for j in range(ii, len(Path[path_idx]), 1):\n",
    "    if Path[path_idx][j]<size_x*size_y or Path[path_idx][j]>=2*size_x*size_y:\n",
    "        break\n",
    "    Path_2.append(int(Path[path_idx][j]-size_x*size_y))\n",
    "ii=j\n",
    "\n",
    "for k in range(ii, len(Path[path_idx]), 1):\n",
    "    if Path[path_idx][k]>=size_x*size_y:\n",
    "        break\n",
    "    Path_3.append(int(Path[path_idx][k]))\n",
    "    \n",
    "print('Path_1'+str(Path_1))\n",
    "print('Path_2'+str(Path_2))\n",
    "print('Path_3'+str(Path_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fef59d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path_1=[17, 27, 27, 17, 7, 17, 27, 26, 36, 46, 45, 44, 43, 53, 52, 51, 61, 71, 81]\n",
    "Path_2=[71, 61, 51, 41, 42, 43, 44, 45, 44, 45, 35, 36, 26, 16, 17, 18]\n",
    "Path_3=[8, 18, 17, 27, 17, 27, 26, 36, 46, 56, 55, 65, 75, 74, 84, 83, 82, 81]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb557966",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###### q_0 ######\n",
    "    \n",
    "x = np.linspace(0,size_x,size_x+1)\n",
    "y = np.linspace(0,size_y,size_y+1) \n",
    "\n",
    "pl.figure(figsize=(10,10))\n",
    "\n",
    "hlines = np.column_stack(np.broadcast_arrays(x[0], y, x[-1], y))\n",
    "vlines = np.column_stack(np.broadcast_arrays(x, y[0], x, y[-1]))\n",
    "lines = np.concatenate([hlines, vlines]).reshape(-1, 2, 2)\n",
    "line_collection = LineCollection(lines, color=\"black\", linewidths=1)\n",
    "ax = pl.gca()\n",
    "ax.add_collection(line_collection)\n",
    "ax.set_xlim(int(x[0]), int(x[-1]))\n",
    "ax.set_ylim(int(y[0]), int(y[-1]))\n",
    "# backgroud color\n",
    "ax.add_patch(pl.Rectangle((0, 0), size_x, size_y, fill=True, color='grey', alpha=.1))\n",
    "\n",
    "# plot the 'a' state 'q_0'\n",
    "b_start_x, b_start_y = 0,0\n",
    "b_size_x, b_size_y = 2,2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='blue', alpha=.4))\n",
    "\n",
    "# plot the 'b' state 'q_0'\n",
    "b_start_x, b_start_y = 8,8\n",
    "b_size_x, b_size_y = 2,2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='green', alpha=.4))\n",
    "\n",
    "# plot the trap state 'q_0'\n",
    "c_start_x, c_start_y = 0,8\n",
    "c_size_x, c_size_y = 2,2\n",
    "ax.add_patch(pl.Rectangle((c_start_x, c_start_y), c_size_x, c_size_y, fill=True, color='orange', alpha=.5))\n",
    "c_start_x, c_start_y = 5,6\n",
    "c_size_x, c_size_y = 2,2\n",
    "ax.add_patch(pl.Rectangle((c_start_x, c_start_y), c_size_x, c_size_y, fill=True, color='orange', alpha=.5))\n",
    "'''\n",
    "c_start_x, c_start_y = 8,0\n",
    "c_size_x, c_size_y = 2,2\n",
    "ax.add_patch(pl.Rectangle((c_start_x, c_start_y), c_size_x, c_size_y, fill=True, color='orange', alpha=.5))\n",
    "c_start_x, c_start_y = 2,4\n",
    "c_size_x, c_size_y = 1,2\n",
    "ax.add_patch(pl.Rectangle((c_start_x, c_start_y), c_size_x, c_size_y, fill=True, color='orange', alpha=.5))\n",
    "c_start_x, c_start_y = 7,4\n",
    "c_size_x, c_size_y = 1,1\n",
    "ax.add_patch(pl.Rectangle((c_start_x, c_start_y), c_size_x, c_size_y, fill=True, color='orange', alpha=.5))\n",
    "c_start_x, c_start_y = 4,2\n",
    "c_size_x, c_size_y = 2,2\n",
    "ax.add_patch(pl.Rectangle((c_start_x, c_start_y), c_size_x, c_size_y, fill=True, color='orange', alpha=.5))\n",
    "'''\n",
    "# plot the BLOCK state 'q_0'\n",
    "b_start_x, b_start_y = 2,6\n",
    "b_size_x, b_size_y = 3,2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='black', alpha=.7))\n",
    "b_start_x, b_start_y = 2,3\n",
    "b_size_x, b_size_y = 2,1\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='black', alpha=.7))\n",
    "b_start_x, b_start_y = 6,2\n",
    "b_size_x, b_size_y = 2,2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='black', alpha=.7))\n",
    "b_start_x, b_start_y = 7,5\n",
    "b_size_x, b_size_y = 1,2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='black', alpha=.7))\n",
    "\n",
    "for i in range(len(Path_1)):\n",
    "    state_idx = Path_1[i]+1\n",
    "    ## convert state index in Julia to 'x,y' coordinates in python\n",
    "    if state_idx%size_x==0:\n",
    "        coord_x = size_x-1    # x_coordinate\n",
    "        #coord_y = int(size_y - (state_idx-state_idx%size_x)/size_x) # y_coordinate\n",
    "        coord_y = int(size_y - (state_idx/size_y)) # y_coordinate\n",
    "    else:\n",
    "        coord_x = state_idx%size_x-1    # x_coordinate\n",
    "        coord_y = int(size_y - 1 - (state_idx-state_idx%size_x)/size_x) # y_coordinate\n",
    "    ax.add_patch(pl.Circle((coord_x+0.5, coord_y+0.5), 0.2, fill=True, color='red', alpha=.4))\n",
    "    if i==0:\n",
    "        # start point\n",
    "        ax.add_patch(pl.Circle((coord_x+0.5, coord_y+0.5), 0.4, fill=True, color='purple', alpha=.9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dfe40912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###### q_1 ######\n",
    "\n",
    "x = np.linspace(0,size_x,size_x+1)\n",
    "y = np.linspace(0,size_y,size_y+1) \n",
    "\n",
    "pl.figure(figsize=(10,10))\n",
    "\n",
    "hlines = np.column_stack(np.broadcast_arrays(x[0], y, x[-1], y))\n",
    "vlines = np.column_stack(np.broadcast_arrays(x, y[0], x, y[-1]))\n",
    "lines = np.concatenate([hlines, vlines]).reshape(-1, 2, 2)\n",
    "line_collection = LineCollection(lines, color=\"black\", linewidths=1)\n",
    "ax = pl.gca()\n",
    "ax.add_collection(line_collection)\n",
    "ax.set_xlim(int(x[0]), int(x[-1]))\n",
    "ax.set_ylim(int(y[0]), int(y[-1]))\n",
    "# backgroud color\n",
    "ax.add_patch(pl.Rectangle((0, 0), size_x, size_y, fill=True, color='grey', alpha=.1))\n",
    "\n",
    "# plot the 'a' state 'q_0'\n",
    "b_start_x, b_start_y = 0,0\n",
    "b_size_x, b_size_y = 2,2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='blue', alpha=.4))\n",
    "\n",
    "# plot the 'b' state 'q_0'\n",
    "b_start_x, b_start_y = 8,8\n",
    "b_size_x, b_size_y = 2,2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='green', alpha=.4))\n",
    "\n",
    "# plot the trap state 'q_0'\n",
    "c_start_x, c_start_y = 0,8\n",
    "c_size_x, c_size_y = 2,2\n",
    "ax.add_patch(pl.Rectangle((c_start_x, c_start_y), c_size_x, c_size_y, fill=True, color='orange', alpha=.5))\n",
    "c_start_x, c_start_y = 5,6\n",
    "c_size_x, c_size_y = 2,2\n",
    "ax.add_patch(pl.Rectangle((c_start_x, c_start_y), c_size_x, c_size_y, fill=True, color='orange', alpha=.5))\n",
    "'''\n",
    "c_start_x, c_start_y = 8,0\n",
    "c_size_x, c_size_y = 2,2\n",
    "ax.add_patch(pl.Rectangle((c_start_x, c_start_y), c_size_x, c_size_y, fill=True, color='orange', alpha=.5))\n",
    "c_start_x, c_start_y = 2,4\n",
    "c_size_x, c_size_y = 1,2\n",
    "ax.add_patch(pl.Rectangle((c_start_x, c_start_y), c_size_x, c_size_y, fill=True, color='orange', alpha=.5))\n",
    "c_start_x, c_start_y = 7,4\n",
    "c_size_x, c_size_y = 1,1\n",
    "ax.add_patch(pl.Rectangle((c_start_x, c_start_y), c_size_x, c_size_y, fill=True, color='orange', alpha=.5))\n",
    "c_start_x, c_start_y = 4,2\n",
    "c_size_x, c_size_y = 2,2\n",
    "ax.add_patch(pl.Rectangle((c_start_x, c_start_y), c_size_x, c_size_y, fill=True, color='orange', alpha=.5))\n",
    "'''\n",
    "# plot the BLOCK state 'q_0'\n",
    "b_start_x, b_start_y = 2,6\n",
    "b_size_x, b_size_y = 3,2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='black', alpha=.7))\n",
    "b_start_x, b_start_y = 2,3\n",
    "b_size_x, b_size_y = 2,1\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='black', alpha=.7))\n",
    "b_start_x, b_start_y = 6,2\n",
    "b_size_x, b_size_y = 2,2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='black', alpha=.7))\n",
    "b_start_x, b_start_y = 7,5\n",
    "b_size_x, b_size_y = 1,2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='black', alpha=.7))\n",
    "\n",
    "for i in range(len(Path_2)):\n",
    "    state_idx = Path_2[i]+1\n",
    "    ## convert state index in Julia to 'x,y' coordinates in python\n",
    "    if state_idx%size_x==0:\n",
    "        coord_x = size_x-1    # x_coordinate\n",
    "        #coord_y = int(size_y - (state_idx-state_idx%size_x)/size_x) # y_coordinate\n",
    "        coord_y = int(size_y - (state_idx/size_y)) # y_coordinate\n",
    "    else:\n",
    "        coord_x = state_idx%size_x-1    # x_coordinate\n",
    "        coord_y = int(size_y - 1 - (state_idx-state_idx%size_x)/size_x) # y_coordinate\n",
    "    ax.add_patch(pl.Circle((coord_x+0.5, coord_y+0.5), 0.2, fill=True, color='red', alpha=.4))\n",
    "    '''\n",
    "    if i==0:\n",
    "        # start point\n",
    "        ax.add_patch(pl.Circle((coord_x+0.5, coord_y+0.5), 0.4, fill=True, color='purple', alpha=.9))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23c50736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###### q_0 again ######\n",
    "\n",
    "x = np.linspace(0,size_x,size_x+1)\n",
    "y = np.linspace(0,size_y,size_y+1) \n",
    "\n",
    "pl.figure(figsize=(10,10))\n",
    "\n",
    "hlines = np.column_stack(np.broadcast_arrays(x[0], y, x[-1], y))\n",
    "vlines = np.column_stack(np.broadcast_arrays(x, y[0], x, y[-1]))\n",
    "lines = np.concatenate([hlines, vlines]).reshape(-1, 2, 2)\n",
    "line_collection = LineCollection(lines, color=\"black\", linewidths=1)\n",
    "ax = pl.gca()\n",
    "ax.add_collection(line_collection)\n",
    "ax.set_xlim(int(x[0]), int(x[-1]))\n",
    "ax.set_ylim(int(y[0]), int(y[-1]))\n",
    "# backgroud color\n",
    "ax.add_patch(pl.Rectangle((0, 0), size_x, size_y, fill=True, color='grey', alpha=.1))\n",
    "\n",
    "# plot the 'a' state 'q_0'\n",
    "b_start_x, b_start_y = 0,0\n",
    "b_size_x, b_size_y = 2,2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='blue', alpha=.4))\n",
    "\n",
    "# plot the 'b' state 'q_0'\n",
    "b_start_x, b_start_y = 8,8\n",
    "b_size_x, b_size_y = 2,2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='green', alpha=.4))\n",
    "\n",
    "# plot the trap state 'q_0'\n",
    "c_start_x, c_start_y = 0,8\n",
    "c_size_x, c_size_y = 2,2\n",
    "ax.add_patch(pl.Rectangle((c_start_x, c_start_y), c_size_x, c_size_y, fill=True, color='orange', alpha=.5))\n",
    "c_start_x, c_start_y = 5,6\n",
    "c_size_x, c_size_y = 2,2\n",
    "ax.add_patch(pl.Rectangle((c_start_x, c_start_y), c_size_x, c_size_y, fill=True, color='orange', alpha=.5))\n",
    "'''\n",
    "c_start_x, c_start_y = 8,0\n",
    "c_size_x, c_size_y = 2,2\n",
    "ax.add_patch(pl.Rectangle((c_start_x, c_start_y), c_size_x, c_size_y, fill=True, color='orange', alpha=.5))\n",
    "c_start_x, c_start_y = 2,4\n",
    "c_size_x, c_size_y = 1,2\n",
    "ax.add_patch(pl.Rectangle((c_start_x, c_start_y), c_size_x, c_size_y, fill=True, color='orange', alpha=.5))\n",
    "c_start_x, c_start_y = 7,4\n",
    "c_size_x, c_size_y = 1,1\n",
    "ax.add_patch(pl.Rectangle((c_start_x, c_start_y), c_size_x, c_size_y, fill=True, color='orange', alpha=.5))\n",
    "c_start_x, c_start_y = 4,2\n",
    "c_size_x, c_size_y = 2,2\n",
    "ax.add_patch(pl.Rectangle((c_start_x, c_start_y), c_size_x, c_size_y, fill=True, color='orange', alpha=.5))\n",
    "'''\n",
    "# plot the BLOCK state 'q_0'\n",
    "b_start_x, b_start_y = 2,6\n",
    "b_size_x, b_size_y = 3,2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='black', alpha=.7))\n",
    "b_start_x, b_start_y = 2,3\n",
    "b_size_x, b_size_y = 2,1\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='black', alpha=.7))\n",
    "b_start_x, b_start_y = 6,2\n",
    "b_size_x, b_size_y = 2,2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='black', alpha=.7))\n",
    "b_start_x, b_start_y = 7,5\n",
    "b_size_x, b_size_y = 1,2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='black', alpha=.7))\n",
    "\n",
    "for i in range(len(Path_3)):\n",
    "    state_idx = Path_3[i]+1\n",
    "    ## convert state index in Julia to 'x,y' coordinates in python\n",
    "    if state_idx%size_x==0:\n",
    "        coord_x = size_x-1    # x_coordinate\n",
    "        #coord_y = int(size_y - (state_idx-state_idx%size_x)/size_x) # y_coordinate\n",
    "        coord_y = int(size_y - (state_idx/size_y)) # y_coordinate\n",
    "    else:\n",
    "        coord_x = state_idx%size_x-1    # x_coordinate\n",
    "        coord_y = int(size_y - 1 - (state_idx-state_idx%size_x)/size_x) # y_coordinate\n",
    "    ax.add_patch(pl.Circle((coord_x+0.5, coord_y+0.5), 0.2, fill=True, color='red', alpha=.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ef1e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
